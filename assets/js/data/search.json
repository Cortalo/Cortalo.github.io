[ { "title": "Modern Digital Signal Processing Explanation", "url": "/posts/modern-dsp-explain/", "categories": "math", "tags": "math", "date": "2024-06-12 04:00:00 +0000", "snippet": "P1video lecturesTextbook: S.M.kay, fundamental of statistical signal processing vol i ii, this book is easy to understand. S. Haykin: adaptive filter theory, fifth edition. Petre Stoica and Randolph Moses: spectral analysis of signal Murphy, machine learningprobability is defined on the subsets of sample spaces\\[P(A \\cup B) = P(A) + P(B), A \\cap B = \\emptyset\\]\\[\\begin{align}&amp; \\forall A_1, A_2, \\dots, A_k \\dots \\subseteq \\Omega, A_i \\cap A_j = \\emptyset, \\forall i,j\\\\&amp; P(\\bigcup\\limits_{k=1}^{\\infty} A_k) = \\sum_{k=1}^{\\infty} P(A_k), \\text{(countable additivity)}\\end{align}\\]\\[P(\\bigcup\\limits_{k=1}^{\\infty} A_k) = \\sum_{k=1}^{\\infty} P(A_k) - \\sum_{i &lt; j}P(A_i \\cap A_j) + \\sum_{i&lt;j&lt;k} P(A_i \\cap A_j \\cap A_k) - \\dots \\text{(inclusive exclusive)}\\]example for using inclusive exclusive: assuming n persons has n hats, now they randomly throw the hats and get one, what is the probability such that no one get the correct hat?Let \\(P(A_k)\\) be the probability that person \\(k\\) get the correct hat, obviously\\[P(A_k) = \\dfrac{(n-1)!}{n!} = \\dfrac{1}{n}\\]the probability that no one get the correct hat\\[P(\\overline{A_1} \\cap \\overline{A_2} \\cap \\dots \\cap \\overline{A_n})\\]using De Morgen’s law\\[P(\\overline{A_1 \\cup A_2 \\cup \\dots \\cup A_n}) = 1 - P(A_1 \\cup \\dots \\cup A_n)\\]\\[P(A_1 \\cup \\dots \\cup A_n) = \\sum_{k=1}^{n} P(A_k) - \\sum_{i&lt;j}P(A_i \\cap A_j) + \\sum_{i&lt;j&lt;k}P(A_i \\cap A_j \\cap A_k) - \\dots\\]where\\[\\begin{align}P(A_k) &amp;= \\dfrac{(n-1)!}{n!} = \\dfrac{1}{n}\\\\P(A_i \\cap A_j) &amp;= \\dfrac{(n-2)!}{n!} = \\dfrac{1}{n(n-1)}\\\\P(A_i \\cap A_j \\cap A_k) &amp;= \\dfrac{(n-3)!}{n!} = \\dfrac{1}{n(n-1)(n-2)}\\\\&amp; \\vdots\\end{align}\\]\\[\\begin{align}P(A_1 \\cup \\dots \\cup A_n) &amp;= n \\cdot \\dfrac{1}{n} - \\binom{n}{2} \\dfrac{1}{n(n-1)} + \\binom{n}{3} \\dfrac{1}{n(n-1)(n-2)} - \\dots\\\\&amp;= 1 - \\dfrac{1}{2!} + \\dfrac{1}{3!} - \\dots\\end{align}\\]the probability is \\(1 - P(A_1 \\cup \\dots \\cup A_n )= \\dfrac{1}{2!} - \\dfrac{1}{3!} + \\dfrac{1}{4!} - \\dots\\)independent\\[P(AB) = P(A) \\cdot P(B)\\]conditional probability\\[P(B|A) = P(AB)/P(A)\\]conditional probability is also a probability, defined on a new sample space, the original sample space is \\(\\Omega\\), the new sample space is \\(A\\).related name for conditional inclusing prior.if \\(A\\) and \\(B\\) are independent, \\(P(B\\vert A) = P(B)\\), meaning knowing \\(A\\) doesn’t give information regarding \\(B\\).total probability formulaif \\(A_i \\cap A_j = \\emptyset\\) and \\(\\bigcup\\limits_{k=1}^n A_k = \\Omega\\)\\[P(B) = P(B|A_1)P(A_1) + \\dots + P(B|A_n)P(A_n)\\]example: n person n hats, the first one will randomly pick one hat, then start from the second person, if his hat is still there, he will pick his own hat; otherwise he randomly pick one hat. For the last person, what is the probability that he pick his own hat?Let the event \\(A_n\\) be person \\(n\\) pick his own hat, when there is in total \\(n\\) person,let the events \\(B_1, B_2, \\dots, B_n\\) be the first person pick hat \\(1, 2,\\dots, n\\).Using total probability formula\\[P(A_n) = P(A_n \\vert B_1)P(B_1) + \\dots + P(A_n \\vert B_n) P(B_n)\\]\\[P(A_n \\vert B_1) P(B_1) = 1 \\cdot \\dfrac{1}{n}\\]\\[P(A_n \\vert B_n) P(B_n) = 0 \\cdot \\dfrac{1}{n}\\]for \\(P(A_n \\vert B_k)\\), we know that person \\(2, \\dots, k-1\\) pick their own hat, then person \\(k\\) need to pick randomly from the remainly \\(n-k+1\\) hats.We can imagin hat \\(1\\) is person \\(k\\)’s original hat, then\\[P(A_n \\vert B_k) = P(A_{n-k+1})\\]\\[P(A_n) = 1 \\cdot \\dfrac{1}{n} + \\left(\\sum_{k=2}^{n-1} \\dfrac{1}{n} P(A_{n-k+1})\\right) + 0 \\cdot \\dfrac{1}{n}\\]\\[P(A_1) = 1\\]\\[P(A_2) = \\dfrac{1}{2}\\]\\[P(A_3) = \\dfrac{1}{3} + \\dfrac{1}{3} P(A_2) = \\dfrac{1}{2}\\]assumes \\(P(A_2) = P(A_3) = \\dots = P(A_{n-1}) = 1/2\\)\\[P(A_n) = \\dfrac{1}{n} + \\dfrac{n-2}{n} \\cdot \\dfrac{1}{2} = \\dfrac{1}{2}\\]Bayesian\\[P(B \\vert A) = \\dfrac{P(A \\vert B) P(B)}{P(A)} = \\dfrac{P(A \\vert B)P(B)}{\\sum_{k} P(A \\vert C_k) P(C_k)}\\]exampleThe first bag has 4 black and 3 white balls, the second bag has 5 black and 2 white ball.Now randomly choose 2 balls from first bag to second bag, then pick one ball from second bag, we found it is white ball.Based on this condition what is the probability that the choosed 2 balls have same color?Let event \\(B\\) be the 2 balls have same color, let event \\(A\\) be we found it is white.Let event \\(C_1, C_2, C_3\\) be the 2 balls are all white, all black and one white one black.\\[P(B\\vert A) = \\dfrac{P(AB)}{P(A)} = \\dfrac{P(AB\\vert C_1) P(C_1) + P(AB \\vert C_2) P(C_2) + P(AB \\vert C_3) P(C_3)}{P(A\\vert C_1)P(C_1) + P(A\\vert C_2)P(C_2) + P(A \\vert C_3) P(C_3)}\\]random variables is a determinstic function \\(X: \\Omega \\to \\mathbb{R}\\).probability density function and culmilitive density functionBernoulli distributionBinomial distribution \\(P(x=k) = \\binom{n}{k}p^k (1-p)^{n-k}\\)Possion distribution \\(P(x=k) = \\dfrac{\\lambda^k}{k!}\\exp(-\\lambda)\\)possion distribution can be derived from binomial distribution, let \\(n \\to \\infty, p \\to 0\\) while \\(np=\\lambda\\)\\[\\begin{align}P(x=k) &amp;= \\dfrac{n!}{k!(n-k)!} \\dfrac{\\lambda^k}{n^k} \\left(1 - \\dfrac{\\lambda}{n}\\right)^{n-k}\\\\&amp;=\\dfrac{\\lambda^k}{k!} \\cdot \\dfrac{n!}{(n-k)! \\cdot n^k} \\cdot \\left(1 - \\dfrac{\\lambda}{n}\\right)^n \\cdot \\left(1- \\dfrac{\\lambda}{n}\\right)^{-k}\\\\&amp;= \\dfrac{\\lambda^k}{k!} \\cdot \\dfrac{n \\cdot (n-1) \\dots (n-k+1)}{n^k} \\cdot \\left(1 - \\dfrac{\\lambda}{n}\\right)^n \\cdot \\left(1- \\dfrac{\\lambda}{n}\\right)^{-k}\\\\&amp;= \\dfrac{\\lambda^k}{k!}\\exp(-\\lambda)\\end{align}\\]here I use \\(\\lim_{n\\to\\infty} \\left(1 + \\dfrac{a}{n}\\right)^n = \\exp(a)\\)uniform distribution: \\(f(x) = \\dfrac{1}{b-a}I_{[a,b]}(x)\\)exponetional distribution: \\(f(x) = \\lambda \\exp(-\\lambda x)I_{[0,\\infty)}(x)\\)Gaussian distribution: \\(f(x) = \\dfrac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp(-\\dfrac{(x-\\mu)^2}{2\\sigma^2})\\)P2expectation (mean)\\[E(X) = \\int_{-\\infty}^{\\infty} x f(x) dx\\]expectation is linear\\[E\\left(\\sum_{k=1}^{n} X_k\\right) = \\sum_{k=1}^{n} E(X_k)\\]examplestill \\(N\\) person randomly choose \\(N\\) hats, what is the expection number of the person pick their own hat?\\[E(X_1 + \\dots X_N) = N \\cdot \\dfrac{1}{N} = 1\\]variance\\[Var(X) = E(X-EX)^2 = E(X^2) - (E(X))^2\\]convex function: convex function is like a bowl, for example \\(g(x) = x^2\\)\\[g(\\alpha x + (1-\\alpha)y) \\le \\alpha g(x) + (1-\\alpha)g(y)\\]\\[g\\left(\\sum_{k=1}^{n} \\alpha_k x_k\\right) \\le \\sum_{k=1}^{n} \\alpha_k g(x_k)\\quad \\text{ where } \\quad \\left(\\alpha_k \\ge 0, \\sum_{k=1}^{n} \\alpha_k = 1\\right)\\]then\\[E\\left(g(X)\\right) \\ge g\\left(E(X)\\right)\\]to feel the correctness for the equation above, note that convex function has another property, such that it has a supporting plane at any point of the function (P2, 0:43:00)\\[g(x) \\ge g(a) + L_a (x-a)\\]then\\[g(X) \\ge g(a) + L_a (X-a)\\]\\[E(g(X)) \\ge g(a) + L_a \\left(E\\left(X\\right)-a\\right)\\]since \\(a\\) can be arbitary, let \\(a = E(X)\\)\\[E(g(X)) \\ge g(E(X))\\]for example, \\(E(X^2) \\ge (E(X))^2\\), then it is easy to show, if we let \\(g(X) = X^2\\)\\[Var(X) = E(X - EX)^2 = E(X^2) - (E(X))^2 \\ge 0\\]approximationwe define mean square distance between two random variable\\[\\left(E(X-Y)^2\\right)^{1/2} = d(X,Y)\\]if we want to find the best approximation in terms of a constant\\[\\min\\limits_{a} E(X-a)^2\\]\\[\\dfrac{d}{da} E(X-a)^2 = 0\\]\\[-2E(X-a) = 0\\]\\[a=EX\\]now can understand mean is the best approximation using an constant, and variance is the square distance.\\[\\min\\limits_{g} E(X-g(Y))^2\\]to find the result above, we need conditional expection\\[E(X \\vert Y)\\]conditional expection has several important properties.   \\(E(X \\vert Y)\\) is a random variable.   \\(E\\left(\\sum\\limits_{k=1}^{n} X_k \\vert Y\\right) = \\sum\\limits_{k=1}^{n} E(X_k \\vert Y)\\)   \\(E(E(X\\vert Y)) = E(X)\\)   \\(E(X h(Y) \\vert Y) = h(Y) E(X \\vert Y)\\)now we can find \\(\\min\\limits_{g} E(X-g(Y))^2\\) intuitively\\[E(X-g(Y))^2 = E(E((X-g(Y))^2 \\vert Y))\\]we know\\[E((X-g(Y))^2 \\vert Y) \\ge E((X - E(X \\vert Y))^2 \\vert Y)\\]thus\\[E(X-g(Y))^2 = E(E((X-g(Y))^2 \\vert Y)) \\ge E( E((X - E(X \\vert Y))^2 \\vert Y) ) = E( (X - E(X \\vert Y))^2)\\]thus the optimal \\(g(Y) = E(X \\vert Y)\\).The above is an intuitive understanding, now we give the formal prove\\[\\begin{align} &amp; E(X-g(Y))^2 = E(X - E(X \\vert Y) + E(X \\vert Y) - g(Y))^2\\\\= &amp; E(X-E(X \\vert Y))^2 + E(E(X \\vert Y) - g(Y))^2 + 2 E((X - E(X \\vert Y))(E(X \\vert Y) - g(Y)))\\end{align}\\]we only need to show the last term equals zero.\\[\\begin{align}&amp; E((X - E(X \\vert Y))(E(X \\vert Y) - g(Y)))\\\\= &amp; E( E( (X - E(X \\vert Y))(E(X \\vert Y) - g(Y))\\vert Y) ) = 0\\end{align}\\]we typicaly want to do parametric or non-parametric model, an example for parametric can be we know the data follows guassian distribution, and we want to find its mean and variance;an example for non-parametric model is clustering.We first look at parametric model.We want to find a function \\(\\hat{\\theta}(X_1, \\dots, X_n)\\), such a function we may call it estimator, statistic or feature.For such estimator, there are two school of thoughts: frequencist and bayesian.We first look at frequencist.It assume the actual \\(\\theta\\) is an unknown constant (not random).\\[\\min\\limits_{\\hat{\\theta}} E(\\hat{\\theta}(X_1, \\dots, X_n) - \\theta)^2\\]\\[\\hat{\\theta}_{opt} = E(\\theta \\vert X_1, \\dots, X_n) = \\theta\\]\\[\\begin{align}&amp; E(\\hat{\\theta} - \\theta)^2 = E(\\hat{theta} - E(\\hat{\\theta}) + E(\\hat{\\theta}) - \\theta)^2\\\\= &amp; E(\\hat{\\theta} - E(\\hat{\\theta}))^2 + E(E(\\hat{\\theta}) - \\theta)^2 + 2E((\\hat{\\theta} - E(\\hat{\\theta}))(E(\\hat{\\theta})-\\theta))\\\\= &amp; E(\\hat{\\theta} - E(\\hat{\\theta}))^2 + E(E(\\hat{\\theta}) - \\theta)^2\\end{align}\\]The first term is variance, the second term is bias.for example, a typical estimator for mean is\\[\\hat{\\theta}(X_1, \\dots, X_n) = \\dfrac{1}{n} \\sum_{k=1}^{n} X_k\\]assume all \\(E(X_k) = \\theta\\)\\[E(\\hat{\\theta}) = \\theta\\]\\[\\begin{align}&amp; Var(\\hat{\\theta}) = E(\\dfrac{1}{n} \\sum_{k=1}^{n} X_k - \\theta)^2\\\\= &amp; \\dfrac{1}{n^2} E(\\sum_{k=1}^{n} (X_k - \\theta))^2\\\\= &amp; \\dfrac{1}{n^2} \\left( \\sum_{k=1}^{n} E(X_k - \\theta)^2 \\right) + \\dfrac{1}{n^2} \\sum_{i \\ne j} E (X_i - \\theta)(X_j - \\theta)\\end{align}\\]assume \\(E (X_i - \\theta)(X_j - \\theta) = 0\\) when \\(i \\ne j\\)\\[Var(\\hat{\\theta}) = \\dfrac{\\sigma^2}{n}\\]a typical estimator for variance is\\[\\hat{\\theta} = \\dfrac{1}{n-1} \\sum_{k=1}^n (X_k - \\overline{X})^2\\]where\\[\\overline{X} = \\dfrac{1}{n} \\sum_{k=1}^{n} X_k\\]we can check \\(E(\\hat{\\theta})\\)\\[\\begin{align}&amp; (n-1) E(\\hat{\\theta}) = E(\\sum_{k=1}^{n} (X_k - \\overline{X})^2)\\\\= &amp; \\sum_{k=1}^n E(X_k^2 - 2 X_k \\overline{X} + \\overline{X}^2)\\\\= &amp; \\sum_{k=1}^n E(X_k^2) - 2 E\\left((\\sum_{k=1}^n X_k) \\overline{X}\\right) + n E(\\overline{X}^2)\\\\= &amp; \\sum_{k=1}^n E(X_k^2) - 2n E(\\overline{X}^2) + n E(\\overline{X}^2)\\\\= &amp; \\sum_{k=1}^n E(X_k^2) - n E(\\overline{X}^2)\\end{align}\\]\\[E(X_k^2) = \\sigma^2 + \\mu^2\\]\\[E(\\overline{X}^2) = \\dfrac{\\sigma^2}{n} + \\mu^2\\]\\[(n-1)E(\\hat{\\theta}) = n (\\sigma^2 + \\mu^2) - n (\\dfrac{\\sigma^2}{n} + \\mu^2) = (n-1) \\sigma^2\\]conditional variance\\[Var(X \\vert Y) = E\\left( \\left( X - E(X \\vert Y) \\right)^2 \\vert Y \\right)\\]we can show\\[Var(X) = Var( E (X \\vert Y)) + E( Var(X \\vert Y))\\]\\[\\begin{align}&amp; E(X - EX)^2 = E(X - E(X \\vert Y) + E(X \\vert Y) - EX)^2\\\\= &amp; E(X - E(X \\vert Y))^2 + E(E(X \\vert Y) - EX)^2 + 2 E \\left( (X-E(X \\vert Y))(E(X \\vert Y) - EX) \\right)\\end{align}\\]we can show the last term is zero\\[E \\left( (X-E(X \\vert Y))(E(X \\vert Y) - EX) \\right) = E\\left( E \\left( (X-E(X \\vert Y))(E(X \\vert Y) - EX) \\vert Y \\right) \\right) = 0\\]the first term\\[E(X - E(X \\vert Y))^2 = E\\left( E\\left( (X - E(X \\vert Y))^2 \\vert Y \\right) \\right) = E(Var(X \\vert Y))\\]the second term\\[E(E(X \\vert Y) - EX)^2 = E( (E(X \\vert Y) - E(E(X \\vert Y)) )^2 ) = Var(E(X \\vert Y))\\]P3Mean Square Error (MSE)\\[MSE(\\hat{\\theta}) = E(\\hat{\\theta} - \\theta)^2\\]unbias estimator\\[E(\\hat{\\theta}) = \\theta\\]then\\[MSE(\\hat{\\theta}) = Var(\\hat{\\theta})\\]sufficiency\\[f(x,\\theta \\vert s = t) \\text{ is independent of } \\theta\\]for example, \\(X_1, \\dots X_n\\) iid ber(p), each \\(X\\) is either 0 or 1.\\[f(x_1, \\dots, x_n) = p^{\\sum_{k=1}^n x_k} (1-p)^{n - \\sum_{k=1}^n x_k}\\]let \\(s = \\sum_{k=1}^n x_k\\)\\[\\begin{align}&amp; P(x_1, \\dots, x_n \\vert s = t)\\\\= &amp; P(x_1, \\dots, x_n, s = t) / P(s=t)\\\\= &amp; p^t (1-p)^{n-t} / \\binom{n}{t} p^t (1-p)^{n-t}\\\\= &amp; 1 / \\binom{n}{t}\\end{align}\\]Naymen Facterization (without proof)\\[s \\text{ is sufficient} \\quad \\iff \\quad f(x,\\theta) = g(s(x),\\theta)h(x)\\]poisson\\[f(x_1, \\dots, x_n, \\lambda) = \\lambda^{\\sum_{k=1}^{n} x_k} \\exp(-\\lambda) \\prod_{k=1}^{n} \\dfrac{1}{(x_k)!}\\]\\[s = \\sum_{k=1}^{n} x_k\\]gaussianassume \\(\\sigma^2\\) is known, but \\(\\mu\\) is unknown\\[f(x_1, \\dots, x_n, \\mu) =\\]Rao Blackwell Procedureassume \\(\\hat{\\theta}\\) is unbias estimator, then if \\(s\\) is sufficient\\[E(\\hat{\\theta} \\vert s)\\]is also an estimator, it is also unbias, and its MSE is smaller than \\(MSE(\\hat{\\theta})\\), to prove it, first it is an estimator,we didn’t prove it here, but \\(s\\) is sufficient will make sure \\(E(\\hat{\\theta} \\vert s)\\) is not a function of \\(\\theta\\) (without proof).Then it is unbias\\[E(E(\\hat{\\theta} \\vert s )) = E(\\hat{\\theta}) = \\theta\\]then MSE is smaller\\[Var(\\hat{\\theta}) = Var(E(\\hat{\\theta} \\vert s)) + E(Var(\\hat{\\theta} \\vert s))\\]for example, \\(X_1, \\dots, X_n\\) iid \\(N(\\mu, \\sigma^2)\\), \\(\\sigma^2\\) is known, \\(s = \\sum_{k=1}^{n} X_k\\), assume \\(\\hat{\\theta} = X_1\\)\\[E(X_1 \\vert \\sum_{k=1}^{n} X_k) \\text{ is the new estimator}\\]from symmetry\\[E(X_j \\vert \\sum_{k=1}^{n} X_k) = E(X_i \\vert \\sum_{k=1}^{n} X_k)\\]\\[n \\cdot E(X_1 \\vert \\sum_{k=1}^{n} X_k) = E( \\sum_{k=1}^{n} X_k \\vert \\sum_{k=1}^{n} X_k ) = \\sum_{k=1}^{n} X_k\\]\\[E(X_j \\vert \\sum_{k=1}^{n} X_k) = \\dfrac{\\sum_{k=1}^{n} X_k}{n}\\]it is clear the MSE get smaller.completenessSay \\(T\\) is a statistic, it is said to be complete, meaning for every function \\(g\\), if \\(E(g(T)) = 0\\) then \\(g(T) = 0\\).Lehmam Scheffe theoremif \\(T\\) is sufficient and complete, and \\(E(h(T)) = \\theta\\), then \\(h(T)\\) is minimum variance unbias estimatorfor any unbias estimator \\(\\hat{\\theta}\\), we know \\(E(\\hat{\\theta} \\vert T)\\) has smaller or equal MSE.\\[E(h(T) - E(\\hat{\\theta} \\vert T)) = \\theta - \\theta = 0\\]so \\(h(T) = E(\\hat{\\theta} \\vert T)\\), so \\(h(T)\\) is better or equal to any unbias estimator.Craner Rao Lower Boundfor any unbias estimaor \\(\\hat{\\theta}\\)\\[\\theta = \\int_{-\\infty}^{\\infty} \\hat{\\theta(x)} f(x, \\theta) dx\\]\\[\\begin{align}1 &amp;= \\dfrac{\\partial}{\\partial \\theta} \\int_{-\\infty}^{\\infty} \\hat{\\theta}(x) f(x,\\theta) dx\\\\&amp;= \\int_{-\\infty}^{\\infty} \\hat{\\theta}(x) \\dfrac{\\partial}{\\partial \\theta} f(x,\\theta) dx\\end{align}\\]\\[1 = \\int_{-\\infty}^{\\infty} f(x,\\theta) dx\\]\\[0 = \\int_{-\\infty}^{\\infty} \\dfrac{\\partial}{\\partial \\theta} f(x,\\theta) dx\\]\\[0 = \\int_{-\\infty}^{\\infty} \\theta \\dfrac{\\partial}{\\partial \\theta} f(x,\\theta) dx\\]\\[\\begin{align}1 &amp;= \\int_{-\\infty}^{\\infty} (\\hat{\\theta}(x) - \\theta) \\dfrac{\\partial}{\\partial \\theta} f(x,\\theta) dx\\\\&amp;= \\int_{-\\infty}^{\\infty} (\\hat{\\theta}(x) - \\theta) \\left(\\dfrac{\\partial}{\\partial \\theta} \\ln f(x,\\theta)\\right) f(x,\\theta) dx\\end{align}\\]we want to use Cauchy-Schwarz inequality\\[\\left( \\int f(x) g(x) dx \\right)^2 \\le \\int f(x)^2 dx \\int g(x)^2 dx\\]\\[\\begin{align}1^2 &amp;= \\left(\\int_{-\\infty}^{\\infty} (\\hat{\\theta}(x) - \\theta) \\sqrt{f(x,\\theta)} \\left(\\dfrac{\\partial}{\\partial \\theta} \\ln f(x,\\theta)\\right) \\sqrt{f(x,\\theta)} dx\\right)^2\\\\&amp; \\le \\int_{-\\infty}^{\\infty} (\\hat{\\theta}(x) - \\theta)^2 f(x,\\theta) dx \\int_{-\\infty}^{\\infty} \\left(\\dfrac{\\partial}{\\partial \\theta} \\ln f(x,\\theta)\\right)^2 f(x,\\theta) dx\\\\&amp;= E(\\hat{\\theta}(x) - \\theta)^2 \\cdot E \\left(\\dfrac{\\partial}{\\partial \\theta} \\ln f(x,\\theta)\\right)^2\\end{align}\\]\\[E(\\hat{\\theta}(x) - \\theta)^2 \\ge 1 / E \\left(\\dfrac{\\partial}{\\partial \\theta} \\ln f(x,\\theta)\\right)^2\\]where \\(E \\left(\\dfrac{\\partial}{\\partial \\theta} \\ln f(x,\\theta)\\right)^2\\) is called Fisher information.It can be proved that (P3, 1:57:00)\\[E \\left(\\dfrac{\\partial}{\\partial \\theta} \\ln f(x,\\theta)\\right)^2 = -E\\left( \\dfrac{\\partial^2}{\\partial \\theta^2} \\ln f(x,\\theta) \\right)\\]the cauchy is equal if and only if\\[\\dfrac{\\partial}{\\partial \\theta} \\ln f(x,\\theta) = k(\\theta)(\\hat{\\theta}(x)-\\theta)\\]notese until P3 somewherevideo until P6 1:00:00jump to video P10" }, { "title": "Modern Digital Signal Processing", "url": "/posts/modern-dsp/", "categories": "math", "tags": "math", "date": "2024-06-12 03:00:00 +0000", "snippet": "P1independent\\[P(AB) = P(A) \\cdot P(B)\\]conditional probability\\[P(B|A) = P(AB)/P(A)\\]total probability formulaif \\(A_i \\cap A_j = \\emptyset\\) and \\(\\bigcup\\limits_{k=1}^n A_k = \\Omega\\)\\[P(B) = P(B|A_1)P(A_1) + \\dots + P(B|A_n)P(A_n)\\]Bayesian\\[P(B \\vert A) = \\dfrac{P(A \\vert B) P(B)}{P(A)} = \\dfrac{P(A \\vert B)P(B)}{\\sum_{k} P(A \\vert C_k) P(C_k)}\\]Bernoulli distributionBinomial distribution \\(P(x=k) = \\binom{n}{k}p^k (1-p)^{n-k}\\)Possion distribution \\(P(x=k) = \\dfrac{\\lambda^k}{k!}\\exp(-\\lambda)\\)uniform distribution: \\(f(x) = \\dfrac{1}{b-a}I_{[a,b]}(x)\\)exponetional distribution: \\(f(x) = \\lambda \\exp(-\\lambda x)I_{[0,\\infty)}(x)\\)Gaussian distribution: \\(f(x) = \\dfrac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp(-\\dfrac{(x-\\mu)^2}{2\\sigma^2})\\)P2expectation (mean)\\[E(X) = \\int_{-\\infty}^{\\infty} x f(x) dx\\]expectation is linear\\[E\\left(\\sum_{k=1}^{n} X_k\\right) = \\sum_{k=1}^{n} E(X_k)\\]variance\\[Var(X) = E(X-EX)^2 = E(X^2) - (E(X))^2\\]approximation\\[\\min\\limits_{a} E(X-a)^2 = EX\\]\\[\\min\\limits_{g} E(X-g(Y))^2 = E(X \\vert Y)\\]to find the result above, we need conditional expection\\[E(X \\vert Y)\\]conditional expection has several important properties.   \\(E(X \\vert Y)\\) is a random variable.   \\(E\\left(\\sum\\limits_{k=1}^{n} X_k \\vert Y\\right) = \\sum\\limits_{k=1}^{n} E(X_k \\vert Y)\\)   \\(E(E(X\\vert Y)) = E(X)\\)   \\(E(X h(Y) \\vert Y) = h(Y) E(X \\vert Y)\\)now we give the formal prove\\[\\begin{align} &amp; E(X-g(Y))^2 = E(X - E(X \\vert Y) + E(X \\vert Y) - g(Y))^2\\\\= &amp; E(X-E(X \\vert Y))^2 + E(E(X \\vert Y) - g(Y))^2 + 2 E((X - E(X \\vert Y))(E(X \\vert Y) - g(Y)))\\end{align}\\]we only need to show the last term equals zero.\\[\\begin{align}&amp; E((X - E(X \\vert Y))(E(X \\vert Y) - g(Y)))\\\\= &amp; E( E( (X - E(X \\vert Y))(E(X \\vert Y) - g(Y))\\vert Y) ) = 0\\end{align}\\]conditional variance\\[Var(X \\vert Y) = E\\left( \\left( X - E(X \\vert Y) \\right)^2 \\vert Y \\right)\\]we can show\\[Var(X) = Var( E (X \\vert Y)) + E( Var(X \\vert Y))\\]\\[\\begin{align}&amp; E(X - EX)^2 = E(X - E(X \\vert Y) + E(X \\vert Y) - EX)^2\\\\= &amp; E(X - E(X \\vert Y))^2 + E(E(X \\vert Y) - EX)^2 + 2 E \\left( (X-E(X \\vert Y))(E(X \\vert Y) - EX) \\right)\\end{align}\\]we can show the last term is zero\\[E \\left( (X-E(X \\vert Y))(E(X \\vert Y) - EX) \\right) = E\\left( E \\left( (X-E(X \\vert Y))(E(X \\vert Y) - EX) \\vert Y \\right) \\right) = 0\\]the first term\\[E(X - E(X \\vert Y))^2 = E\\left( E\\left( (X - E(X \\vert Y))^2 \\vert Y \\right) \\right) = E(Var(X \\vert Y))\\]the second term\\[E(E(X \\vert Y) - EX)^2 = E( (E(X \\vert Y) - E(E(X \\vert Y)) )^2 ) = Var(E(X \\vert Y))\\]P3Mean Square Error (MSE)\\[MSE(\\hat{\\theta}) = E(\\hat{\\theta} - \\theta)^2\\]unbias estimator\\[E(\\hat{\\theta}) = \\theta\\]then\\[MSE(\\hat{\\theta}) = Var(\\hat{\\theta})\\]sufficiency\\[f(x,\\theta \\vert s = t) \\text{ is independent of } \\theta\\]for example, \\(X_1, \\dots X_n\\) iid ber(p), each \\(X\\) is either 0 or 1.\\[f(x_1, \\dots, x_n) = p^{\\sum_{k=1}^n x_k} (1-p)^{n - \\sum_{k=1}^n x_k}\\]let \\(s = \\sum_{k=1}^n x_k\\)\\[\\begin{align}&amp; P(x_1, \\dots, x_n \\vert s = t)\\\\= &amp; P(x_1, \\dots, x_n, s = t) / P(s=t)\\\\= &amp; p^t (1-p)^{n-t} / \\binom{n}{t} p^t (1-p)^{n-t}\\\\= &amp; 1 / \\binom{n}{t}\\end{align}\\]Naymen Facterization (without proof)\\[s \\text{ is sufficient} \\quad \\iff \\quad f(x,\\theta) = g(s(x),\\theta)h(x)\\]Rao Blackwell Procedureassume \\(\\hat{\\theta}\\) is unbias estimator, then if \\(s\\) is sufficient\\[E(\\hat{\\theta} \\vert s)\\]is also an estimator, it is also unbias, and its MSE is smaller than \\(MSE(\\hat{\\theta})\\), to prove it, first it is an estimator,we didn’t prove it here, but \\(s\\) is sufficient will make sure \\(E(\\hat{\\theta} \\vert s)\\) is not a function of \\(\\theta\\) (without proof).Then it is unbias\\[E(E(\\hat{\\theta} \\vert s )) = E(\\hat{\\theta}) = \\theta\\]then MSE is smaller\\[Var(\\hat{\\theta}) = Var(E(\\hat{\\theta} \\vert s)) + E(Var(\\hat{\\theta} \\vert s))\\]for example, \\(X_1, \\dots, X_n\\) iid \\(N(\\mu, \\sigma^2)\\), \\(\\sigma^2\\) is known, \\(s = \\sum_{k=1}^{n} X_k\\), assume \\(\\hat{\\theta} = X_1\\)\\[E(X_1 \\vert \\sum_{k=1}^{n} X_k) \\text{ is the new estimator}\\]from symmetry\\[E(X_j \\vert \\sum_{k=1}^{n} X_k) = E(X_i \\vert \\sum_{k=1}^{n} X_k)\\]\\[n \\cdot E(X_1 \\vert \\sum_{k=1}^{n} X_k) = E( \\sum_{k=1}^{n} X_k \\vert \\sum_{k=1}^{n} X_k ) = \\sum_{k=1}^{n} X_k\\]\\[E(X_j \\vert \\sum_{k=1}^{n} X_k) = \\dfrac{\\sum_{k=1}^{n} X_k}{n}\\]it is clear the MSE get smaller.completenessSay \\(T\\) is a statistic, it is said to be complete, meaning for every function \\(g\\), if \\(E(g(T)) = 0\\) then \\(g(T) = 0\\).Lehmam Scheffe theoremif \\(T\\) is sufficient and complete, and \\(E(h(T)) = \\theta\\), then \\(h(T)\\) is minimum variance unbias estimatorfor any unbias estimator \\(\\hat{\\theta}\\), we know \\(E(\\hat{\\theta} \\vert T)\\) has smaller or equal MSE.\\[E(h(T) - E(\\hat{\\theta} \\vert T)) = \\theta - \\theta = 0\\]so \\(h(T) = E(\\hat{\\theta} \\vert T)\\), so \\(h(T)\\) is better or equal to any unbias estimator.Craner Rao Lower Boundfor any unbias estimaor \\(\\hat{\\theta}\\)\\[MSE(\\hat{\\theta}) \\ge 1/I(\\theta)\\]where \\(I(\\theta)\\) is fisher information\\[1/I(\\theta) = E \\left(\\dfrac{\\partial}{\\partial \\theta} \\ln f(x,\\theta)\\right)^2 = -E\\left( \\dfrac{\\partial^2}{\\partial \\theta^2} \\ln f(x,\\theta) \\right)\\]P4one dimenitonal \\(X\\) one dimentional \\(Y\\)\\[\\min_{\\alpha} E(Y-\\alpha X)^2 = (EX^2)^{-1} E(XY)\\]multi dimention \\(X\\) and one dimention \\(Y\\)\\[\\begin{align}&amp; Y \\in \\mathbb{R}, \\quad X \\in \\mathbb{R}^n, \\quad \\alpha \\in \\mathbb{R}^n\\\\&amp; \\min_{\\alpha} E(Y - \\alpha^{\\intercal}X) = (E(X X^{\\intercal}))^{-1} E(XY)\\end{align}\\]continuous \\(X\\) and one dimention \\(Y\\)\\[\\begin{align}&amp; \\min_{h} E\\left(Y(t) - \\int_{-\\infty}^{\\infty}h(t-\\tau) X(\\tau) d\\tau\\right)^2\\\\&amp; H_{opt}(\\omega) = \\left( S_X(\\omega) \\right)^{-1} S_{XY}(\\omega)\\end{align}\\]P5the above\\[\\begin{align}&amp; \\min_{h} E\\left(Y(t) - \\int_{-\\infty}^{\\infty}h(t-\\tau) X(\\tau) d\\tau\\right)^2\\\\&amp; H_{opt}(\\omega) = \\left( S_X(\\omega) \\right)^{-1} S_{XY}(\\omega)\\end{align}\\]cannot gaurantee causal \\(h\\)." }, { "title": "SPICE", "url": "/posts/spice/", "categories": "analog-circuit", "tags": "analog", "date": "2024-03-12 03:00:00 +0000", "snippet": "BRIDGE-T CIRCUIT*VBIAS 1 0 12R1 1 2 10R2 2 0 10R3 2 3 5R4 1 3 5*.OP.ENDngspicesource bridge.cktlistingopprint v(1)print allquitby default the current flowing from the positive terminal to negative terminal is positive current.SPICE title cardx1 1 2 pvcellx2 2 0 pvcellx3 1 3 pvcellx4 3 0 pvcelliload 1 0.subckt pvcell 1 3rs 1 2 0.01isc 3 2 1.5d1 2 3 pnjuncrp 2 3 5.model pnjunc d is=1e-9 vj=0.65.ends pvcell.dc iload 0 3 0.01.controlrunplot v(1).endc.endngspicesource 1.cktTED# run a specific testpytest -s -vv some_file.py::test1# run all tests in a filepytest -s -vv some_file.py# run all tests inside a directorypytest -s -vv some_dir# run in parallelpytest -n 5 -s -vv some_dir" }, { "title": "High Frequency Design Technique Explanation", "url": "/posts/high-frequency-design-technique-explanation/", "categories": "EM", "tags": "EM", "date": "2023-09-27 06:00:00 +0000", "snippet": "IntroductionAC AnalysisFor a total impedance \\(Z(s)\\) consists of \\(R, L\\) and \\(C\\), assume all the initial conditions of \\(L\\) and \\(C\\) are \\(0\\), then\\[V(s) = I(s)Z(s)\\]assume when \\(t\\ge 0\\), we apply current \\(i(t)=I \\cos(\\omega t + \\varphi)\\), then the voltage response can be calculated by\\[v(t) = \\int_0^t z(\\tau) I \\cos\\left(\\omega(t-\\tau) + \\varphi\\right)d\\tau\\]we are interested in the response when \\(t\\to\\infty\\).Instead of directly calculating the above integration, we calculate\\[y(t) = I \\int_0^t z(\\tau)e^{j\\omega(t-\\tau)+\\varphi}d\\tau = e^{j\\omega t} I e^{j\\varphi} \\int_0^t z(\\tau)e^{-j\\omega \\tau} d\\tau\\]when \\(t \\to \\infty\\)\\[y(t) \\to e^{j\\omega t} I e^{j\\varphi} \\int_0^\\infty z(\\tau)e^{-j\\omega \\tau}d\\tau = e^{j\\omega t} I e^{j\\varphi} Z(j\\omega)\\]then\\[\\begin{cases}v(t) = \\mathrm{Re}\\left(y(t)\\right) \\to V \\cos(\\omega t + \\theta)\\\\V e^{j\\theta} = I e^{j\\varphi} \\cdot Z(j\\omega)\\end{cases}\\]Average Power ConsumptionAssume\\[\\begin{cases}v(t) = V \\cos(\\omega t)\\\\i(t) = I \\cos(\\omega t + \\theta)\\end{cases}\\]\\[\\overline{P} = \\dfrac{1}{T} \\int_0^{T} VI\\cos(\\omega t) \\cos(\\omega t + \\theta) dt =\\]Plane WaveIn Lossless MediaExample 1assume \\(E_y = E_z = \\dfrac{\\partial E_x}{\\partial x} = \\dfrac{\\partial E_x}{\\partial y} = 0\\)\\[\\dfrac{d^2 E_x(z)}{dz^2} + k^2 E_{x}(z) = 0\\]\\[E_x(z) = E_0^+ e^{-jkz} + E_0^{-} e^{jkz}\\]Then the magnatic filed can be derived from Maxwell’s equation, we don’t have to solve the differential equation again\\[\\nabla \\times \\vec{E} = -j\\omega\\mu \\vec{H}\\]For example, if \\(E_x(z) = E_0^+ e^{-jkz} = E_x^+\\)\\[\\begin{cases}H_x^+ = 0\\\\H_y^+ = \\dfrac{k}{\\omega \\mu}E_x^+ = \\dfrac{1}{\\eta} E_x^+\\\\H_z^+ = 0\\end{cases}\\]where\\[\\eta = \\sqrt{\\dfrac{\\mu}{\\eta}}\\]" }, { "title": "High Frequency Design Technique", "url": "/posts/high-frequency-design-technique/", "categories": "EM", "tags": "EM", "date": "2023-09-27 05:00:00 +0000", "snippet": "IntroductionWhat Are High FrequencyOur familiar lumped circuit analysis is only an approximation, which is approximately valid when the circuit dimensions are less than \\(\\lambda/10\\). \\(f\\) \\(\\lambda\\) \\(\\lambda/10\\) 1 GHz 30 cm 3cm 10 GHz 3 cm 3 mm 100 GHz 3 mm 300 um AC AnalysisFor a impedance \\(Z(s)\\) consists of \\(R, L\\) and \\(C\\), if we apply a current \\(i(t)=I \\cos(\\omega t + \\varphi)\\), then the voltage “response” can be understand as\\[v(t) \\sim V \\cos(\\omega t + \\theta)\\]with\\[V e^{j \\theta} = I e^{j \\varphi} \\cdot Z(j\\omega)\\]Average Power ConsumptionAssume\\[\\begin{cases}v(t) = V \\cos(\\omega t + \\varphi)\\\\i(t) = I \\cos(\\omega t + \\theta)\\end{cases}\\]\\[\\overline{P} = \\dfrac{1}{T} \\int_0^{T} VI\\cos(\\omega t + \\varphi) \\cos(\\omega t + \\theta) dt = \\dfrac{VI}{2} \\cos(\\theta-\\varphi)\\]In phaser representation\\[\\overline{P} = \\dfrac{1}{2} \\mathrm{Re}(V I^*)\\]Maximum Power TransferAssume a AC voltage source \\(V_G \\cos(\\omega t)\\) with internal impedance \\(Z_G = R_G + j X_G\\). What load impedance will results maximum average power on the load?\\[\\overline{P_L} = \\dfrac{1}{2}\\mathrm{Re}\\left(\\dfrac{V_G(R_L+j X_L)}{R_G+jX_G + R_L + jX_L} \\cdot \\dfrac{V_G}{R_G-j X_G + R_L - j X_L}\\right)\\]we need to maximax\\[\\dfrac{R_L}{(R_G+R_L)^2+(X_G+X_L)^2}\\]so the maximum power transfer happens when\\[\\begin{cases}R_L = R_G\\\\X_L = -X_G\\end{cases}\\]Maxwell’s Equations\\[\\begin{cases}\\nabla \\cdot \\vec{D} = \\rho\\\\\\nabla \\cdot \\vec{B} = 0\\\\\\nabla \\times \\vec{E} = -\\dfrac{\\partial \\vec{B}}{\\partial t}\\\\\\nabla \\times \\vec{H} = \\dfrac{\\partial \\vec{D}}{\\partial t} + \\vec{J}\\\\\\vec{D} = \\epsilon \\vec{E}\\\\\\vec{H} = \\vec{B}/\\mu\\\\\\nabla \\cdot \\vec{J} + \\dfrac{\\partial \\rho}{\\partial t} = 0\\\\\\vec{J} = \\rho \\vec{u}\\end{cases}\\]In fact, the two divergence equation can be derived from the two curl equation (how?).Lorentz’s Force\\[\\vec{F} = q(\\vec{E} + \\vec{u}\\times\\vec{B})\\]Boundary Condition\\[\\begin{align}E_{1t} = E_{2t}\\\\\\dots\\end{align}\\]For boundary condition between dielectric and conductor, \\(\\hat{a_n}\\) points fron conductor to dielectric.Wave Equation\\[\\dfrac{1}{\\mu\\epsilon}\\nabla^2 \\vec{E} = \\dfrac{\\partial^2 \\vec{E}}{\\partial t^2}\\]\\[\\dfrac{1}{\\mu\\epsilon}\\nabla^2 \\vec{H} = \\dfrac{\\partial^2 \\vec{H}}{\\partial t^2}\\]\\[c^2 = \\dfrac{1}{\\mu\\epsilon}\\]Example\\[c^2 \\dfrac{\\partial^2 f}{\\partial x^2} = \\dfrac{\\partial^2 f}{\\partial t^2}\\]From d’Alembert’s solution, the general solution is given by\\[f(x,t) = F(kx+\\omega t) + G(kx-\\omega t)\\]where \\(\\omega/k = c\\)Time Harmonic Field Steady State Solution\\[\\vec{E}(x,y,z;t) = \\mathrm{Re}\\left(\\vec{E}_{0}(x,y,z) e^{j\\omega t}\\right)\\]Similar to the phasor in AC analysis, assume each component of the vector field is a phasor (having the same time frequency).\\[\\begin{cases}\\nabla \\times \\vec{E} = -j\\omega \\mu \\vec{H}\\\\\\nabla \\times \\vec{H} = \\vec{J} + j\\omega\\epsilon \\vec{E}\\\\\\nabla \\cdot \\vec{E} = \\dfrac{\\rho}{\\epsilon}\\\\\\nabla \\cdot \\vec{H} = 0\\end{cases}\\]then for source free wave equation\\[\\nabla^2 \\vec{E} + k^2 \\vec{E} = 0\\]where \\(k^2 = \\omega^2 \\mu\\epsilon\\)In the case of wave in conductor, we can define the compex permittivity\\[\\epsilon_c = \\epsilon - j \\dfrac{\\sigma}{\\omega}\\]where \\(\\vec{J} = \\sigma \\vec{E}\\) (Ohms law).Plane WaveIn Lossless Media\\[\\nabla^2 \\vec{E} + k^2 \\vec{E} = 0\\]\\[\\begin{cases}\\left(\\dfrac{\\partial^2}{\\partial x^2} + \\dfrac{\\partial^2}{\\partial y^2} + \\dfrac{\\partial^2}{\\partial z^2} + k^2\\right) E_x(x,y,z) = 0\\\\\\left(\\dfrac{\\partial^2}{\\partial x^2} + \\dfrac{\\partial^2}{\\partial y^2} + \\dfrac{\\partial^2}{\\partial z^2} + k^2\\right) E_y(x,y,z) = 0\\\\\\left(\\dfrac{\\partial^2}{\\partial x^2} + \\dfrac{\\partial^2}{\\partial y^2} + \\dfrac{\\partial^2}{\\partial z^2} + k^2\\right) E_z(x,y,z) = 0\\end{cases}\\]Assume \\(E_x(x,y,z) = X(x)Y(y)Z(z)\\)\\[\\left(\\dfrac{\\partial^2}{\\partial x^2} + \\dfrac{\\partial^2}{\\partial y^2} + \\dfrac{\\partial^2}{\\partial z^2} + k^2\\right) X(x)Y(y)Z(z) = 0\\]\\[\\dfrac{X''}{X} + \\dfrac{Y''}{Y} + \\dfrac{Z''}{Z} + k^2 = 0\\]\\[k_x^2 + k_y^2 + k_z^2 = k^2\\]\\[E_x(x,y,z) = A e^{-j k_x x} B e^{-j k_y y} C e^{-j k_z z} = E_{x0} e^{-j \\vec{k} \\cdot \\vec{R}}\\]Assume for \\(E_y, E_z\\) they have the same \\(\\vec{k}\\) as \\(E_x\\).\\[\\vec{E}(x,y,z) = \\vec{E_0} e^{-j \\vec{k}\\cdot\\vec{R}}\\]Does it allow different \\(k_x,k_y,k_z\\) for different components?Assume source free space\\[\\nabla \\cdot \\vec{E} = 0\\]\\[\\vec{E} = \\vec{E_0} e^{-j \\vec{k}\\cdot\\vec{R}}\\]\\[\\vec{k} \\cdot \\vec{E} = 0\\]The magnetic field can be calculated from\\[\\begin{align}\\vec{H} &amp;= -\\dfrac{1}{j\\omega \\mu} \\nabla \\times \\left( \\vec{E_0} e^{-j \\vec{k}\\cdot \\vec{R}} \\right)\\\\&amp;= \\dfrac{1}{-j\\omega\\mu} \\nabla\\left(e^{-j \\vec{k}\\cdot\\vec{R}}\\right)\\times \\vec{E_0}\\\\&amp;= \\dfrac{-j\\vec{k}}{-j\\omega\\mu}e^{-j\\vec{k}\\cdot\\vec{R}} \\times \\vec{E_0}\\\\&amp;= \\dfrac{\\vec{k} \\times \\vec{E}}{\\omega\\mu} = \\dfrac{1}{\\eta} \\hat{a_n} \\times \\vec{E}\\end{align}\\]This is typical TEM wave, where the field is orthgonal to travel direction.PolarizationAssume\\[\\begin{align}\\vec{E}(z) &amp;= \\left(\\hat{a_x} E_x + \\hat{a_y}E_y\\right) e^{-jkz}\\\\&amp;= \\left(\\hat{a_x} E_{x0} e^{-j\\phi_{x0}} + \\hat{a_y} E_{y0} e^{-j\\phi_{y0}}\\right) e^{-jkz}\\end{align}\\]Dependent on the values of \\(\\phi_{x0}, \\phi_{y0}\\) and \\(E_{x0}, E_{y0}\\), it will show different polarization.In Lossy Media\\[\\nabla^2 \\vec{E} + k_c^2 \\vec{E} = 0\\]\\[k_c = \\omega \\sqrt{\\mu \\epsilon_c}\\]\\[\\begin{align}\\gamma &amp;= jk_c = j\\omega \\sqrt{\\mu\\epsilon}\\left(1+\\dfrac{\\sigma}{j\\omega\\epsilon}\\right)^{1/2}\\\\&amp;= \\alpha + j\\beta\\end{align}\\]\\[\\nabla^2 \\vec{E} - \\gamma^2 \\vec{E} = 0\\]\\[\\vec{E} = \\hat{a_x} E_0 e^{-\\gamma z} = \\hat{a_x} E_0 e^{-\\alpha z} e^{-j\\beta z}\\]Low-Loss DielectricsGood ConductorsGroup Velocityphase velocity\\[u_p = \\dfrac{\\omega}{\\beta}\\]Dispersion: the situation when the phase velocity is different for different frequency.Assume two frequency \\(\\omega_0 \\pm \\Delta \\omega\\) with the same amptitude.\\[\\begin{align}E(z,t) &amp;= E_0 \\cos\\left[(\\omega_0+\\Delta\\omega)t - (\\beta_0+\\Delta\\beta)z\\right] + E_0\\cos\\left[(\\omega_0-\\Delta\\omega)t - (\\beta_0-\\Delta\\beta)z\\right]\\\\&amp;= 2E_0\\cos(\\Delta \\omega t - \\Delta \\beta z) \\cos(\\omega_0 t - \\beta_0 z)\\end{align}\\]This is a carrier at frequency \\(\\omega_0\\) and an envelop at frequency \\(\\Delta\\omega\\).The phase velocity of carrier is\\[u = \\dfrac{\\omega_0}{\\beta_0}\\]The phase velocity of envelop is\\[u = \\dfrac{\\Delta \\omega}{\\Delta \\beta}\\]We call it group velocity\\[u_g = \\dfrac{1}{d\\beta/d\\omega}\\]\\[u_g = \\dfrac{u_p}{1-\\dfrac{\\omega}{u_p} \\dfrac{du_p}{d\\omega}}\\]Poynting Vector\\[\\begin{align}&amp;\\nabla \\cdot (\\vec{E} \\times \\vec{H}) = \\vec{H} \\cdot (\\nabla \\times \\vec{E}) - \\vec{E} \\cdot (\\nabla \\times \\vec{H})\\\\=&amp; -\\dfrac{\\partial}{\\partial t} \\left(\\dfrac{1}{2}\\epsilon E^2 + \\dfrac{1}{2}\\mu H^2\\right) - \\sigma E^2\\end{align}\\]  \\(\\vec{E}\\times\\vec{H}\\) (Poynting vector) is the total power flow.In the case of steady state sinosoidal analysis, the average power flow\\[P_{\\mathrm{av}} = \\dfrac{1}{2}\\mathrm{Re}(\\vec{E}\\times \\vec{H}^*)\\]Normal Incidence At A Plane Conducting BoundaryAssume the wave travel from \\(\\sigma=0\\) to \\(\\sigma=\\infty\\).Incident wave\\[\\begin{cases}\\vec{E_i}(z) = \\hat{a_x} E_{i0} e^{-j\\beta_{1}z}\\\\\\vec{H_i}(z) = \\hat{a_y} \\dfrac{E_{i0}}{\\eta_1} e^{-j\\beta_1 z}\\end{cases}\\]Reflected wave\\[\\vec{E_r} = \\hat{a_x} E_{r0} e^{+j\\beta_1 z}\\]Total \\(\\vec{E}\\) in medium 1\\[\\vec{E_i} + \\vec{E_r} = \\hat{a_x} E_{i0} e^{-j\\beta_{1}z} + \\hat{a_x} E_{r0} e^{+j\\beta_1 z}\\]using boundary condition for perfect conductor\\[E_{r0} = - E_{i0}\\]  \\(\\vec{H_r}\\) can get from \\(\\vec{E_r}\\).Oblique Incidence At A Plane Conducting BoundaryPerpendicular PolarizationFor the components that \\(\\vec{E_i}\\) is perpendicular to plane of incidence.We guess the reflected wave \\(\\vec{E_r}\\) is also perpendicular to the plane of incidence.\\[\\hat{a_{ni}} = \\hat{a_x}\\sin\\theta_i + \\hat{a_z} \\cos\\theta_i\\]\\[\\vec{E_i} = \\hat{a_y} E_{i0} e^{-j\\beta_1 (x\\sin\\theta_i + z\\cos\\theta_i)}\\]Incidence At A Plane DielectricWaveguidesUniform Guiding Structure\\[\\vec{E}(x,y,z,t) = \\mathrm{Re}\\left\\{ \\vec{E_0}(x,y) e^{-\\gamma z} e^{j\\omega t}\\right\\}\\]\\[(\\nabla^2 + k^2) \\vec{E} = 0\\]\\[\\nabla^2 = \\nabla_{xy}^2 + \\gamma^2 = \\nabla_{t}^2 + \\gamma^2\\]\\[\\left(\\nabla_{xy}^2 + \\gamma^2 + k^2\\right) \\vec{E} = 0\\]where \\(t\\) means transverse (TE,TM,TEM), which means orthogonal to something.Note that the solution may not be plane wave, which means the \\(\\vec{E}, \\vec{H}\\) may not be orthogonal to travelling direction.So we need to solve all the components \\(E_x, E_y, E_z, H_x, H_y, H_z\\).Assuming source free in the waveguide (not on the surface)\\[\\begin{cases}\\nabla \\times \\vec{E} = -j\\omega\\mu\\vec{H}\\\\\\nabla \\times \\vec{H} = -j\\omega\\epsilon \\vec{E}\\end{cases}\\]Once we solved \\(E_z, H_z\\), the other four components\\[\\begin{cases}H_x = -\\dfrac{1}{h^2}\\left(\\gamma \\dfrac{\\partial H_z}{\\partial x} - j\\omega\\epsilon \\dfrac{\\partial E_z}{\\partial y}\\right)\\\\\\vdots\\end{cases}\\]where \\(h^2 = \\gamma^2 + k^2\\) TEM waves: \\(E_z=H_z=0\\). TM waves: \\(H_z=0\\). TE waves: \\(E_z=0\\).Is it possible \\(E_z\\ne 0, H_z\\ne 0\\)?Parallel-Plate WaveguideAssume \\(w \\gg b\\).TEMAssume TEM mode, and assume a plane wave\\[\\begin{cases}\\vec{E} = \\hat{a_y} E_0 e^{-\\gamma z}\\\\\\vec{H} = -\\hat{a_x} \\dfrac{E_0}{\\eta} e^{-\\gamma z}\\end{cases}\\]where\\[\\begin{cases}\\gamma = j\\beta = j\\omega\\sqrt{\\mu\\epsilon}\\\\\\eta = \\sqrt{\\dfrac{\\mu}{\\epsilon}}\\end{cases}\\]Since we already know such wave satisfy Maxwell’s equation,we need to further check the boundary condition \\(y=0, y=b\\) for the plane wave.\\[\\begin{cases}E_t = 0\\\\H_n = 0\\end{cases}\\]and the boundary condition is satisfied.So we find a simple solution.The frequency \\(\\omega\\) can be any frequency.The parallel plate will have surface charge and surface current.\\[\\begin{cases}\\rho = \\hat{a_n} \\cdot \\vec{D}\\\\\\vec{J} = \\hat{a_n} \\times \\vec{H}\\end{cases}\\]for \\(y=0\\), \\(\\hat{a_n} = +\\hat{a_y}\\).\\[\\begin{cases}\\rho = \\epsilon E_0 e^{-j\\beta z}\\\\\\vec{J} = \\hat{a_z} \\dfrac{E_0}{\\eta} e^{-j\\beta z}\\end{cases}\\]TMAssume \\(E_z(x,y,z)\\) doesn’t depends on \\(x\\), since \\(x\\) is wide enough.\\[E_z(x,y,z) = E_z^0(y)e^{-\\gamma z}\\]\\[\\left(\\nabla_{xy}^2 + h^2\\right) \\vec{E} = 0\\]\\[\\dfrac{d^2}{dy^2} E_{z}^0 (y) + h^2 E_z^0 (y) = 0\\]with boundary condition\\[E_z^0(y) \\vert_{y=0,y=b} = 0\\]\\[E_z^0(y) = A_n \\sin \\dfrac{n\\pi y}{b}\\]\\[h = \\dfrac{n\\pi}{b}\\]then all the other components can be calculated by the equations.\\[\\gamma = \\sqrt{h^2-k^2} = \\sqrt{\\left(\\dfrac{n\\pi}{b}\\right)^2 - \\omega^2\\mu\\epsilon}\\]Dielectric WaveguideAntennasFor static field (static \\(\\rho, \\vec{J}\\)).\\[\\begin{cases}\\nabla \\times \\vec{E} = 0 \\quad \\implies \\quad \\vec{E} = -\\nabla V\\\\\\nabla \\cdot \\vec{B} = 0 \\quad \\implies \\quad \\vec{B} = \\nabla \\times \\vec{A}\\end{cases}\\]\\[\\begin{cases}V = \\int \\dfrac{1}{4\\pi\\epsilon} \\dfrac{\\rho dv}{R}\\\\\\vec{A} = \\int \\dfrac{\\mu}{4\\pi} \\dfrac{\\vec{J} dv}{R}\\end{cases}\\]For time varying field\\[\\nabla \\times \\vec{E} = -\\dfrac{\\partial \\vec{B}}{\\partial t} = -\\dfrac{\\partial}{\\partial t} \\left(\\nabla \\times \\vec{A}\\right)\\]\\[\\vec{E} = \\dfrac{1}{j\\omega\\epsilon} \\nabla \\times \\vec{H}\\]\\[\\nabla \\times \\left(\\vec{E} + \\dfrac{\\partial \\vec{A}}{\\partial t}\\right) = 0\\]\\[\\vec{E} + \\dfrac{\\partial \\vec{A}}{\\partial t} = -\\nabla V\\]\\[\\vec{E} = -\\nabla V - \\dfrac{\\partial \\vec{A}}{\\partial t}\\]\\[\\nabla \\times \\left(\\dfrac{\\nabla \\times \\vec{A}}{\\mu}\\right) = \\vec{J} + \\dfrac{\\partial}{\\partial t} \\left[\\epsilon \\left(-\\nabla V - \\dfrac{\\partial \\vec{A}}{\\partial t}\\right)\\right]\\]\\[\\begin{align}\\nabla \\times \\left(\\nabla \\times \\vec{A}\\right) &amp;= \\mu \\vec{J} + \\mu\\epsilon \\dfrac{\\partial}{\\partial t}(-\\nabla V - \\dfrac{\\partial \\vec{A}}{\\partial t})\\\\&amp;= \\nabla \\left(\\nabla \\cdot \\vec{A}\\right) - \\nabla^2 \\vec{A}\\end{align}\\]\\[\\nabla^2 \\vec{A} - \\mu\\epsilon \\dfrac{\\partial^2 \\vec{A}}{\\partial t^2} = -\\mu \\vec{J} + \\nabla\\left(\\nabla\\cdot \\vec{A} + \\mu\\epsilon\\dfrac{\\partial V}{\\partial t}\\right)\\]Let’s define\\[\\begin{cases}\\nabla \\times \\vec{A} = \\vec{B}\\\\\\nabla \\cdot \\vec{A} + \\mu\\epsilon\\dfrac{\\partial V}{\\partial t} = 0 \\quad \\text{(Lorentz condition)}\\end{cases}\\]\\[\\nabla^2 \\vec{A} - \\mu\\epsilon\\dfrac{\\partial^2 \\vec{A}}{\\partial t^2} = -\\mu \\vec{J}\\]\\[\\begin{align}- \\nabla \\cdot \\left(\\nabla V\\right) &amp;= \\nabla \\cdot \\left(\\vec{E}+\\dfrac{\\partial \\vec{A}}{\\partial t}\\right)\\\\&amp;= \\dfrac{\\rho}{\\epsilon} + \\dfrac{\\partial}{\\partial t} \\left(\\nabla \\cdot \\vec{A}\\right)\\\\&amp;= \\dfrac{\\rho}{\\epsilon} + \\dfrac{\\partial}{\\partial t} \\left(-\\mu\\epsilon \\dfrac{\\partial V}{\\partial t}\\right)\\\\&amp;= -\\nabla^2 V\\end{align}\\]\\[\\nabla^2 V - \\mu\\epsilon \\dfrac{\\partial^2 V}{\\partial t^2} = -\\dfrac{\\rho}{\\epsilon}\\]the solution is given by\\[V(\\vec{R},t) = \\dfrac{1}{4\\pi\\epsilon} \\int \\dfrac{\\rho \\left(\\vec{R'}, t-\\dfrac{\\vert \\vec{R} - \\vec{R}' \\vert}{u}\\right)}{\\vert \\vec{R}-\\vec{R'} \\vert} dv\\]where \\(u = \\dfrac{1}{\\sqrt{\\mu\\epsilon}}\\).For sinosoidal steady state\\[\\vec{A}(\\vec{R}, t) = \\dfrac{\\mu}{4\\pi} \\int \\dfrac{\\mathrm{Re}\\left\\{\\vec{J}(\\vec{R'}) e^{j\\omega(t-\\Delta t)}\\right\\}}{\\vert \\vec{R'} - \\vec{R} \\vert} dv\\]In phasor form\\[\\vec{A} = \\dfrac{\\mu}{4\\pi}\\int \\dfrac{\\vec{J} \\cdot e^{-jkR}}{R} dv\\]\\[V = \\dfrac{1}{4\\pi\\epsilon}\\int \\dfrac{\\rho e^{-jkR}}{R} dv\\]Hertizen Dipoles\\[i(t) = I\\cos\\omega t = \\dfrac{d q(t)}{dt}\\]\\[I=j\\omega Q\\]\\[\\vec{P} = \\hat{a_z} \\cdot dl \\cdot Q = \\hat{a_z} \\dfrac{I \\cdot dl}{j\\omega}\\]\\[\\begin{align}\\vec{A} &amp;= \\dfrac{\\mu}{4\\pi} \\dfrac{\\vec{J} e^{-jkR} dv}{R}\\\\&amp;= \\hat{a_z} \\dfrac{\\mu}{4\\pi} \\dfrac{I dl \\cdot e^{-j\\beta R}}{R}\\\\&amp;= \\hat{a_R} A_R + \\hat{a_\\theta} A_\\theta\\end{align}\\]\\[\\vec{H} = \\dfrac{1}{\\mu} \\nabla \\times \\vec{A}\\]Transmission Lines\\[-\\dfrac{\\partial v(z,t)}{\\partial z} = R i(z,t) + L \\dfrac{\\partial i(z,t)}{\\partial t}\\]\\[-\\dfrac{\\partial i(z,t)}{\\partial z} = G v(z,t) + C \\dfrac{\\partial v(z,t)}{\\partial t}\\]\\[\\begin{cases}-\\dfrac{d V(z)}{dz} = (R+j\\omega L) I(z)\\\\-\\dfrac{d I(z)}{dz} = (G + j\\omega C) V(z)\\end{cases}\\]\\[\\begin{cases}\\dfrac{d^2 V(z)}{dz^2} = \\gamma^2 V(z)\\\\\\dfrac{d^2 I(z)}{dz^2} = \\gamma^2 V(z)\\end{cases}\\]\\[\\gamma = \\alpha + j\\beta = \\sqrt{(R+j\\omega L)(G+j\\omega C)}\\]\\[Z_0 = \\dfrac{R+j\\omega L}{\\gamma} = \\dfrac{\\gamma}{G+j\\omega C} = \\sqrt{\\dfrac{R+j\\omega L}{G+j\\omega C}}\\]Finite Transmission Lines\\[\\begin{cases}V(z) = V_0^+ e^{-\\gamma z} + V_0^- e^{+\\gamma z}\\\\I(z) = I_0^+ e^{-\\gamma z} + I_0^- e^{+\\gamma z}\\end{cases}\\]\\[\\dfrac{V_0^+}{I_0^+} = - \\dfrac{V_0^-}{I_0^-} = Z_0\\]Boundary condition at \\(z = l\\)\\[\\dfrac{V(l)}{I(l)} = \\dfrac{V_L}{I_L} = Z_L\\]we can imagine if \\(Z_L = Z_0\\), then with only \\(V_0^+\\), it can satisfy the boundary condition, such that there is no refection wave.\\[\\begin{cases}V(l) = V_L = V_0^+ e^{-\\gamma l} + V_0^- e^{+\\gamma l}\\\\I(l) = I_L = \\dfrac{V_0^+}{Z_0} e^{-\\gamma l} - \\dfrac{V_0^-}{Z_0} e^{+\\gamma l}\\end{cases}\\]\\[\\begin{cases}V(z) = \\dfrac{I_L}{2} \\left[(Z_L+Z_0)e^{\\gamma (l-z)} + (Z_L-Z_0)e^{-\\gamma(l-z)}\\right]\\\\I(z) = \\dfrac{I_L}{2Z_0} \\left[(Z_L+Z_0)e^{\\gamma (l-z)} - (Z_L-Z_0)e^{-\\gamma(l-z)}\\right]\\end{cases}\\]\\[\\begin{cases}V(z') = I_L (Z_L \\cosh \\gamma z' + Z_0 \\sinh \\gamma z')\\\\I(z') = \\dfrac{I_L}{Z_0} (Z_L \\sinh \\gamma z' + Z_0 \\cosh \\gamma z')\\end{cases}\\]\\[Z_i = Z_0 \\dfrac{Z_L + Z_0 \\tanh \\gamma l}{Z_0 + Z_L \\tanh \\gamma l}\\]Transmission Line As Circuit ElementsFor lossless transmission line\\[Z_i = Z_0 \\dfrac{Z_L + j Z_0 \\tan\\beta l}{Z_0 + j Z_L \\tan\\beta l}\\]If \\(Z_L \\to \\infty\\)\\[Z_i = -j Z_0 \\cot \\beta l\\]If \\(Z_L = 0\\)\\[Z_i = j Z_0 \\tan\\beta l\\]Quarter-wave section: \\(l = \\lambda/4, \\beta l = \\pi/2\\).\\[Z_i = \\dfrac{Z_0^2}{Z_L}\\]also called quarter wave transformer.Half-wave section: \\(l = \\lambda/2, \\beta l = \\pi\\).\\[Z_i = Z_L\\]Lines With Resistive Termination\\[\\begin{align}V(z') &amp;= \\dfrac{I_L}{2} (Z_L + Z_0) e^{\\gamma z'} \\left[1 + \\dfrac{Z_L - Z_0}{Z_L + Z_0}e^{-2\\gamma z'}\\right]\\\\&amp;= \\dfrac{I_L}{2}(Z_L + Z_0) e^{\\gamma z'} [1 + \\Gamma e^{-2\\gamma z'}]\\end{align}\\]\\[\\Gamma = \\dfrac{Z_L - Z_0}{Z_L + Z_0}\\]Transmission Line Circuits\\[V(z') = \\dfrac{Z_0 V_g}{Z_0 + Z_g} e^{-\\gamma z} \\left(\\dfrac{1+\\Gamma e^{-2\\gamma z'}}{1-\\Gamma_g \\Gamma e^{-2\\gamma l}}\\right)\\]The Smith ChartLossless Transmission Line.\\[\\Gamma = \\dfrac{z_L - 1}{z_L + 1}\\]\\[z_L = \\dfrac{1+\\Gamma}{1-\\Gamma}\\]plot \\(\\Gamma\\) on the complex plane.Now 9.6(1/2)LC Resonance and Matching NetworksLC ResonanceFor series-connected LC circuit, when the reactance magnitudes of \\(L\\) and \\(C\\) are equal, the pair resonates.\\[\\omega_0 = \\dfrac{1}{\\sqrt{LC}}\\]Series Circuit Quality FactorsImpedance \\(Z\\), resistance \\(R\\) and reactance \\(X\\).Q of Inductors and CapacitorsWe can model real inductor as the combination of resistance and reactance\\[Z_L = R_L + j X_L\\]\\[Z_C = R_C + j X_C\\]For series \\(L\\) and \\(R_L\\)\\[Q = \\dfrac{X_L}{R_L} = \\dfrac{\\omega L}{R_L}\\]For series \\(C\\) and \\(R_C\\)\\[Q = \\dfrac{X_C}{R_C} = \\dfrac{1}{\\omega C R_C}\\]Unloaded QThe above apply to inductor or capacitor only.If we connect such real inductor and capacitor in series, it seems we get two different \\(Q\\)\\[Q_1 = \\dfrac{\\omega L}{R_L + R_C}\\]\\[Q_2 = \\dfrac{1}{\\omega C (R_L+R_C)}\\]Which one should we use? Actually at frequency \\(\\omega_0 = 1/\\sqrt{LC}\\)\\[Q_U = \\dfrac{\\omega_0 L}{R_L+R_C} = \\dfrac{1}{\\omega_0 C(R_L+R_C)}\\]the \\(Q_U\\) is called as unloaded Q.Loaded QThen, the series connected real inductor and capacitor, and connected to some resistors and sources (including their internal resistance). Naturally, the loaded Q is defined as\\[Q_L = \\dfrac{\\omega_0 L}{Z_{\\text{others}} + R_L + R_C} = \\dfrac{1}{\\omega_0 C (Z_{\\text{others}} + R_L + R_C)}\\]Parallel Circuit Quality FactorsAdmittance \\(Y\\), conductance \\(G\\) and susceptance \\(B\\).\\[Y_L = G_L + jB_L\\]\\[Y_C = G_C + jB_C\\]For parallel \\(L\\) and \\(G_L\\)\\[Q = \\dfrac{B_L}{G_L} = \\dfrac{1}{\\omega L G_L}\\]For parallel \\(C\\) and \\(G_C\\)\\[Q = \\dfrac{B_C}{G_C} = \\dfrac{\\omega C}{G_C}\\]Similarly\\[Q_U = \\dfrac{1}{\\omega L (G_L+G_C)} = \\dfrac{\\omega C}{G_L + G_C}\\]\\[Q_L = \\dfrac{1}{\\omega_L (G_{\\text{others}} + G_L + G_C)} = \\dfrac{\\omega C}{G_{\\text{\\others}} + G_L + G_C}\\]Coupled ResonatorsDistributed Circuits" }, { "title": "ISSCC", "url": "/posts/isscc/", "categories": "analog-circuit", "tags": "analog", "date": "2023-09-26 05:00:00 +0000", "snippet": "Coupling Effects Digital circuit capacitance and switching analysis for ground bounce in ICs with a high-ohmic substrateISSCC 2024Session 10: Frequency Synthesis10.1 An 8.75GHz Fractional-N Digital PLL with a Reverse-Concavity Variable-Slope DTC Achieving 57.3fsrms Integrated Jitter and −252.4dB FoM conventional variable slope dtc its linearity is limited by the slope-dependent propagation delay of the output buffer (comparator). in practice, variable slope dtc can be implemented as rc delay, the delay is defined by the rc constant, thus it is linear with respect to either r or c. The slope dependent comparator delay can be mitigated by adding a fixed capacitance to reduce the slope variation. However, as discussed in the slides, adding fixed capacitance increases power and jitter. In this paper, by adding a tail resistor to intentionally introduce an upward concavity nonlinear, the overall nonlinear can be reduced by combining the two different concavity.10.2 A 5.5μs-Calibration-Time, Low-Jitter, and Compact-Area Fractional-N Digital PLL Using the Recursive-Least-Squares (RLS) Algorithm for multi-variable lms calibration, the steps for different control should be much larger or smaller to avoid racing. As a result, the smallest step will suffer slow convergence. proposed recursive-least-square, with dichotomous coordinate descent-based recursive-least-square, achieve fast convergence with small hardware complexity.10.3 A 7GHz Digital PLL with Cascaded Fractional Divider and Pseudo-Differential DTC Achieving -62.1dBc Fractional Spur and 143.7fs Integrated Jitter The proposed cascaded fractional divider, isn’t it the same as fcw subtractive dithering? It doesn’t compare fractional spur with fcw subtractive dithering. use differential dtc (why it is called “pseudo differential”)? Isn’t differential dtc has been implemented?10.4 A 45.5fs-Integrated-Random-Jitter and -75dBc-Integer-Boundary-Spur BiCMOS Fractional-N PLL with Suppression of Fractional, Horn, and Wandering Spurs prior art: successive requantizer, probability mass redistributor, time-invariant probability modulator, probability density shaping, enhanced nonlinear-induced noise performance. another technique to mitigate fractional spur.10.5 A 76 fsrms- Jitter and -65dBc- Fractional-Spur Fractional-N Sampling PLL Using a Nonlinearity-Replication Technique constant-slope dtc or inverse constant-slope dtc increases thermal noise due to long pre-charge time (limited voltage slope).ISSCC 2023Session 32: Intelligent Biomedical Circuits and Systems32.1 A Behind-The-Ear Patch-Type Mental Healthcare Integrated Interface with 275-Fold Input Impedance Boosting and Adaptive Multimodal Compensation Capabilities Battery Behind-The-Ear ExG,PPG,GSR,BIOZ,tVNS32.7 Fascicle-Selective Bidirectional Peripheral Nerve Interface IC with 173dB FOM Noise-Shaping SAR ADCs and 1.38pJ/b Frequency-Multiplying Current-Ripple Radio Transmitter Ultrasound Power Nerve Cuff ElectrodesISSCC 2022Session 12: Monolithic System for Robot and Bio ApplicationsSession 20: Body and Brain Interfaces20.5 A Miniaturized Wireless Neural Implant with Body-Coupled Data Transmission and Power Delivery for Freely Behaving Animals Body-Coupled Communication Body-Coupled Power DeliveryISSCC 2020Session 26: Biomedical Innovations26.7 A 280µW 108dB DR Readout IC with Wireless Capacitive Powering Using a Dual-Output Regulating Rectifier for Implantable PPG Recording" }, { "title": "PLL Study", "url": "/posts/pll-study/", "categories": "analog-circuit", "tags": "analog", "date": "2023-09-14 05:00:00 +0000", "snippet": "Inductor Simulationusing sp or psp simulation to simulate inductor, we calculate the Z parameter\\[\\begin{bmatrix}V_1\\\\V_2\\end{bmatrix}=\\begin{bmatrix}Z_{11} &amp; Z_{12} \\\\Z_{21} &amp; Z_{22}\\end{bmatrix}\\begin{bmatrix}i_1\\\\-i_1\\end{bmatrix}\\]\\[Z_{diff} = \\dfrac{V_1-V_2}{i_1} = Z_{11} + Z_{22} - Z_{12} - Z_{21}\\]\\[L_s = \\dfrac{\\text{Im}(Z_{diff})}{2\\pi f}\\]\\[R_s = \\text{Re}(Z_{diff})\\]\\[Q = \\dfrac{\\text{Im}(Z_{diff})}{\\text{Re}(Z_{diff})}\\]\\[R_p = (1+Q^2)R_s\\]In virtuosoImagz = imag(zpm('psp 1 1)) + imag(zpm('psp 2 2)) - imag(zpm('psp 1 2)) - imag(zpm('psp 2 1))Ls = Imagz / 2 / 3.14 / xval(Imagz)Rs = real(zpm('psp 1 1)) + real(zpm('psp 2 2)) - real(zpm('psp 1 2)) - real(zpm('psp 2 1))Q = Imagz / RsRp = (1+Q*Q)*RsCapacitor Simulation\\[C_s = -\\dfrac{1}{2\\pi f} \\cdot \\dfrac{1}{Im(Z_{diff})}\\]\\[R_s = Re(Z_{diff})\\]Phase NoiseReferences:A Charge-Sharing Locking Technique With a General Phase Noise Theory of Injection LockingReference phase noise\\[\\begin{align}\\mathcal{L}_{ref}(f) &amp;= \\dfrac{1}{2} S_{\\phi} = \\dfrac{1}{2} S_{t} (2\\pi F_{ref})^2\\\\&amp;= \\dfrac{1}{2} \\cdot \\dfrac{\\sigma_{t}^2}{F_{ref}/2} \\cdot (2\\pi F_{ref})^2\\\\&amp;= 4\\pi^2 F_{ref} \\cdot \\sigma_{t}^2\\end{align}\\]For example, \\(\\sigma_t = 112.5 \\,\\mathrm{ fs}\\) with \\(F_{ref} = 200\\,\\mathrm{ MHz}\\), \\(\\mathcal{L}_{ref} = -160 \\,\\mathrm{ dBc/Hz}\\).However, this is the reference phase noise in the reference frequency domain.To the output it will be multiplied by \\(N^2\\) and low-passed by the loop filter.VCO phase noise\\[\\begin{align}\\mathcal{L}_{osc}(f) &amp;= \\dfrac{1}{2} S_{\\phi} = \\dfrac{1}{2}S_t (2\\pi F_{osc})^2\\\\&amp;= \\dfrac{1}{2} \\cdot \\dfrac{\\sigma_t^2}{F_{osc}/2} \\cdot \\left(\\dfrac{F_{osc}}{2\\pi f}\\right)^2 \\cdot (2\\pi F_{osc})^2\\\\&amp;= \\sigma_t^2 \\cdot \\dfrac{F_{osc}^3}{f^2}\\end{align}\\]For example, \\(\\sigma_t = 1 \\,\\mathrm{fs}, F_{osc} = 10\\,\\mathrm{GHz}, f = 10\\,\\mathrm{MHz}\\), \\(\\mathcal{L}_{osc}(f) = -140\\,\\mathrm{dBc/Hz}\\).Fractional-N PLL quantization noise:BBPLL TheoryMultirate time-domain modelConvert to single-rate modelEquivalent linear modelUnder some assumption, approximately\\[K_{BPD} = \\sqrt{\\dfrac{2}{\\pi}} \\cdot \\dfrac{1}{\\sigma_{\\Delta t}}\\]\\[\\sigma_q^2 = 1 - K_{BPD}^2 \\cdot \\sigma_{\\Delta t}^2 = 1 - \\dfrac{2}{\\pi}\\]Given \\(\\sigma_{ref}, \\sigma_{DCO}, K_P, K_I, N, K_T\\), we want to find the value of \\(K_{BPD}\\).For simplicity, we assume \\(K_I\\) is very small (\\(K_I \\ll K_P\\)) such that we can ignore it.And we denote \\(\\chi = N K_{BPD} K_P K_T\\).\\[\\begin{align}Y_1[k+1] &amp;= Y_1[k] + \\chi \\left(\\sigma_{ref}[k] - Y_1[k]\\right)\\\\&amp;= (1-\\chi) Y_1[k] + \\chi \\sigma_{ref}[k]\\end{align}\\]since \\(Y_1[k]\\) and \\(\\sigma_{ref}[k]\\) are independent\\[\\sigma_{Y_1}^2 = (1-\\chi)^2 \\sigma_{Y_1}^2 + \\chi^2 \\sigma_{ref}^2\\]\\[\\sigma_{Y_1}^2 = \\dfrac{\\chi}{2-\\chi} \\sigma_{ref}^2\\]\\[\\sigma_{\\Delta t_1}^2 = \\sigma_{ref}^2 + \\sigma_{Y_1}^2 = \\dfrac{2}{2-\\chi} \\sigma_{ref}^2\\]\\[\\begin{align}Y_2[k+1] &amp;= Y_2[k] + \\dfrac{\\chi}{K_{BPD}}\\left(\\sigma_q[k] - K_{BPD} Y_2[k]\\right)\\\\&amp;= \\left(1 - \\chi\\right) Y_2[k] + \\dfrac{\\chi}{K_{BPD}} \\sigma_q[k]\\end{align}\\]\\[\\sigma_{\\Delta t_2}^2 = \\sigma_{Y_2}^2 = \\dfrac{\\chi}{2-\\chi} \\dfrac{\\sigma_{q}^2}{K_{BPD}^2}\\]\\[\\begin{align}Y_3[k+1] &amp;= Y_3[k] + (\\sqrt{N} \\sigma_{DCO}[k] - \\chi Y_3[k])\\\\&amp;= (1-\\chi) Y_3[k] + \\sqrt{N} \\sigma_{DCO}[k]\\end{align}\\]\\[\\sigma_{\\Delta t_3}^2 = \\sigma_{Y_3}^2 = \\dfrac{N}{\\chi(2-\\chi)} \\sigma_{DCO}^2\\]\\[\\begin{align}\\sigma_{\\Delta t}^2 &amp;= \\sigma_{\\Delta t_1}^2 + \\sigma_{\\Delta t_2}^2 + \\sigma_{\\Delta t_3}^2\\\\&amp;= \\dfrac{2}{2-\\chi} \\sigma_{ref}^2 + \\dfrac{\\chi}{2-\\chi} \\dfrac{\\sigma_q^2}{K_{BPD}^2} + \\dfrac{N}{\\chi(2-\\chi)}\\sigma_{DCO}^2\\end{align}\\]combine with\\[K_{BPD} = \\sqrt{\\dfrac{2}{\\pi}} \\cdot \\dfrac{1}{\\sigma_{\\Delta t}}\\]\\[\\sigma_q^2 = 1 - \\dfrac{2}{\\pi}\\]After some algebra (please provide)\\[\\sigma_{\\Delta t} = \\dfrac{\\eta}{2} + \\sqrt{\\left(\\dfrac{\\eta}{2}\\right)^2 + \\sigma_{ref}^2}\\]\\[\\eta = \\sqrt{\\dfrac{\\pi}{2}} \\cdot \\left(\\dfrac{\\sigma_{DCO}^2}{2 K_P K_T} + \\dfrac{N K_P K_T}{2}\\right)\\]\\[K_{P,opt} = \\dfrac{\\sigma_{DCO}}{K_T \\sqrt{N}}\\]\\[\\eta_{opt} = \\sqrt{\\dfrac{N \\pi}{2}} \\sigma_{DCO}\\]\\[\\Delta t[k] = \\sigma_{ref}[k] - \\sigma_{out}[k]\\]\\[\\sigma_{\\Delta t}^2 = \\sigma_{ref}^2 + \\sigma_{out}^2\\]\\[\\sigma_{out}^2 = \\sigma_{\\Delta t}^2 - \\sigma_{ref}^2\\]BBPLL locking time, Frequency Synthesizers Based on Fast-Locking Bang-Bang PLL for Cellular Applications1 JSSC 2009Circuit Design2 ISSCC 2018 JSSC 2019Circuit DesignThe circuits is composed of a VCO, a sampler, one reference buffer and two VCO buffer.SamplerWe have to make sure the SampleEdge’s falling edge comes when CKvco- is 0.There are two possibilities, one is CKref asserts when CKvco+ is 0; another one is CKref asserts when CKvco+ is 1.In either case, SampleEdge’s falling edge happens when CKvco+ is 1, then CKvco- is 0.The diagram shows single-ended sampling, but the real circuits adopts differential sampling.Reference bufferThe reference buffer is calibrated per die such that CKref transit at the crossover of the reference signal.Inband Phase Noise Analysis\\[S_{n,sample} = \\dfrac{2kT}{C_{sample}} \\cdot \\dfrac{1}{F_{ref}/2} = \\dfrac{4kT}{C_{sample} \\cdot F_{ref}}\\]\\[\\mathcal{L}_{n,sample} = \\dfrac{1}{2} \\cdot S_{n,sample} \\cdot \\left(\\dfrac{N}{2A_{ref}}\\right)^2 = \\dfrac{2kT}{C_{sample}\\cdot F_{ref}} \\cdot \\left(\\dfrac{N}{2A_{ref}}\\right)^2\\]\\[S_{n,vcobuf} = \\dfrac{\\overline{\\Delta V^2}}{SL^2} \\cdot (2\\pi F_{ref})^2 \\cdot \\dfrac{1}{F_{ref}/2}\\]\\[\\mathcal{L}_{n,vcobuf} = \\dfrac{1}{2} \\cdot N^2 \\cdot S_{n,vcobuf}\\]" }, { "title": "Analog Circuits Explanation", "url": "/posts/analog-circuits-explanation/", "categories": "analog-circuit", "tags": "analog", "date": "2023-07-30 06:00:00 +0000", "snippet": "BasicsCrystal OscillatorResonant FrequencyWe want to calculate the resonant frequency of a crystal, including \\(C_0, R_m, L_m, C_m\\).We will first calculate the total impedance.Then by definition, resonat frequency is the frequency such that the total impedance is purely real.\\[Z_0 = -\\dfrac{j}{\\omega C_0}\\]\\[Z_m = R_m + j\\omega L_m - \\dfrac{j}{\\omega C_m}\\]\\[Z_p = \\dfrac{Z_0 Z_m}{Z_0 + Z_m} = \\dfrac{\\dfrac{L_m}{C_0} - \\dfrac{1}{\\omega^2 C_0 C_m} - \\dfrac{j R_m}{\\omega C_0}}{R_m + j\\left(\\omega L_m - \\dfrac{1}{\\omega C_m} - \\dfrac{1}{\\omega C_0}\\right)}\\]The condition for \\(Z_p\\) to be purely real\\[\\dfrac{-\\dfrac{R_m}{\\omega C_0}}{\\dfrac{L_m}{C_0}-\\dfrac{1}{\\omega^2 C_0 C_m}} = \\dfrac{\\omega L_m - \\dfrac{1}{\\omega C_m} - \\dfrac{1}{\\omega C_0}}{R_m}\\]\\[-\\dfrac{R_m^2}{\\omega C_0} = \\left(\\dfrac{L_m}{C_0}-\\dfrac{1}{\\omega^2 C_0 C_m}\\right) \\left(\\omega L_m - \\dfrac{1}{\\omega C_m} - \\dfrac{1}{\\omega C_0}\\right)\\]\\[\\omega^4 + \\omega^2 \\left(\\dfrac{R_m^2}{L_m^2}-\\dfrac{2}{L_m C_m} - \\dfrac{1}{L_m C_0}\\right) + \\left(\\dfrac{1}{L_m^2 C_m^2} + \\dfrac{1}{L_m^2 C_m C_0}\\right) = 0\\]we can find two resonant frequency\\[\\omega^2 = \\dfrac{1}{2} \\left\\{\\left(\\dfrac{2}{L_m C_m} + \\dfrac{1}{L_m C_0} - \\dfrac{R_m^2}{L_m^2}\\right) \\pm\\left[\\left(\\dfrac{R_m^2}{L_m^2}-\\dfrac{2}{L_m C_m} - \\dfrac{1}{L_m C_0}\\right)^2 - 4\\left(\\dfrac{1}{L_m^2 C_m^2} + \\dfrac{1}{L_m^2 C_m C_0}\\right)\\right]^{1/2}\\right\\}\\]some more algebra\\[\\begin{align}&amp;\\left(\\dfrac{R_m^2}{L_m^2}-\\dfrac{2}{L_m C_m} - \\dfrac{1}{L_m C_0}\\right)^2 - 4\\left(\\dfrac{1}{L_m^2 C_m^2} + \\dfrac{1}{L_m^2 C_m C_0}\\right)\\\\=&amp; \\left( \\dfrac{1}{L_m C_0}-\\dfrac{R_m^2}{L_m^2} \\right)^2 - \\dfrac{4 R_m^2}{L_m^3 C_m}\\end{align}\\]We assume \\(\\left( \\dfrac{1}{L_m C_0}-\\dfrac{R_m^2}{L_m^2} \\right)^2 \\gg \\dfrac{4 R_m^2}{L_m^3 C_m}\\).For example, \\(R_m=10\\Omega, L_m=500uH, C_m=5fF, C_0=1pF\\), that will be \\(4 \\times 10^{30} \\gg 6.4 \\times 10^{26}\\).Then\\[\\omega^2 \\approx \\left(\\dfrac{1}{L_m C_m} + \\dfrac{1}{2 L_m C_0} - \\dfrac{R_m^2}{2 L_m^2}\\right) \\pm \\left( \\dfrac{1}{2 L_m C_0}-\\dfrac{R_m^2}{2 L_m^2} \\right)\\]\\[\\omega_s \\approx \\dfrac{1}{\\sqrt{L_m C_m}}\\]\\[\\omega_a \\approx \\left(\\dfrac{1}{L_m C_m} + \\dfrac{1}{L_m C_0} - \\dfrac{R_m^2}{L_m^2}\\right)^{1/2}\\]assume again \\(\\dfrac{1}{L_m C_0} \\gg \\dfrac{R_m^2}{L_m^2}\\), for example, \\(2\\times 10^{15} \\gg 4 \\times 10^{8}\\).\\[\\begin{align}\\omega_a \\approx \\sqrt{\\dfrac{1}{L_m \\left(C_m \\Vert C_0\\right)}}\\end{align}\\]or\\[\\begin{align}\\omega_a \\approx \\sqrt{\\dfrac{1}{L_m C_m}} \\left(1+\\dfrac{C_m}{C_0}\\right)^{1/2} \\approx \\sqrt{\\dfrac{1}{L_m C_m}} \\left(1+\\dfrac{C_m}{2C_0}\\right)\\end{align}\\]If there are load capacitance, the load capacitance \\(C_L\\) will be parallel with \\(C_0\\), thus\\[\\omega_s \\approx \\dfrac{1}{\\sqrt{L_m C_m}}\\]\\[\\omega_a \\approx \\sqrt{\\dfrac{1}{L_m \\left(C_m \\Vert \\left(C_0+C_L\\right)\\right)}} \\approx \\sqrt{\\dfrac{1}{L_m C_m}} \\left(1+\\dfrac{C_m}{2(C_0+C_L)}\\right)\\]depending on how the oscillator is designed, the oscillation frequency will be betwenn the two value\\[\\omega_s \\le \\omega_{osc} \\le \\omega_a\\]Equivalent ResistanceLet’s denote \\(jX = j\\omega L_m - \\dfrac{j}{\\omega C_m}\\), and \\(j X_{C0} = -\\dfrac{j}{\\omega C_0}\\).The total impedence of the crystal\\[\\begin{align}Z &amp;= \\dfrac{\\left(R_m + jX\\right)\\left(j X_{C0}\\right)}{R_m + j(X+X_{C0})}\\\\&amp;= \\dfrac{R_m X_{C0}^2}{R_m^2 + \\left(X+X_{C0}\\right)^2} + \\dfrac{jX_{C0}\\left[R_m^2 + X\\left(X+X_{C0}\\right)\\right]}{R_m^2 + \\left(X+X_{C0}\\right)^2}\\end{align}\\]The effective resistance is given by\\[R_e = \\dfrac{R_m X_{C0}^2}{R_m^2 + \\left(X+X_{C0}\\right)^2}\\]consider the frequency between \\(\\dfrac{1}{\\sqrt{L_m C_m}} \\le \\omega \\le \\dfrac{1}{\\sqrt{L_m C_m}} \\left(\\dfrac{C_m}{2(C_0+C_L)}\\right)\\).Since it is a narrow frequency range, \\(X_{C0}\\) stay almost constant.From \\(\\omega_s\\) to \\(\\omega_a\\), \\(X\\) will increase from 0 to some positive value.Since \\(X_{C0}\\) is negative, \\(R_e\\) will increase if we sweep the frequency from \\(\\omega_s\\) to \\(\\omega_a\\).At \\(\\omega_s\\), \\(X=0\\)\\[R_e\\vert_{\\omega=\\omega_s} = \\dfrac{R_m X_{C0}^2}{R_m^2 + X_{C0}^2} \\approx R_m \\quad (\\text{ usually } \\vert X_{C0} \\vert \\gg R_m)\\]on the other hand, when \\(\\omega=\\omega_a\\), we can use either of the two equations for \\(\\omega_a\\), consider the one\\[\\omega_a \\approx \\sqrt{\\dfrac{1}{L_m \\left(C_m \\Vert \\left(C_0+C_L\\right)\\right)}}\\]then\\[\\omega_a L_m = \\dfrac{1}{\\omega_a C_m} + \\dfrac{1}{\\omega_a (C_0+C_L)}\\]and\\[X = \\dfrac{1}{\\omega_a\\left(C_L+C_0\\right)}\\]then\\[\\begin{align}R_e\\vert_{\\omega=\\omega_a} &amp;= \\dfrac{R_m \\left(\\dfrac{1}{\\omega_a C_0}\\right)^2}{R_m^2 + \\left(\\dfrac{1}{\\omega_a\\left(C_L+C_0\\right)} - \\dfrac{1}{\\omega_a C_0}\\right)^2}\\\\&amp;= \\dfrac{R_m X_{C0}^2}{R_m^2 + X_{C0}^2 \\left(\\dfrac{C_L}{C_L+C_0}\\right)^2}\\end{align}\\]assume \\(\\left\\vert X_{C0} \\left(\\dfrac{C_L}{C_L+C_0} \\right)\\right\\vert \\gg R_m\\)\\[\\begin{align}R_e\\vert_{\\omega=\\omega_a} &amp;\\approx R_m \\left(\\dfrac{C_L+C_0}{C_L}\\right)^2\\end{align}\\]" }, { "title": "Analog Circuits", "url": "/posts/analog-circuits/", "categories": "analog-circuit", "tags": "analog", "date": "2023-07-30 05:00:00 +0000", "snippet": "PapersA high-swing, high-impedance MOS cascode circuit regulated cascode (or RGC) achieves higher output impedance than simple cascode. RGC has larger output voltage range than optimally biased simple cascode.The active-input regulated-cascode current mirrorUltra high-compliance CMOS current mirrors for low voltage charge pumps and referencesRelationship between frequency response and settling time of operational amplifiersAn Integrated Linear Regulator With Fast Output Voltage Transition for Dual-Supply SRAMs in DVFS SystemsFrom Analog Design EssentialsAmplifiers, source followers and cascodes024: Single-transistor amplifier\\[A_v = g_m r_{DS} = \\dfrac{2 I_{DS}}{V_{ov}} \\cdot \\dfrac{V_E L}{I_{DS}} = \\dfrac{2 V_E L}{V_{ov}}\\] high gain needs large \\(L\\) and high \\(g_m/i_d\\)0213: Miller capacitance feedback effects the \\(C_{gd}\\) miller capacitor gives a positive zero: the gain magnitude becomes constant at high frequency, since the transistor is diode connected with impedance \\(1/g_m\\) the phase becomes in-phase at high frequency instead of completely-out-of-phase at low frequency such a phenomenon can only be explained by a positve zero after the negative pole full small-signal analysis reveals that \\(f_z = \\dfrac{g_m}{2\\pi C_F}\\) 0250: Cascode with resistive load cascode convert current to voltage, the gain will be larger if the load resistance is larger. however, in reality the gain cannot increase to infinity, to see the limit we have to include the \\(R_B\\) and \\(r_{DS}\\).Stability of Operational amplifiers0515: What makes an opamp an opamp?An opamp is really a single-pole system.As a result, it allows exchange of gain with bandwidth, within a specific GBW.Usually the number number of high impedance nodes indicates the number of poles, there can be only one internal node at high impedance for opamp.All two-stage amplifiers have two high-impedance nodes and hence two poles.Therefore, all amplifiers with two high-impedance nodes are called two-stage amplifiers, irrespective of the number of transistors.Wideband amplifiers are very different.They consist of more stages, each of them having a pole.They are normally compensated at one particular setting of the gain.They are not meant to exchange gain for bandwidth.0526: Relation PM, damping and \\(f_2/\\text{GBW}\\) \\(\\dfrac{f_2}{\\text{GBW}}\\) PM (\\(^\\circ\\)) \\(\\zeta = \\dfrac{1}{2}\\sqrt{\\dfrac{f_2}{GBW}}\\) \\(P_f\\) (dB) \\(P_t\\) (dB) 0.5 27 0.35 3.6 2.3 1 45 0.5 1.25 1.3 1.5 56 0.61 0.28 0.73 2 63 0.71 0 0.37 3 72 0.87 0 0.04 When designing opamp we will try to avoid peaking.Thus \\(72^\\circ\\) PM is a good safety position to start with, as all parasitic capacitances come in after layout, pushing the non-domiant pole to lower values and decreasing the phase margin.Eventually \\(63^\\circ\\) PM is good enough to not have peaking in the frequency domain.Another reason for \\(63^\\circ \\sim 72^\\circ\\) PM is that it is a good compromise between peaking and bandwidth;Larger PM reduces the bandwidth too much.0530: Generic 2-stage opamp  \\(GBW = \\dfrac{g_{m1}}{2\\pi C_c}\\), since node 1 is virtual ground, and the output voltage is generated by the current flow through \\(C_c\\).  \\(f_{nd} = \\dfrac{g_{m2}}{2\\pi C_L}\\), since at high frequency \\(C_c\\) is shorted, the non-domiant pole is calculated by the impedance at node 1.  \\(f_z = \\dfrac{g_{m2}}{2\\pi C_c}\\), this is a positive zero.Systematic Design of Operational AmplifiersBasicsProf. Jri Lee’s LectureDifferential Pair\\[A_v = - g_m R_D\\]CMRR\\[CMRR = \\dfrac{\\vert A_{DM} \\vert}{\\vert A_{CM-DM} \\vert}\\]\\[\\vert A_{CM-DM}\\vert_1 = \\left\\vert \\dfrac{\\Delta R_c}{1/g_m + 2 R_{EE}} \\right\\vert\\]\\[\\vert A_{CM-DM}\\vert_1 = \\left\\vert \\dfrac{\\Delta g_m R_c}{1+2g_mR_{EE}} \\right\\vert\\]\\[\\vert A_{CM-DM} \\vert = \\sqrt{\\vert A_{CM-DM} \\vert_1^2 + \\vert A_{CM-DM}\\vert_2^2}\\]Offset\\[V_{os,1} = \\dfrac{(V_{GS}-V_{TH})\\Delta R_D}{2 R_D}\\]\\[V_{os,2} = \\dfrac{(V_{GS}-V_{TH})\\Delta\\left(\\dfrac{W}{L}\\right)}{2\\left(\\dfrac{W}{L}\\right)}\\]\\[V_{os,3} = \\Delta V_{TH}\\]\\[V_{os} = \\sqrt{V_{os,1}^2+V_{os,2}^2+V_{os,3}^2}\\]Current Mirror Load\\[A_v = g_m (r_{on}\\Vert r_{op})\\]Frequency Response (Lecture 6)\\[H(s) = A_v \\cdot \\dfrac{(1+s/\\omega_{z,1})\\dots (1_s/\\omega_{z,n})}{(1+s/\\omega_{p,1}) \\dots (1+s/\\omega_{p,n})}\\]Coupling Capacitor and Bypass CapacitorThe approximated method: Each capacitor generates (exactly) one pole and one zero. The zero may degenerate to zero or infinite frequency. The pole frequency is found by RC time constant. The zero frequency is found by finding the frequency when the output is zero (when all the other capacitors are working ideally small or large).Miller EffectLHP zero is okay. RHP zero is dangerous.Open Circuit Time Constant (Lecture 8)Common SourceCommon Gate (Lecture 9)Common Drain (Lecture 10)Differential PairFeedback (Lecture 13)Operational AmplifierTwo Stage AmplifierSlew RatePSRRFolded OpampLimiterCMFBOne of the CMFB is using triode devices.The M7 and M8 will be in deep triode region, such that the current sources M5 and M6 are degenarated.Assume the current sources M5 and M6 are bigger than M9 and M10, then the output common mode will be lower than nomimal output CM level.The gate voltage of M7 and M8 will decrease, and their resistance will increase, the current of M5 and M6 will decrease.Another way of CMFB is using a differential pair to generate a current.Digital CircuitsSR LatchD-Flip FlopC2MOS LatchCML LatchImpedance Formula\\[R \\Vert \\dfrac{1}{sC} = \\dfrac{R}{1+sRC}\\]Impedance TransformationSeries-to-ParallelInductor\\[Q = \\dfrac{\\omega L_s}{R_s}\\]\\[R_p = R_s(1+Q^2)\\]\\[L_p = L_s (1+\\dfrac{1}{Q^2})\\]Capacitor\\[Q = \\dfrac{1}{\\omega R_s C_s}\\]\\[R_p = R_s(1+Q^2)\\]\\[C_p = \\dfrac{C_s}{1+\\dfrac{1}{Q^2}}\\]Parallel-to-SeriesInductor\\[Q = \\dfrac{R_p}{\\omega L_p}\\]\\[R_s = \\dfrac{R_p}{1+Q^2}\\]\\[L_s = \\dfrac{L_p}{1+\\dfrac{1}{Q^2}}\\]Capacitor\\[Q = \\omega R_p C_p\\]\\[R_s = \\dfrac{R_p}{1+Q^2}\\]\\[C_s = C_p (1+\\dfrac{1}{Q^2})\\]High-Q SystemFor inductor\\[Q = \\dfrac{\\omega L_s}{R_s} = \\dfrac{R_p}{\\omega L_p}\\]if \\(Q\\) is large, \\(L_s \\approx L_p\\) (with error \\(1 + \\dfrac{1}{Q^2}\\))\\[R_s R_p = (\\omega L)^2\\]For capacitor\\[Q = \\dfrac{1}{\\omega R_s C_s} = \\omega R_p C_p\\]if \\(Q\\) is large, \\(C_s \\approx C_p\\) (with error \\(1 + \\dfrac{1}{Q^2}\\))\\[R_s R_p = \\dfrac{1}{(\\omega C)^2}\\]OscillatorFrom Prof. Jri Lee’s LecturesBarkhausen CriteriaWhen the oscillator just starts to oscillate, we can use small signal model.If the closed-loop transfer function is\\[H(s) = \\dfrac{A(s)}{1+A(s)\\beta(s)}\\]It must be unstable at the equilibrium point, thus from the concept of gain margin\\[\\begin{align}\\angle A(j\\omega_{\\text{x}})\\beta(j\\omega_{\\text{x}}) &amp;= -180^\\circ\\\\\\vert A(j\\omega_{\\text{x}})\\beta(j\\omega_{\\text{x}}))\\vert &amp;&gt; 1\\end{align}\\]Wien-Bridge Oscillatorwe assume the input is atlet\\[\\begin{align}Z_p &amp;= \\dfrac{R}{1+sRC}\\\\Z_s &amp;= R + \\dfrac{1}{sC}\\end{align}\\]the closed-loop transfer function can be calculated as\\[H(s) = \\dfrac{V_{out}(s)}{V_{in}(s)} = \\dfrac{(R_1 + R_2)/R_1}{1 - \\dfrac{R_2}{R_1} \\dfrac{Z_p}{Z_s}}\\]\\[\\dfrac{R_2}{R_1} \\dfrac{Z_p}{Z_s} = \\dfrac{R_2}{R_1} \\dfrac{j\\omega RC}{1-\\omega^2 R^2C^2 + 2j\\omega RC}\\]\\[\\omega_x = \\dfrac{1}{RC}\\]\\[\\dfrac{R_2}{2R_1} &gt; 1\\]Colpitts OscillatorSelf-Charged XOISSCC 2012, JSSC 2016 The sine wave will first be converted to square wave, with the requirement that the output will trip exactly in the middle of the input voltage swing. A self-adaptive boby-biasing technique is used.ISSCC 2014Electronics Letters 2016ASSCC 2017JSSC 2019VLSI-DAT 2020JSSC 2022Flicker Noise\\[A \\cdot \\dfrac{f_c}{f}\\]\\[1.38 A \\cdot \\dfrac{f_c}{f_{min}} \\sum_{n=0}^{N_{max}} \\dfrac{10^{-n}}{1+(\\dfrac{f}{10^n f_{min}})^2}\\]Phase DetectorHarmonic-Rejection MixerOriginal paperA circuit for all seasonsA mixer can be used as a phase detector.\\[\\cos(\\omega t) \\cdot \\cos(\\omega t + \\phi) = \\dfrac{1}{2} \\cdot \\cos\\left( 2\\omega t + \\phi\\right) + \\dfrac{1}{2} \\cdot \\cos\\phi\\]When mixing two sinosoidal signal, there is no higher harmonics.\\[\\cos(\\omega_1 t) \\cdot \\cos(\\omega_2 t) = \\dfrac{1}{2} \\cdot \\cos\\left( (\\omega_1 + \\omega_2) t\\right) + \\dfrac{1}{2} \\cdot \\cos\\left( (\\omega_1 - \\omega_2) t\\right)\\]When mixing sinosoidal with square wave, there is higher harmonics.\\[f_1(t) = \\cos(\\omega t) - \\dfrac{1}{3} \\cos(3\\omega t) + \\dfrac{1}{5} \\cos(5\\omega t) - \\dfrac{1}{7} \\cos(7\\omega t) + \\dots\\]If we have available different phase (\\(\\pm 45^\\circ\\)) of \\(f_1(t)\\)\\[\\begin{align}f_2(t) &amp;= f_1\\left(t+\\dfrac{T}{8}\\right)\\\\&amp;= \\cos(\\omega t + \\pi/4) - \\dfrac{1}{3}\\cos(3\\omega t + 3 \\pi /4) + \\dfrac{1}{5}\\cos(5\\omega t + 5\\pi/4) + \\dots\\\\&amp;= \\dfrac{1}{\\sqrt{2}} \\left( \\left(\\cos(\\omega t)-\\sin(\\omega t)\\right) +\\dfrac{1}{3}\\left(\\cos(3\\omega t) + \\sin(3\\omega t)\\right) - \\dfrac{1}{5}\\left( \\cos(5\\omega t) - \\sin(3\\omega t) \\right) \\right)\\end{align}\\]\\[\\begin{align}f_3(t) &amp;= f_1\\left(t-\\dfrac{T}{8}\\right)\\\\&amp;= \\cos(\\omega t - \\pi/4) - \\dfrac{1}{3}\\cos(3\\omega t - 3 \\pi /4) + \\dfrac{1}{5}\\cos(5\\omega t - 5\\pi/4) + \\dots\\\\&amp;= \\dfrac{1}{\\sqrt{2}} \\left( \\left(\\cos(\\omega t)+\\sin(\\omega t)\\right) +\\dfrac{1}{3}\\left(\\cos(3\\omega t) - \\sin(3\\omega t)\\right) - \\dfrac{1}{5}\\left( \\cos(5\\omega t) + \\sin(3\\omega t) \\right) \\right)\\end{align}\\]thus using \\(\\sqrt{2}f_1(t) + f_2(t) + f_3(t)\\) can cancel third and fifth harmonics.Crystal OscillatorResonant FrequencyWe want to calculate the resonant frequency of a crystal, including \\(C_0, R_m, L_m, C_m\\).By definition, resonat frequency is the frequency such that the total impedance is purely real.\\[Z_0 = -\\dfrac{j}{\\omega C_0}\\]\\[Z_m = R_m + j\\omega L_m - \\dfrac{j}{\\omega C_m}\\]\\[Z_p = \\dfrac{Z_0 Z_m}{Z_0 + Z_m} = \\dfrac{\\dfrac{L_m}{C_0} - \\dfrac{1}{\\omega^2 C_0 C_m} - \\dfrac{j R_m}{\\omega C_0}}{R_m + j\\left(\\omega L_m - \\dfrac{1}{\\omega C_m} - \\dfrac{1}{\\omega C_0}\\right)}\\]The condition for \\(Z_p\\) to be purely real\\[\\dfrac{-\\dfrac{R_m}{\\omega C_0}}{\\dfrac{L_m}{C_0}-\\dfrac{1}{\\omega^2 C_0 C_m}} = \\dfrac{\\omega L_m - \\dfrac{1}{\\omega C_m} - \\dfrac{1}{\\omega C_0}}{R_m}\\]Solving the equatoin, we can find two resonant frequency\\[\\omega_s \\approx \\dfrac{1}{\\sqrt{L_m C_m}}\\]\\[\\begin{align}\\omega_a \\approx \\dfrac{1}{\\sqrt{L_m \\left(C_m \\Vert C_0\\right)}} \\approx \\dfrac{1}{\\sqrt{L_m C_m}} \\left(1+\\dfrac{C_m}{2C_0}\\right)\\end{align}\\]If there are load capacitance, the load capacitance \\(C_L\\) will be parallel with \\(C_0\\), thus\\[\\omega_s \\approx \\dfrac{1}{\\sqrt{L_m C_m}}\\]\\[\\omega_a \\approx \\dfrac{1}{\\sqrt{L_m \\left(C_m \\Vert \\left(C_0+C_L\\right)\\right)}} \\approx \\dfrac{1}{\\sqrt{L_m C_m}} \\left(1+\\dfrac{C_m}{2(C_0+C_L)}\\right)\\]Depending on how the oscillator is designed, the oscillation frequency will be betwenn the two value (this is stated by many books, I didn’t know the real reason for this statement though).\\[\\omega_s \\le \\omega_{osc} \\le \\omega_a\\]Equivalent ResistanceLet’s denote \\(jX = j\\omega L_m - \\dfrac{j}{\\omega C_m}\\), and \\(j X_{C0} = -\\dfrac{j}{\\omega C_0}\\).The total impedence of the crystal (note that the load \\(C_L\\) is not included)\\[\\begin{align}Z &amp;= \\dfrac{\\left(R_m + jX\\right)\\left(j X_{C0}\\right)}{R_m + j(X+X_{C0})}\\\\&amp;= \\dfrac{R_m X_{C0}^2}{R_m^2 + \\left(X+X_{C0}\\right)^2} + \\dfrac{jX_{C0}\\left[R_m^2 + X\\left(X+X_{C0}\\right)\\right]}{R_m^2 + \\left(X+X_{C0}\\right)^2}\\end{align}\\]The effective resistance is given by\\[R_e = \\dfrac{R_m X_{C0}^2}{R_m^2 + \\left(X+X_{C0}\\right)^2}\\]consider when we sweep the frequency from \\(\\omega_s\\) to \\(\\omega_a\\), formally \\(\\dfrac{1}{\\sqrt{L_m C_m}} \\le \\omega \\le \\dfrac{1}{\\sqrt{L_m C_m}} \\left(\\dfrac{C_m}{2(C_0+C_L)}\\right)\\).The effective resistance will increase from\\[R_e\\vert_{\\omega=\\omega_s} \\approx R_m\\]to\\[\\begin{align}R_e\\vert_{\\omega=\\omega_a} &amp;\\approx R_m \\left(\\dfrac{C_L+C_0}{C_L}\\right)^2\\end{align}\\]" }, { "title": "Engineering Mathematics Explanation", "url": "/posts/engineering-mathematics-explanation/", "categories": "math", "tags": "math", "date": "2023-07-23 08:00:00 +0000", "snippet": "First-Order First-Degree ODEUseful FormulasIntegrations\\[\\begin{align}&amp; \\int x^n dx = \\dfrac{x^{n+1}}{n+1} + C\\\\&amp; \\int \\dfrac{1}{x} dx = \\ln \\vert x \\vert + C\\\\&amp; \\int \\frac{1}{x+a} dx = \\ln \\vert x+a \\vert + C\\\\?&amp; \\int \\dfrac{dx}{\\sqrt{x^2+a^2}} = \\ln \\vert x + \\sqrt{x^2 + a^2} \\vert + C\\\\&amp; \\int \\cos x dx = \\sin x + C\\\\&amp; \\int \\exp(x)\\sin x = \\dfrac{1}{2}\\exp(x)(\\sin x - \\cos x) + C\\\\&amp; \\int \\dfrac{1}{1+x^2}dx = \\tan^{-1}x + C\\\\&amp; \\int_{-\\infty}^{\\infty} e^{-a x^2} dx = \\sqrt{\\dfrac{\\pi}{a}}, \\quad a &gt; 0\\\\&amp; \\int_0^\\infty \\dfrac{\\sin x}{x} dx = \\dfrac{\\pi}{2}\\\\&amp; \\int_{-\\infty}^\\infty \\dfrac{\\sin x}{x} dx = \\pi\\\\&amp; \\int_{-\\infty}^{\\infty} \\exp(j\\omega t) d\\omega = 2\\pi \\delta(t)\\end{align}\\]Others\\[\\begin{align}&amp; \\lim_{A\\to\\infty} \\dfrac{\\sin(Ax)}{\\pi x} = \\delta(x)\\\\&amp; \\delta(ax + b) = \\dfrac{1}{\\vert a \\vert} \\delta\\left(x + \\dfrac{b}{a}\\right)\\end{align}\\]Integration by Parts\\[\\int u dv = uv - \\int vdu\\]Exercise:\\[\\int \\exp(x) x dx\\]\\[u = x, \\quad v = \\exp(x)\\]\\[du = dx, \\quad dv = \\exp(x)dx\\]\\[\\int \\exp(x) x dx = x \\exp(x) - \\int \\exp(x) dx = x \\exp(x) - \\exp(x) + C\\]Ordinary, Partial, Order and DegreeOrdinary differential equation (ODE)\\[y' + 5y = 3x\\]\\[\\dfrac{\\partial^2 u}{\\partial x^2} + \\dfrac{\\partial^2 u}{\\partial y^2} = 0\\]\\[ay'' + by' + cy = f(x) \\quad \\text{ (second-order)}\\]\\[(y')^2 + y = e^x \\quad \\text{ (degree of 2) }\\]The degree of an ODE is the degree of the highest order derivative.\\[a_n(x) y^{(n)} + a_{n-1}(x)y^{(n-1)} + \\dots + a_1(x)y' + a_0(x)y = f(x)\\]Linear ODE has all the derivaties (including zero’s derivative) are degree of 1. General solution: solutions without specifying initial condition. Particular solution: a solution with specifying initial condtion. Singular solution: some other solutions cannot get from standard procedures, but they are indeed solutions of differential equation. Usually we are not focusing on such solutions.Seperable After Changing Variable First-Order First-Degree Homogeneous ODE, \\(y'=f(y/x)\\)\\[y' = f(y/x)\\]if\\[f(\\lambda x, \\lambda y) = f(x,y)\\]\\[M(\\lambda x, \\lambda y) = \\lambda^m M(x,y), \\quad N(\\lambda x, \\lambda y) = \\lambda^m N(x,y)\\]Exercise 8:\\[(x-\\sqrt{xy})y' = y\\]\\[y' = \\dfrac{\\dfrac{y}{x}}{1 - \\sqrt{\\dfrac{y}{x}}}\\]Let \\(v = y/x, \\implies y = vx \\implies y' = v' x + v\\)\\[v' x + v = \\dfrac{v}{1-\\sqrt{v}}\\]\\[\\dfrac{1-\\sqrt{v}}{v \\sqrt{v}} dv = \\dfrac{dx}{x}\\]\\[-2 v^{-1/2} - \\ln \\vert v \\vert = \\ln \\vert x \\vert + C\\]since \\(v \\ge 0\\)\\[\\vert y \\vert = k e^{-2\\sqrt{\\frac{x}{y}}}\\]\\[\\vert y \\vert e^{2\\sqrt{\\frac{x}{y}}} = k\\]Exercise 9:\\[(3xy + y^2) + (x^2 + xy) \\dfrac{dy}{dx} = 0\\]\\[(3 \\dfrac{y}{x} + (\\dfrac{y}{x})^2) dx + (1 + \\dfrac{y}{x})dy = 0\\]Let \\(v=y/x \\implies dy = xdv + vdx\\)\\[(3 v + v^2)dx + (1+v)(xdv + vdx) = 0\\]\\[\\dfrac{dx}{x} + \\dfrac{1+v}{2v(v+2)}dv = 0\\]there is a trick to find \\(\\dfrac{A}{v} + \\dfrac{B}{v+2}\\) quickly\\[\\dfrac{dx}{x} + \\dfrac{dv}{4v} + \\dfrac{dv}{4(v+2)} = 0\\]\\[\\ln\\vert x \\vert + \\dfrac{1}{4} \\ln \\vert v \\vert + \\dfrac{1}{4} \\ln \\vert v + 2 \\vert = C\\]\\[\\ln \\vert x^4 v (v+2) \\vert = C\\]\\[\\vert x^2 y (y+2x) \\vert = k\\]Exercise 10:\\[\\dfrac{dy}{dx} = \\dfrac{vy-s\\sqrt{x^2+y^2}}{vx}\\]\\[y(w) = 0\\]\\[udx + xdu = udx - (s/v) \\sqrt{1+u^2} dx = udx - r\\sqrt{1+u^2} dx\\]\\[\\dfrac{du}{\\sqrt{1+u^2}} + r \\dfrac{dx}{x} = 0\\]\\[\\ln \\vert u + \\sqrt{1+u^2} \\vert + r \\ln \\vert x \\vert = C\\]\\[(u + \\sqrt{1+u^2})x^r = k\\]\\[(\\dfrac{y}{x} + \\sqrt{1+(\\dfrac{y}{x})^2})x^r = k\\]\\[k = w^{s/v}\\]   \\((a_1 x + b_1 y + c_1)dx + (a_2 x +b_2 y + c2)dy = 0\\)First case: if \\(\\dfrac{a_1}{a_2} \\ne \\dfrac{b_1}{b_2}\\)\\[\\begin{cases}a_1 x + b_1 y + c_1 = 0\\\\a_2 x + b_2 y + c_2 = 0\\end{cases}\\]the two lines cross at the point \\((x,y)=(\\alpha,\\beta)\\).Let\\[\\begin{cases}x = u+\\alpha\\\\y = v + \\beta\\end{cases}\\]\\[[a_1(u+\\alpha) + b_1(v + \\beta) + c_1] du + [\\alpha_2 (u+\\alpha) + b_2(v+\\beta) + c_2] dv = 0\\]\\[(a_1 u + b_1 v) du + (a_2 u + b_2 v) dv = 0\\]Exercise 11:\\[(-3x + y + 6)dx + (x+y+2)dy = 0\\]\\[(x,y) = (1, -3)\\]\\[\\begin{cases}x = u+1\\\\y = v-3\\end{cases}\\]\\[(-3u+v)du + (u+v)dv = 0\\]Let \\(s = u/v, du = sdv + vds\\)\\[(\\dfrac{-3s+1}{s+1})(sdv + vds) + dv = 0\\]\\[\\dfrac{dv}{v} + \\dfrac{3s-1}{(3s+1)(s-1)}ds\\]\\[\\dfrac{dv}{v} + \\dfrac{1}{2}\\dfrac{1}{s+\\dfrac{1}{3}} ds + \\dfrac{1}{2}\\dfrac{1}{s-1} ds = 0\\]\\[\\ln \\vert v \\vert + \\dfrac{1}{2} \\ln \\vert s + \\dfrac{1}{3} \\vert + \\dfrac{1}{2} \\ln \\vert s - 1 \\vert = C\\]\\[v^2 \\vert (s+\\frac{1}{3})(s-1) \\vert = C\\]\\[\\vert (3u + v)(u - v) \\vert = C\\]\\[\\vert (3x+y)(x-y-4) \\vert = C\\]Second case:\\[(a_1 x + b_1 y + c_1) dx + (a_2 x +b_2 y + c_2) dy = 0\\]\\[\\dfrac{a_1}{a_2} = \\dfrac{b_1}{b_2} = m \\ne \\dfrac{c_1}{c_2}\\]Let \\(a_2 x + b_2 y = z\\)\\[dy = \\dfrac{dz - a_2 dx}{b_2}\\]\\[(mz + c_1) dx + (z + c_2) \\dfrac{dz - a_2 dx}{b_2}\\]\\[dx + \\dfrac{(z+c_2)dz}{(b_2 m - a_2)z + (b_2 c_1 - a_2 c_2)} = 0\\]Exercise 12:\\[(x+y)dx + (3x + 3y - 4)dy = 0\\]Let \\(z = x+y\\)\\[zdx + (3z-4)(dz - dx) = 0\\]\\[dx - \\dfrac{3}{2}dz - \\dfrac{dz}{z-2} = 0\\]\\[x - \\dfrac{3}{2}z - \\ln \\vert z-2 \\vert = C\\]   \\(y'=f(ax+by+c)\\)\\[t = ax + by + c\\]\\[dt = adx + bdy\\]Exercise 13:\\[y' = \\tan^2(x+y)\\]\\[u=x+y\\]\\[du - dx = \\tan^2(u) dx\\]\\[\\dfrac{du}{1+\\tan^2 u} - dx = 0\\]\\[\\cos^2(u)du - dx = 0\\]\\[(1+\\cos(2u))du - 2dx = 0\\]\\[u + \\dfrac{1}{2}\\sin u - 2x = C\\]Exact Differential Equations\\[xy = 10\\]\\[ydx + xdy = 0\\]How to determine if some ODE is exact differential equation?\\[M(x,y)dx + N(x,y)dy = 0\\]\\[M(x,y) = \\dfrac{\\partial \\phi}{\\partial x}\\]\\[N(x,y) = \\dfrac{\\partial \\phi}{\\partial y}\\]if \\(\\phi\\) has continuous second order partial derivatives, then\\[\\dfrac{\\partial^2 \\phi}{\\partial y \\partial x} = \\dfrac{\\partial^2 \\phi}{\\partial x \\partial y}\\]\\[\\dfrac{\\partial M}{\\partial y} = \\dfrac{\\partial N}{\\partial x} \\iff \\text{ it is an exact differential equation}\\]\\[\\phi(x,y) = \\int^x M d x + f(y) = \\int^y N d y + g(x)\\]Exercise 14:\\[(3x^2 y^2 + e^y) \\dfrac{dy}{dx} + 2(xy^3 + 1) = 0\\]\\[2(xy^3 + 1) dx + (3x^2 y^2 + e^y) dy = 0\\]\\[\\dfrac{\\partial}{\\partial y} 2(xy^3 + 1) = 6xy^2\\]\\[\\dfrac{\\partial}{\\partial x} (3x^2 y^2 +e^y) = 6xy^2\\]\\[\\phi(x,y) = \\int^x 2(xy^3+1) dx + f(y) = x^2y^3 + 2x + f(y)\\]\\[\\phi(x,y) = \\int^y (3x^2 y^2 + e^y)dy + g(x) = x^2y^3 + e^y + g(x)\\]\\[\\phi(x,y) = x^2y^3 + 2x + e^y = C\\]Exercise 15:\\[(x^2-4xy-y^2)dx + (y^2-2xy-2x^2)dy = 0\\]\\[\\dfrac{\\partial}{\\partial y} (x^2 - 4xy - y^2) = -4x - 2y\\]\\[\\dfrac{\\partial}{\\partial x} (y^2-2xy-2x^2) = -2y - 4x\\]\\[\\phi(x,y) = \\int^x (x^2-4xy-y^2)dx = \\dfrac{x^3}{3} - 2x^2y - xy^2 + f(y)\\]\\[\\phi(x,y) = \\int^y (y^2-2xy-2x^2)dy = \\dfrac{y^3}{3} - xy^2 - 2x^2 y + g(x)\\]\\[\\phi(x,y) = \\dfrac{x^3}{3} - 2x^2y - xy^2 + \\dfrac{y^3}{3} = C\\]Integrating Factors\\[M(x,y)dx + N(x,y)dy = 0\\]\\[I(x,y)M(x,y)dx + I(x,y)N(x,y)dy = 0\\]\\[IMdx + INdy = 0\\]\\[\\dfrac{\\partial}{\\partial y} (IM) = \\dfrac{\\partial I}{\\partial y} M + \\dfrac{\\partial M}{\\partial y} I\\]\\[\\dfrac{\\partial}{\\partial x} (IN) = \\dfrac{\\partial I}{\\partial x} N + \\dfrac{\\partial N}{\\partial x} I\\]\\[I \\dfrac{\\partial M}{\\partial y} + M \\dfrac{\\partial I}{\\partial y} = I \\dfrac{\\partial N}{\\partial x} + N \\dfrac{\\partial I}{\\partial x}\\]\\[(\\dfrac{\\partial M}{\\partial y} - \\dfrac{\\partial N}{\\partial x}) I = N \\dfrac{\\partial I}{\\partial x} - M \\dfrac{\\partial I}{\\partial y}\\]If \\(I\\) is only function of \\(x\\), then\\[\\dfrac{dI}{I} = \\dfrac{\\dfrac{\\partial M}{\\partial y} - \\dfrac{\\partial N}{\\partial x}}{N} dx\\]the right side must be only function of \\(x\\).\\[I(x) = \\exp(\\int \\dfrac{\\dfrac{\\partial M}{\\partial y} - \\dfrac{\\partial N}{\\partial x}}{N} dx )\\]If \\(I\\) is only function of \\(y\\), then\\[\\dfrac{dI}{I} = \\dfrac{\\dfrac{\\partial M}{\\partial y} - \\dfrac{\\partial N}{\\partial x}}{-M} dy\\]the right side must be only function of \\(y\\).\\[I(y) = \\exp(\\int \\dfrac{\\dfrac{\\partial M}{\\partial y} - \\dfrac{\\partial N}{\\partial x}}{-M} dy )\\]Exercise 16:\\[(4x+3y^2)dx + 2xydy=0\\]\\[M = 4x + 3y^2, \\quad N = 2xy\\]\\[\\dfrac{\\partial M}{\\partial y} - \\dfrac{\\partial N}{\\partial x} = 6y - 2y = 4y\\]\\[\\dfrac{\\dfrac{\\partial M}{\\partial y} - \\dfrac{\\partial N}{\\partial x}}{N} = \\dfrac{4y}{2xy} = \\dfrac{2}{x}\\]\\[I(x) = \\exp(\\int \\dfrac{2}{x}dx) = x^2\\]\\[(4x^3+3x^2y^2)dx + 2x^3ydy=0\\]\\[\\phi(x,y) = \\int^x (4x^3 + 3x^2y^2)dx = x^4 + x^3y^2 + f(y)\\]\\[\\phi(x,y) = \\int^y 2x^3ydy = x^3y^2 + f(x)\\]\\[\\phi(x,y) = x^4 + x^3y^2 = C\\]\\[I(x), I(y), I(x+y), I(xy)\\]Exercise 17:\\[(y^4+2y)dx + (xy^3+2y^4-4x)dy = 0\\]\\[M = y^4+2y, \\quad N = xy^3 + 2y^4 - 4x\\]\\[\\dfrac{\\partial M}{\\partial y} - \\dfrac{\\partial N}{\\partial x} = 4y^3 + 2 - (y^3 -4) =3y^3 + 6\\]\\[\\dfrac{\\dfrac{\\partial M}{\\partial y} - \\dfrac{\\partial N}{\\partial x}}{-M} = \\dfrac{3y^3+6}{-(y^4 + 2y)} = -\\dfrac{3}{y}\\]\\[I(y) = \\exp(\\int - \\dfrac{3}{y}dy) = y^{-3}\\]\\[(y+2y^{-2})dx + (x+2y-4x y^{-3})dy = 0\\]\\[\\phi(x,y) = \\int^x (y + 2y^{-2}) dx = xy + 2xy^{-2} + f(y)\\]\\[\\phi(x,y) = \\int^y (x+2y - 4xy^{-3})dy = xy + y^2 +2xy^{-2} + g(x)\\]\\[\\phi(x,y) = xy + y^2 + 2xy^{-2} = C\\]   \\(I(x+y)\\):\\[\\dfrac{\\partial}{\\partial x} (x^2+y)^2 = 2(x^2+y) \\cdot 2x\\]\\[\\dfrac{\\partial}{\\partial x} f(u(x,y)) = \\dfrac{df}{du} \\cdot \\dfrac{\\partial u}{\\partial x}\\]\\[(\\dfrac{\\partial M}{\\partial y} - \\dfrac{\\partial N}{\\partial x}) I = N \\dfrac{\\partial I}{\\partial x} - M \\dfrac{\\partial I}{\\partial y}\\]\\[I(x) = \\exp(\\int \\dfrac{\\dfrac{\\partial M}{\\partial y} - \\dfrac{\\partial N}{\\partial x}}{N} dx )\\]\\[I(y) = \\exp(\\int \\dfrac{\\dfrac{\\partial M}{\\partial y} - \\dfrac{\\partial N}{\\partial x}}{-M} dy )\\]\\[I(x+y) = \\exp(\\int \\dfrac{\\dfrac{\\partial M}{\\partial y} - \\dfrac{\\partial N}{\\partial x}}{N-M} d(x+y))\\]\\[I(xy) = \\exp(\\int \\dfrac{\\dfrac{\\partial M}{\\partial y} - \\dfrac{\\partial N}{\\partial x}}{Ny-Mx} d(xy))\\]\\[I(x-y) = \\exp(\\int \\dfrac{\\dfrac{\\partial M}{\\partial y} - \\dfrac{\\partial N}{\\partial x}}{N+M} d(x+y))\\]Exercise:\\[3xydx + 2x^2 dy = 0\\]\\[\\dfrac{\\partial M}{\\partial y} - \\dfrac{\\partial N}{\\partial x} = 3x - 4x = -x\\]\\[I(x) = \\exp(\\int \\dfrac{-x}{2x^3} dx) = x^{-1/2}\\]\\[3x^{1/2}ydx + 2x^{3/2}dy = 0\\]\\[\\phi(x,y) = 2 x^{3/2} y + f(y)\\]\\[\\phi(x,y) = 2x^{3/2} y + g(x)\\]\\[\\phi(x,y) = 2x^{3/2}y = C\\]\\[I(xy) = xy\\]\\[3x^2y^2 dx + 2x^3y dy = 0\\]\\[\\phi(x,y) = x^3y^2 = C\\]Exercise F.1:\\[(3xy+y^2)dx + (x^2+xy)dy = 0\\]\\[M = 3xy+y^2, \\quad N = x^2+xy\\]\\[\\dfrac{\\partial M}{\\partial y} - \\dfrac{\\partial N}{\\partial x} = 3x + 2y - (2x + y) = x +y\\]\\[I(x) = \\exp(\\int \\dfrac{dx}{x}) = x\\]\\[(3x^2 y + xy^2)dx + (x^3 + x^2y)dy = 0\\]\\[\\phi(x,y) = x^3y + \\dfrac{1}{2}x^2y^2 + f(y)\\]\\[\\phi(x,y) = x^3y + \\dfrac{1}{2}x^2y^2 + g(x)\\]\\[\\phi(x,y) = x^3y + \\dfrac{1}{2}x^2 y^2 = C\\]Exercise F.2:\\[2xydx + (4y+3x^2)dy = 0; y(1) = 1\\]\\[\\dfrac{\\partial M}{\\partial y} - \\dfrac{\\partial N}{\\partial x} = 2x - 6x = -4x\\]\\[I(y) = \\exp(\\int \\dfrac{-4x}{-2xy} dy) = y^2\\]\\[2xy^3 dx + (4y^3 + 3x^2 y^2)dy = 0\\]\\[\\phi(x,y) = x^2y^3 + f(y)\\]\\[\\phi(x,y) = y^4 + x^2y^3 + g(x)\\]\\[\\phi(x,y) = y^4 + x^2 y^3 = C = 2\\]First Order Linear ODE\\[y' + P(x)y = Q(x)\\]\\[(P(x)y - Q(x))dx + dy = 0\\]\\[\\dfrac{\\dfrac{\\partial M}{\\partial y} - \\dfrac{\\partial N}{\\partial x}}{N} = P(x)\\]\\[I(x) = \\exp(\\int P(x) dx)\\]\\[I(x) \\dfrac{dy}{dx} + P(x) I(x) y = Q(x) I(x)\\]\\[\\dfrac{d}{dx}[y \\exp(\\int P(x) dx)] = Q(x)I(x)\\]\\[y = \\dfrac{1}{I(x)} \\int Q(x) I(x) dx + \\dfrac{C}{I(x)}\\]Exercise 18:\\[y' + y = \\sin(x)\\]\\[P(x) = 1, \\quad Q(x) = \\sin(x)\\]\\[I(x) = \\exp(x)\\]\\[\\int Q(x) I(x) dx = \\int \\sin(x)\\exp(x)dx\\]\\[\\sin(x) = \\dfrac{\\exp(ix) - \\exp(-ix)}{2i}\\]\\[\\begin{align}&amp;\\int Q(x) I(x) dx = \\int \\sin(x)\\exp(x)dx\\\\=&amp; \\dfrac{\\exp(x)}{2}(\\sin x - \\cos x)\\end{align}\\]\\[y = \\dfrac{1}{2}(\\sin x - \\cos x) + C \\exp(-x)\\]Exercise 19:\\[y' = \\dfrac{y}{2x+y^3 \\exp(y)}\\]\\[\\dfrac{dx}{dy} = \\dfrac{2x+y^3 \\exp(y)}{y}\\]\\[x' - \\dfrac{2}{y}x = y^2 \\exp(y)\\]\\[I(y) = \\exp(\\int -\\dfrac{2}{y} dy) = y^{-2}\\]\\[\\int y^2\\exp(y) y^{-2} dy = \\exp(y)\\]\\[x = y^2 \\exp(y) + C y^2\\]Linearized ODE Bernoulli Equation: \\(y' + P(x)y = Q(x) \\cdot y^n\\). Let \\(u=y^{1-n}\\)\\[y'+P(x)y = Q(x) \\cdot y^n\\]when \\(n\\ne 0, 1\\), it is not linear ODE.\\[y^{-n}y' + P(x)y^{1-n} = Q(x)\\]Let \\(u = y^{1-n}\\)\\[\\dfrac{du}{dx} = (1-n)y^{-n} \\cdot \\dfrac{dy}{dx}\\]\\[\\dfrac{1}{1-n} \\cdot \\dfrac{du}{dx} + P(x) \\cdot u = Q(x)\\]\\[\\dfrac{du}{dx} + (1-n)P(x) \\cdot u = (1-n)Q(x)\\]   \\(\\dfrac{dv}{dy}y' + P(x)v(y)=Q(x)\\)\\[\\dfrac{dv}{dy}y' + P(x)v(y) = Q(x)\\]\\[\\dfrac{dv}{dy}y' = \\dfrac{dv}{dy} \\dfrac{dy}{dx} = \\dfrac{dv}{dx}\\]then it becomes linear. Riccati Equation: \\(y'=P(x)y^2 + Q(x)y + R(x)\\)\\[y' = P(x)y^2 + Q(x)y + R(x)\\]If we have found a solution (by any possibel ways), \\(s(x)\\), such that\\[s' = P(x)s^2 + Q(x)s + R(x)\\]Then the general soluton is\\[y = s + \\dfrac{1}{z}\\]and the functin \\(z\\) can be found as\\[0 = z' + (2P(x)s(x) + Q(x)) \\cdot z + P(x)\\]which is an linear ODE for \\(z\\).Fourier SeriesFourier SeriesExample (odd function square wave)\\[f(t) =\\begin{cases}-1, \\quad -T/2 &lt; t &lt; 0\\\\1, \\quad 0 &lt; t &lt; T/2\\end{cases}\\]\\[\\begin{align}f(t) = \\dfrac{4}{\\pi} \\sum_{n=1,3,5,\\dots}^{\\infty} \\dfrac{1}{n} \\sin\\left(n\\omega t\\right)\\end{align}\\]Multi-Variable FunctionsTaylor’s FormulaFor multi-variable function \\(f(x,y)\\). To expand it around \\((a,b)\\), let\\[\\begin{cases}x = a + (x_0-a)t\\\\y = b + (y_0-b)t\\end{cases}\\]then \\(f(x(t),y(t))=F(t)\\)\\[F(t) = F(0) + \\dfrac{F'(0)}{1!} t + \\dfrac{F''(0)}{2!}t^2 + \\dots\\]\\[F'(t) = (x_0-a)f_x + (y_0-b)f_y\\]\\[F''(t) = (x_0-a)^2f_{xx} + 2(x_0-a)(y_0-b)f_{xy} + (y_0-b)^2 f_{yy}\\]Implicit FunctionFrom \\(f(x,y)=0\\), we try to find the Taylor series of \\(y(x)\\).Then in a sense that, if we can uniquely determine all the derivatives \\(y^{(n)}(x_0)\\), then we can make sure \\(y(x)\\) is unique in a neighborhood of \\(x_0\\).\\[y(x) = y(x_0) + \\dfrac{y'(x_0)}{1!}(x-x_0) + \\dots\\]Let \\(R(x) = f(x,y(x)) = 0\\)\\[\\begin{align}R'(x_0) &amp;= \\dfrac{\\partial f}{\\partial x} \\dfrac{dx}{dx} + \\dfrac{\\partial f}{\\partial y} \\dfrac{dy}{dx}\\\\&amp;= \\dfrac{\\partial f}{\\partial x} + \\dfrac{\\partial f}{\\partial y} \\dfrac{dy}{dx}\\\\&amp;= 0\\end{align}\\]\\[y'(x_0) = -\\dfrac{\\dfrac{\\partial f}{\\partial x}(x_0,y_0)}{\\dfrac{\\partial f}{\\partial y}(x_0, y_0)}\\]We can also calculate \\(y''(x_0)\\), from\\[y' = -\\dfrac{f_x}{f_y}\\]\\[\\begin{align}y'' &amp;= -\\dfrac{(\\dfrac{d}{dx} f_x)f_y - f_x (\\dfrac{d}{dx}f_y)}{f_y^2}\\\\&amp;= -\\dfrac{(f_{xx}+f_{xy}y')f_y - f_x(f_{xy} + f_{yy}y')}{f_y^2}\\end{align}\\]3D VectorsPolar Coordinates\\[\\begin{cases}r = \\sqrt{x^2+y^2}\\\\\\theta = \\mathrm{atan2}(y,x) \\in (-\\pi, \\pi]\\end{cases}\\]\\[\\mathrm{atan2}(y, x)= \\begin{cases}\\arctan \\left(\\frac{y}{x}\\right) &amp; \\text { if } x&gt;0 \\\\ \\arctan \\left(\\frac{y}{x}\\right)+\\pi &amp; \\text { if } x&lt;0 \\text { and } y \\geq 0 \\\\ \\arctan \\left(\\frac{y}{x}\\right)-\\pi &amp; \\text { if } x&lt;0 \\text { and } y&lt;0 \\\\ \\frac{\\pi}{2} &amp; \\text { if } x=0 \\text { and } y&gt;0 \\\\ -\\frac{\\pi}{2} &amp; \\text { if } x=0 \\text { and } y&lt;0 \\\\ \\text { undefined } &amp; \\text { if } x=0 \\text { and } y=0 .\\end{cases}\\]Field TheoryCombination and Laplacian\\[\\nabla \\cdot (u \\vec{v}) = \\nabla u \\cdot \\vec{v} + u \\nabla \\cdot \\vec{v}\\]Proof:\\[\\begin{align}&amp;(\\dfrac{\\partial}{\\partial x}\\hat{i} + \\dfrac{\\partial}{\\partial y}\\hat{j} + \\dfrac{\\partial}{\\partial z}\\hat{k}) \\cdot (u v_x \\hat{i} + uv_y \\hat{j} + uv_z \\hat{k})\\\\=&amp; \\dfrac{\\partial (u v_x)}{\\partial x} + \\dfrac{\\partial (u v_y)}{\\partial y} + \\dfrac{\\partial (u v_z)}{\\partial z}\\\\=&amp; \\dfrac{\\partial u}{\\partial x} v_x + \\dfrac{\\partial u}{\\partial y} v_y + \\dfrac{\\partial u}{\\partial z} v_z + u \\dfrac{\\partial v_x}{\\partial x} + u \\dfrac{\\partial v_y}{\\partial y} + u \\dfrac{\\partial v_z}{\\partial z}\\end{align}\\]Non-Cartesian CoordinatesCylindrical Coordinates\\[\\begin{align}\\nabla &amp;= \\hat{i} \\dfrac{\\partial}{\\partial x} + \\hat{j} \\dfrac{\\partial}{\\partial y} + \\hat{k} \\dfrac{\\partial}{\\partial z}\\\\&amp;= (c \\hat{e_r} - s\\hat{e_\\theta}) \\left(\\dfrac{\\partial}{\\partial r} \\dfrac{\\partial r}{\\partial x} + \\dfrac{\\partial}{\\partial\\theta} \\dfrac{\\partial\\theta}{\\partial x} + \\dfrac{\\partial}{\\partial z}\\dfrac{\\partial z}{\\partial x}\\right) + \\dots\\end{align}\\]Note that from chain rule, it should really be written as \\(\\dfrac{\\partial r}{\\partial x} \\dfrac{\\partial}{\\partial r}\\).The \\(\\dfrac{\\partial r}{\\partial x}\\) will not be derivated by \\(\\dfrac{\\partial}{\\partial r}\\).Fourier MethodSturm-Liouville ProblemExample 1\\[\\begin{cases}y'' + \\lambda y = 0\\\\x \\in [0,L]\\\\y(0) = y(L) = 0\\end{cases}\\]\\[y = e^{rx}\\]\\[r = \\pm \\sqrt{\\lambda} i\\]\\[y(x) =\\begin{cases}A\\cos\\sqrt{\\lambda}x + B \\sin\\sqrt{\\lambda}x, \\quad \\lambda \\ne 0\\\\C + Dx, \\quad \\lambda = 0\\end{cases}\\]  \\(\\lambda=0\\) is not an eigenvalue, since the corresponding solution \\(y=0\\) is a trivial solution. The eigenvalue should corresponds to nontrivial solution.\\[\\begin{cases}0 = A \\cdot 1 + B \\cdot 0\\\\0 = A \\cos\\sqrt{\\lambda}L + B\\sin\\sqrt{\\lambda} L\\end{cases}\\]The condition for nontrivial solution\\[\\begin{vmatrix}1 &amp; 0\\\\\\cos\\sqrt{\\lambda} L &amp; \\sin\\sqrt{\\lambda} L\\end{vmatrix}=0\\]\\[\\begin{cases}\\lambda_n = \\dfrac{n^2 \\pi^2}{L^2}\\\\\\phi_n = \\sin \\dfrac{n\\pi x}{L}\\\\n = 1,2,3,\\dots\\end{cases}\\]Example 2\\[\\begin{cases}y'' - 2y' + \\lambda y =0\\\\y(0)=0, \\quad y(\\pi)=0\\end{cases}\\]multiplying \\(\\sigma(x)\\)\\[\\sigma y'' - 2\\sigma y' + \\lambda \\sigma y = 0\\]with\\[\\sigma' = -2\\sigma\\]Let’s take \\(\\sigma(x) = e^{-2x}\\), then\\[\\left(e^{-2x}y'\\right)' + \\lambda e^{-2x}y = 0\\]is a Sturm-Liouville problem.Diffusion EquationSeparation of VariablesExample 1\\[\\begin{cases}L[u] = a^2 u_{xx} - u_t = 0, \\quad (0 &lt; x &lt; L, 0 &lt; t &lt; \\infty)\\\\u(0,t) = u_1, \\quad u(L,t) = u_2, \\quad (0 &lt; t &lt; \\infty)\\\\u(x,0) = f(x), \\quad (0 &lt; x &lt; L)\\end{cases}\\]\\[u(x,t) = X(x)T(t)\\]\\[\\dfrac{X''}{X} = \\dfrac{1}{a^2} \\dfrac{T'}{T} = -k^2\\]\\[X =\\begin{cases}A\\cos kx + B\\sin kx, &amp; \\quad k \\ne 0\\\\D + Ex, &amp; \\quad k = 0\\end{cases}\\]\\[T =\\begin{cases}F e^{-k^2a^2 t}, &amp; \\quad k \\ne 0\\\\G, &amp; \\quad k = 0\\end{cases}\\]\\[u = H + Ix + (J\\cos kx + K \\sin kx)e^{-k^2a^2 t}\\]First apply boundary condition \\(u(0,t) = u_1\\).\\[H + Je^{-k^2 a^2 t} = u_1\\]Since \\((1, e^{-a^2k^2 t})\\) are linear independent\\[\\begin{cases}H = u_1\\\\J = 0\\end{cases}\\]\\[u = u_1 + Ix + (K\\sin kx) e^{-k^2a^2 t}\\]Now apply the boundary condition \\(u(L,t)=u_2\\)\\[u_1 + I L + (K \\sin k L) e^{-k^2 a^2 t} = u_2\\]\\[\\begin{cases}u_1 + IL = u_2\\\\K \\sin kL = 0\\end{cases}\\]To maintain the solution as robust as possible, set \\(\\sin kL = 0\\)\\[kL = n \\pi, \\quad (n = 1,2,3,\\dots)\\]\\[k_n = \\dfrac{n \\pi}{L}, \\quad (n = 1,2,3,\\dots)\\]  \\(n\\) doesn’t need to be \\(0\\), since that is included when \\(K = 0\\).Also, \\(n\\) doesn’t need to be negative, since that is included by changing \\(K\\).By superposition\\[u(x,t) = u_1 + \\left(\\dfrac{u_2-u_1}{L}\\right)x + \\sum_{n=1}^{\\infty} K_n \\sin\\dfrac{n\\pi x}{L} e^{-(n\\pi a/L)^2 t}\\]now apply initial condition \\(u(x,0) = f(x)\\)\\[u_1 + \\left(\\dfrac{u_2-u_1}{L}\\right)x + \\sum_{n=1}^{\\infty} K_n \\sin\\dfrac{n\\pi x}{L} = f(x)\\]\\[f(x) -\\left[ u_1 + \\left(\\dfrac{u_2-u_1}{L}\\right)x\\right] = F(x) = \\sum_{n=1}^{\\infty} K_n \\sin\\dfrac{n\\pi x}{L}\\]This is half range sin expansion of \\(F(x)\\).\\[K_n = \\dfrac{2}{L}\\int_0^L F(x)\\sin\\dfrac{n\\pi x}{L}\\, dx\\]note that if \\(f(x)\\) is not continuous, we will not get the solution corresponding to \\(f(x)\\).The value will be different at the discontinuous point.However, in engineering practice, we didn’t care this much.Fourier Transform\\[\\begin{cases}\\alpha^2 u_{xx} = u_{t}\\\\-\\infty &lt; x &lt; \\infty, 0 &lt; t &lt; \\infty\\\\u(\\pm \\infty, t) = u_{x}(\\pm \\infty, t) = 0\\\\u(x,0) = f(x)\\end{cases}\\]Let’s do FT w.r.t. \\(x\\).Note that when \\(u(\\pm \\infty, t) = u_{x}(\\pm \\infty, t) = 0\\), \\(F\\left\\{u_{xx}\\right\\} = (i\\omega)^2 \\hat{u}(\\omega, t)\\)\\[\\alpha^2 (i\\omega)^2 \\hat{u}(\\omega, t) = \\dfrac{\\partial}{\\partial t} \\hat{u}(\\omega,t)\\]\\[\\dfrac{d \\hat{u}}{d t} + \\alpha^2 \\omega^2 \\hat{u} = 0\\]\\[\\hat{u} = A \\cdot e^{-\\alpha^2 \\omega^2 t}\\]\\[A = \\hat{f}(\\omega)\\]\\[\\hat{u}(\\omega,t) = \\hat{f}(\\omega) e^{-\\alpha^2 \\omega^2 t} = \\hat{f}(\\omega) \\hat{g}(\\omega)\\]\\[g(x) = \\dfrac{1}{2\\alpha \\sqrt{\\pi t}} e^{-x^2/(4\\alpha^2 t)}\\]\\[u(x,t) = \\dfrac{1}{2\\alpha \\sqrt{\\pi t}} \\int_{-\\infty}^{\\infty} f(\\xi)e^{-(x-\\xi)^2/(4\\alpha^2 t)} \\, d\\xi\\]Laplace Transform\\[\\begin{cases}\\alpha^2 u_{xx} = u_{t}\\\\0 \\le x &lt; \\infty, \\quad 0 \\le t &lt; \\infty\\\\u(0,t) = g(t), \\quad 0 \\le t &lt; \\infty\\\\u(\\infty, t) = 0, \\quad 0 \\le t &lt; \\infty\\\\u(x,0) = 0, \\quad 0 \\le x &lt; \\infty\\end{cases}\\]Let’s do LT w.r.t. \\(t\\).Since \\(u(x,0) = 0\\).\\[\\alpha^2 \\overline{u}_{xx} = s \\overline{u}(x,s)\\]\\[\\overline{u}(x,s) = A e^{\\sqrt{s}x/\\alpha} + B e^{-\\sqrt{s}x/\\alpha}\\]apply \\(u(\\infty,t) = 0\\).\\[\\overline{u}(\\infty, s) = 0\\]\\[\\overline{u}(x,s) = B e^{-\\sqrt{s}x/\\alpha}\\]apply \\(u(0,t) = g(t)\\)\\[B = \\overline{g}(s)\\]\\[\\overline{u}(x,s) = \\overline{g}(s) e^{-\\sqrt{s}x/\\alpha}\\]\\[\\begin{align}u(x,t) &amp;= g(t) * \\dfrac{xe^{-x^2/(4\\alpha^2 t)}}{2\\alpha \\sqrt{\\pi} t^{3/2}}\\\\&amp;= \\dfrac{x}{2\\alpha\\sqrt{\\pi}} \\int_0^t g(t-\\tau) \\dfrac{e^{-x^2/(4\\alpha^2 \\tau)}}{\\tau^{3/2}} \\, d\\tau\\end{align}\\]Wave Equation\\[c^2 \\nabla^2 u = u_{tt}\\]Separation of Variable\\[\\begin{cases}c^2 y_{xx} = y_{tt}\\\\0 \\le x \\le L, \\quad 0 \\le t &lt; \\infty\\\\y(0,t) = 0\\\\y(L,t) = 0\\\\y(x,0) = f(x)\\\\y_{t}(x,0) = g(x)\\end{cases}\\]\\[y(x,t) = X(x)T(t)\\]\\[c^2 X'' T = X T''\\]\\[\\dfrac{X''}{X} = \\dfrac{1}{c^2}\\dfrac{T''}{T} = -k^2\\]\\[X =\\begin{cases}A + Bx, \\quad k = 0\\\\D\\cos kx + E \\sin kx, \\quad k \\ne 0\\end{cases}\\]\\[T =\\begin{cases}H + It, \\quad k = 0\\\\J\\cos kct + K \\sin kct, \\quad k \\ne 0\\end{cases}\\]\\[y(x,t) = (A+Bx)(H+It) + (D \\cos kx + E\\sin kx)(J \\cos kct + K \\sin kct)\\]First apply boundary condition \\(y(0,t) = 0\\)\\[A (H + It) + D (J \\cos kct + K\\sin kct) = 0\\]We choose \\(A = D = 0\\) so that the solution is as sobust as possible\\[y(x,t) = x (P + Qt) + \\sin kx (R \\cos kct + S \\sin kct)\\]apply boundary condition \\(y(L,t) = 0\\)\\[L(P+Qt) + \\sin kL (R\\cos kct + S\\sin kct) = 0\\]\\[\\begin{cases}P = Q = 0\\\\\\sin kL = 0\\end{cases}\\]\\[k_n = \\dfrac{n\\pi}{L}, \\quad n = 1,2,3\\]\\[y(x,t) = \\sum_{n=1}^{\\infty} \\sin \\dfrac{n\\pi x}{L} \\left(R_n \\cos\\dfrac{n\\pi ct}{L} + S_n \\sin\\dfrac{n\\pi ct}{L}\\right)\\]now apply initial condition \\(y(x,0) = f(x)\\) and \\(y_t(x,0)=g(x)\\)\\[f(x) = \\sum_{n=1}^{\\infty} R_n \\sin\\dfrac{n\\pi x}{L}\\]\\[g(x) = \\sum_{n=1}^{\\infty} \\dfrac{n\\pi c}{L} S_n \\sin \\dfrac{n\\pi x}{L}\\]they are half range sin expansion\\[R_n = \\dfrac{2}{L}\\int_0^L f(x)\\sin\\dfrac{n\\pi x}{L}\\, dx\\]\\[S_n = \\dfrac{2}{n\\pi c}\\int_{0}^{L} g(x)\\sin \\dfrac{n\\pi x}{L}\\, dx\\]Let’s look at\\[\\sin\\dfrac{n\\pi x}{L}\\cos\\dfrac{n\\pi ct}{L} = \\dfrac{1}{2}\\left(\\sin \\dfrac{n\\pi (x+ct)}{L} + \\sin\\dfrac{n\\pi (x-ct)}{L}\\right)\\]That is two wave travel to right \\(f(x-ct)\\) and left \\(f(x+ct)\\), with speed \\(c\\).\\[\\begin{cases}\\omega = 2\\pi / T\\\\k = 2\\pi / \\lambda\\\\c = \\dfrac{\\omega}{k} = \\dfrac{\\lambda}{T}\\end{cases}\\]Two Dimentional Case\\[\\begin{cases}c^2 (w_{xx} + w_{yy}) = w_{tt}\\\\0 \\le x \\le a\\\\0 \\le y \\le b\\\\0 \\le t &lt; \\infty\\\\w(0,y,t) = w(a,y,t) = w(x,0,t) = w(x,b,t) = 0\\\\w(x,y,0) = f(x,y)\\\\w_t(x,y,0) = 0\\end{cases}\\]\\[w(x,y,t) = X(x)Y(y)T(t)\\]\\[c^2(X''YT + XY''T) = XYT''\\]\\[\\dfrac{X''}{X}+\\dfrac{Y''}{Y} = \\dfrac{1}{c^2}\\dfrac{T''}{T} = -k^2\\]\\[\\begin{cases}T'' + k^2 c^2 T = 0\\\\X'' + \\alpha^2 X = 0\\\\Y'' + (k^2 - \\alpha^2)Y = 0\\end{cases}\\]\\[w(w,y;t) = (Ax+B)(Cy+D)(Et+G) + (H \\cos\\alpha x + I \\sin \\alpha x)(J \\cos \\sqrt{k^2-\\alpha^2}y + K \\sin\\sqrt{k^2-\\alpha^2}y)(L \\sin kct + M \\cos kct)\\]apply \\(w(0,y,t) = 0\\)\\[B(Cy+D)(Et+G) + H(J \\cos \\sqrt{k^2-\\alpha^2}y + K \\sin\\sqrt{k^2-\\alpha^2}y)(L \\sin kct + M \\cos kct) = 0\\]\\[B = H = 0\\]\\[w(w,y;t) = x(Cy+D)(Et+G) + \\sin \\alpha x(J \\cos \\sqrt{k^2-\\alpha^2}y + K \\sin\\sqrt{k^2-\\alpha^2}y)(L \\sin kct + M \\cos kct)\\]apply \\(w(x,0,t)=0\\)\\[Dx(Et+G) + J \\sin \\alpha x(L \\sin kct + M \\cos kct) = 0\\]\\[D = J = 0\\]\\[w(w,y;t) = xy(Et+G) + \\sin \\alpha x \\sin\\sqrt{k^2-\\alpha^2}y(L \\sin kct + M \\cos kct)\\]apply \\(w(a,y,t)=0\\)\\[ay(Et+G) + \\sin \\alpha a \\sin\\sqrt{k^2-\\alpha^2}y(L \\sin kct + M \\cos kct) = 0\\]\\[E = G = 0\\]\\[\\alpha = \\dfrac{m\\pi}{a}, \\quad m = 1,2,3\\]apply \\(w(x,b,t) = 0\\)\\[\\sqrt{k^2-\\alpha^2} = \\dfrac{n \\pi}{b}, \\quad n = 1,2,3\\]\\[k = \\pi\\sqrt{\\dfrac{m^2}{a^2} + \\dfrac{n^2}{b^2}}\\]\\[w(x,y,t) = \\sum_{n=1}^{\\infty}\\sum_{m=1}^{\\infty} \\sin \\dfrac{m\\pi x}{a} \\sin\\dfrac{n\\pi y}{b} \\left(G_{mn}\\sin \\omega_{mn} t + H_{mn} \\cos \\omega_{mn} t \\right)\\]\\[\\omega_{mn} = \\pi c\\sqrt{\\dfrac{m^2}{a^2} + \\dfrac{n^2}{b^2}}\\]apply \\(w_t(x,y,0) = 0\\)\\[G_{mn} = 0\\]\\[w(x,y,t) = \\sum_{n=1}^{\\infty}\\sum_{m=1}^{\\infty} H_{mn} \\sin \\dfrac{m\\pi x}{a} \\sin\\dfrac{n\\pi y}{b} \\cos \\omega_{mn} t\\]apply \\(w(x,y,0) = f(x,y)\\)\\[f(x,y) = \\sum_{n=1}^{\\infty}\\sum_{m=1}^{\\infty} H_{mn} \\sin \\dfrac{m\\pi x}{a} \\sin\\dfrac{n\\pi y}{b}\\]This is a double Fourier series.\\[H_{mn} = \\dfrac{4}{ab} \\int_0^b \\int_0^a f(x,y) \\sin\\dfrac{n\\pi x}{a} \\sin \\dfrac{n\\pi y}{b} \\, dx dy\\]" }, { "title": "Engineering Mathematics", "url": "/posts/engineering-mathematics/", "categories": "math", "tags": "math", "date": "2023-07-23 07:00:00 +0000", "snippet": "From Dr. Chun-Yao Wang’ LectureInteresting Formula\\[\\dfrac{1-x^N}{1-x} = 1 + x + x^2 + \\dots + x^{N-1}\\]gong bi ci fang jian yi, chu yi gong bi jian yi\\[\\sum_{n=0}^{N-1} e^{jk(2\\pi/N)n} = 0, \\text{ when } k \\ne 0, \\pm N, \\pm 2N, \\dots\\]First-Order First-Degree ODEIntegration by Parts\\[\\int u dv = uv - \\int vdu\\]Ordinary, Partial, Order and DegreeOrdinary differential equation.Partial differential equation.Order of the differential equation.Degree of the differential equation.Linear differential equation.General solution.Particular solution.Singular solution.\\[y'=f(x,y)\\]or\\[M(x,y)dx + N(x,y)dy = 0\\]Direct IntegralSeperable EquationSeperable After Changing Variable First-Order First-Degree Homogeneous ODE, \\(y'=f(y/x)\\)   \\((a_1 x + b_1 y + c_1)dx + (a_2 x +b_2 y + c2)dy = 0\\)   \\(y'=f(ax+by+c)\\)Exact Differential Equations\\[\\begin{align}&amp; M(x,y)dx + N(x,y)dy = 0\\\\&amp; \\dfrac{\\partial M}{\\partial y} = \\dfrac{\\partial N}{\\partial x}\\end{align}\\]Integrating Factors\\[I(x,y)M(x,y)dx + I(x,y)N(x,y)dy = 0 \\quad \\text{ is exact differential equation}\\]\\[I(x) = \\exp(\\int \\dfrac{\\dfrac{\\partial M}{\\partial y} - \\dfrac{\\partial N}{\\partial x}}{N} dx )\\]\\[I(y) = \\exp(\\int \\dfrac{\\dfrac{\\partial M}{\\partial y} - \\dfrac{\\partial N}{\\partial x}}{-M} dy )\\]\\[I(x+y) = \\exp(\\int \\dfrac{\\dfrac{\\partial M}{\\partial y} - \\dfrac{\\partial N}{\\partial x}}{N-M} d(x+y))\\]\\[I(xy) = \\exp(\\int \\dfrac{\\dfrac{\\partial M}{\\partial y} - \\dfrac{\\partial N}{\\partial x}}{Ny-Mx} d(xy))\\]\\[I(x-y) = \\exp(\\int \\dfrac{\\dfrac{\\partial M}{\\partial y} - \\dfrac{\\partial N}{\\partial x}}{N+M} d(x+y))\\]First Order Linear ODE\\[y'+P(x)y = Q(x)\\]\\[I(x) = \\exp(\\int P(x)dx)\\]\\[y = \\dfrac{1}{I(x)} \\int Q(x) I(x) dx + \\dfrac{C}{I(x)}\\]Linearized ODE Bernoulli Equation: \\(y' + P(x)y = Q(x) \\cdot y^n\\). Let \\(u=y^{1-n}\\)   \\(\\dfrac{dv}{dy}y' + P(x)v(y)=Q(x)\\) Riccati Equation: \\(y'=P(x)y^2 + Q(x)y + R(x)\\)Linear ODEWe concentrate on linear ODE\\[y'' + p(x)y' + q(x)y = f(x)\\]If \\(f(x)=0\\), we call it homogeneous equation.Otherwise non-homogeneous equation.Linear ODE TheoryHomogeneous Equation Theorem 2.2:Let \\(y_1(x)\\) and \\(y_2(x)\\) be solutoins of \\(y''+p(x)y'+q(x)y = 0\\).Then any linear combination of these solutions, e.g., \\(c_1 y_1(x) + c_2 y_2(x)\\), is also a solutoin. Definition 2.1: Linear Dependence/Independence\\[c_1 y_1 + c_2 y_2 + \\dots + c_n y_n = 0\\]only when\\[c_1 = c_2 = \\dots = 0\\] Wronskian\\[\\begin{equation}W(x) = \\left|\\begin{array}{rr}y_1 &amp; y_2 \\\\y_1' &amp; y_2'\\end{array}\\right| = y_1(x)y_2'(x) - y_1'(x)y_2(x)\\end{equation}\\]If \\(W(x) = 0\\), they are linear dependent. Otherwise L.I.The Wronskian also apply for more than two functions.\\[\\begin{equation}W(x) = \\left|\\begin{array}{rr}y_1 &amp; y_2 &amp; y_3\\\\y_1' &amp; y_2' &amp; y_3'\\\\y_1'' &amp; y_2'' &amp; y_3''\\end{array}\\right|\\end{equation}\\] Abel’s Identity\\[y'' + P(x)y' + Q(x)y = 0\\]  \\(y_1, y_2\\) are solutions. Then\\[W(x) = C \\cdot \\exp(-\\int P(x)dx)\\] Theorem 2.4 (YouTube 8C):Let \\(y_1, y_2\\) be linearly independent solutions of \\(y'' + P(x)y' + Q(x)y = 0\\).Then every solution is a linear combination of \\(y_1\\) and \\(y_2\\).\\[y'' + p(x)y' + q(x)y = 0\\]with initial conditon \\(y(x_0)=A, y'(x_0)=B\\)\\[y = C_1 y_1 + C_2 y_2\\]Cramer’s rule\\[\\begin{bmatrix}y_1(x_0) &amp; y_2(x_0)\\\\y_1'(x_0) &amp; y_2'(x_0)\\end{bmatrix}\\begin{bmatrix}C_1\\\\C_2\\end{bmatrix}=\\begin{bmatrix}A\\\\B\\end{bmatrix}\\]\\[\\begin{equation}C_1 = \\dfrac{1}{W(x_0)} \\left|\\begin{array}{rr}A &amp; y_2(x_0)\\\\B &amp; y_2'(x_0)\\\\\\end{array}\\right|\\end{equation}\\]\\[\\begin{equation}C_2 = \\dfrac{1}{W(x_0)} \\left|\\begin{array}{rr}y_1(x_0) &amp; A\\\\y_1'(x_0) &amp; B\\\\\\end{array}\\right|\\end{equation}\\]Nonhomogeneous Equation\\[y''+p(x) + q(x)y = f(x)\\]Theorem 2.5:  \\(y_1, y_2\\) is linearly independent solutions for \\(y''+p(x)y'+q(x)y=0\\).If \\(y_p\\) is solution for the nonhomogeneous equation, then any solution can be given by\\[c_1 y_1 + c_2 y_2 + y_p\\]Linear Constant Coefficient ODE\\[y'' + a_1 y' + a_0 y =R(x)\\]Homogeneous Solutions\\[\\lambda^2 + a_1 \\lambda + a_0 = 0\\]\\[\\lambda = \\dfrac{-a_1 \\pm \\sqrt{a_1^2 - 4a_0}}{2}\\]Case 1: \\(a_1^2 - 4a_0 &gt; 0\\)\\[\\begin{align}\\lambda_1 &amp;= \\dfrac{-a_1 + \\sqrt{a_1^2 - 4a_0}}{2}\\\\\\lambda_2 &amp;= \\dfrac{-a_1 - \\sqrt{a_1^2 - 4a_0}}{2}\\end{align}\\]\\[y_h(x) = C_1 \\exp(\\lambda_1 x) + C_2 \\exp(\\lambda_2 x)\\]Case 2: \\(a_1^2 - 4a_0 = 0\\)\\[\\begin{align}\\lambda &amp;= \\dfrac{-a_1}{2}\\end{align}\\]\\[y_h(x) = C_1 \\exp(\\lambda x) + C_2 x\\exp(\\lambda x)\\]Case 3: \\(a_1^2 - 4a_0 &lt; 0\\)\\[\\lambda = \\alpha \\pm i\\beta\\]\\[y_h(x) = C_1 \\exp(\\alpha x) \\cos(\\beta x) + C_2 \\exp(\\alpha x) \\sin(\\beta x)\\]Method of Undetermined Coefficient\\[y^{(n)} + a_{n-1} y^{(n-1)} + \\dots + a_1 y^{(1)} + a_0 y =R(x)\\] \\(R(x)\\) \\(y_p(x)\\) \\(R_1(x)+R_2(x)\\) \\(y_{p1}(x)+y_{p2}(x)\\) \\(k\\) \\(A\\) \\(e^{ax}\\) \\(Ae^{ax}\\) \\(\\cos ax\\) \\(A\\cos ax + B\\sin ax\\) \\(\\sin ax\\) \\(A\\cos ax + B\\sin ax\\) \\(a_n x^n + \\dots + a_1 x + a_0\\) \\(A_n x^n + \\dots + A_1 x + A_0\\) \\(e^{ax}\\cos bx \\quad\\) or \\(\\quad e^{ax}\\sin bx\\) \\(e^{ax}(A\\cos bx + B\\sin bx)\\) \\(e^{ax}(a_n x^n + \\dots + a_1 x + a_0)\\) \\(e^{ax}(A_n x^n + \\dots + A_1 x + A_0)\\) \\(\\cos bx (a_n x^n + \\dots + a_1 x + a_0)\\) \\((A_n x^n + \\dots + A_0)\\cos bx + (B_n x^n + \\dots + B_0)\\sin bx\\) \\(e^{ax} \\cos bx (a_n x^n + \\dots + a_0)\\) \\(e^{ax}(A_n x^n + \\dots + A_0)\\cos bx + e^{ax}(B_n x^n + \\dots + B_0)\\sin bx\\) Theorem: If the \\(y_p(x)\\) in the table is in the homogeneous solution, we should use \\(x^m y_p(x)\\), where \\(m\\) is the minimum positive integer such that \\(x^m y_p(x)\\) is not in the homogeneous solution.This claim can be proved by the method of variation of parameters.Method of Variation Parameter\\[y'' + a_1(x)y' + a_0(x)y = R(x)\\]\\[y_h(x) = C_1 y_1(x) + C_2 y_2(x)\\]\\[y_p(x) = \\phi_1(x)y_1(x) + \\phi_2(x)y_2(x)\\]\\[\\begin{bmatrix}y_1 &amp; y_2\\\\y_1' &amp; y_2'\\end{bmatrix}\\begin{bmatrix}\\phi_1'\\\\\\phi_2'\\end{bmatrix}=\\begin{bmatrix}0\\\\R(x)\\end{bmatrix}\\]\\[\\phi_1(x) = \\int \\dfrac{-R(x)y_2(x)}{W(y_1,y_2)}dx\\]\\[\\phi_2(x) = \\int \\dfrac{R(x)y_1(x)}{W(y_1,y_2)}dx\\]If third order\\[\\begin{bmatrix}y_1 &amp; y_2 &amp; y_3\\\\y_1' &amp; y_2' &amp; y_3'\\\\y_1'' &amp; y_2'' &amp; y_3''\\end{bmatrix}\\begin{bmatrix}\\phi_1'\\\\\\phi_2'\\\\\\phi_3'\\end{bmatrix}=\\begin{bmatrix}0\\\\0\\\\R(x)\\end{bmatrix}\\]Cauchy-Euler Equation\\[a_n x^n y^{(n)} + \\dots + a_1 x y^{(1)} + a_0 y = R(x)\\]Let \\(x = \\exp(u)\\), \\(u = \\ln x\\) , \\(D_u = \\dfrac{d}{du}\\)\\[x^n \\dfrac{d^n y}{dx^n} = x^n (D_x)^n y = D_u (D_u - 1) (D_u - 2) \\dots (D_u - n + 1)y\\]Legendre ODE\\[a_n (bx + c)^n y^{(n)} + \\dots + a_1 (bx+c) y^{(1)} + a_0 y = R(x)\\]Let \\(z = bx + c\\)\\[\\dfrac{d^n y}{dx^n} = b^{n} \\dfrac{d^n y}{dz^n}\\]Reduction of Order (13A)\\[y'' + P(x)y' + Q(x)y = R(x)\\]if \\(u(x)\\) is a homogenuous solution\\[u'' + P(x)u' + Q(x)u = 0\\]Let\\[y(x) = u(x)v(x)\\]System of Linear ODEsA linear first-order system of \\(n\\) equations in the \\(n\\) unknowns \\(x_1(t),\\dots,x_n(t)\\) is of the form\\[\\begin{align}a_{11}(t) x_1' + \\dots + a_{1n}(t)x_n' + b_{11}(t) x_1 + \\dots + b_{1n}(t) x_n &amp;= f_1(t)\\\\&amp;\\vdots\\\\a_{n1}(t) x_1' + \\dots + a_{nn}(t)x_n' + b_{n1}(t) x_1 + \\dots + b_{nn}(t) x_n &amp;= f_n(t)\\\\\\end{align}\\]A linear second-order system will be similar.Laplace TransformThe Laplace Transform\\[F(s) = \\int_{0}^{\\infty} f(t)e^{-st}dt\\]The Laplace transform table \\(f(t)\\) \\(F(s)\\) Notes \\(t^{n}\\) \\(\\dfrac{n!}{s^{n+1}}\\) \\(n = 0,1,2,3,\\dots\\) \\(e^{at}\\) \\(\\dfrac{1}{s-a}\\)   \\(\\sin(at)\\) \\(\\dfrac{a}{s^2+a^2}\\)   \\(\\cos(at)\\) \\(\\dfrac{s}{s^2+a^2}\\)   \\(\\delta(t)\\) \\(1\\)   The inverse Laplace transform table \\(F(s)\\) \\(f(t)\\) Notes \\(\\dfrac{1}{s^{n+1}}\\) \\(\\dfrac{t^n}{n!}\\) \\(n = 0,1,2,3,\\dots\\) \\(\\dfrac{1}{s-a}\\) \\(e^{at}\\)   \\(\\dfrac{1}{s^2+a^2}\\) \\(\\dfrac{1}{a}\\sin(at)\\)   \\(\\dfrac{s}{s^2+a^2}\\) \\(\\cos(at)\\)   \\[\\mathscr{L}\\{c_1 f(t) + c_2 g(t)\\} = c_1 F(s) + c_2 G(s)\\]\\[\\mathscr{L}\\{f(t)e^{at}\\} = F(s-a), \\quad a \\in \\mathbb{R}\\]\\[\\mathscr{L}\\{f(t-a)u(t-a)\\} = e^{-as}F(s), \\quad a &gt; 0\\]\\[\\mathscr{L}\\{f(at)\\} = \\dfrac{1}{a}F(\\dfrac{s}{a}), \\quad a &gt; 0\\]\\[\\mathscr{L}\\{\\int_0^t f(\\tau)d\\tau\\} = \\dfrac{1}{s}F(s)\\]\\[\\mathscr{L}\\{\\dfrac{f(t)}{t}\\} = \\int_{s}^{\\infty} F(u)du\\]\\[f(t)*g(t) = \\int_0^t f(\\tau)g(t-\\tau) d\\tau\\]\\[\\mathscr{L}\\{f(t)*g(t)\\} = F(s)G(s)\\]Heaviside Expansion\\[F(s) = \\dfrac{P(s)}{Q(s)}\\]Assume \\(P(s), Q(s)\\) are real polynomials, they have no common factors, and the order of \\(Q(s)\\) is higher than \\(P(s)\\).Case 1: \\(Q(s)=0\\) has \\(n\\) distinct roots.\\[\\dfrac{P(s)}{Q(s)} = \\dfrac{A_1}{s-a_1} + \\dfrac{A_2}{s-a_2} + \\dots + \\dfrac{A_n}{s-a_n}\\]\\[A_k = \\dfrac{P(s)}{Q(s)} \\cdot (s-a_k)\\]Case 2: \\(Q(s)=0\\) has repeated roots, e.g., \\(m\\)-repeated \\(a_k\\).\\[\\dfrac{P(s)}{Q(s)} = \\dfrac{A_1}{s-a_1} + \\dots + \\dfrac{C_m}{(s-a_k)^m} + \\dfrac{C_{m-1}}{(s-a_k)^{m-1}} + \\dots \\dfrac{C_1}{s-a_k} + \\dots\\]\\[C_m = \\lim_{s\\to a_k} [\\dfrac{P(s)}{Q(s)} (s-a_k)^m ]\\]\\[C_{m-1} = \\lim_{s\\to a_k} \\{ \\dfrac{d}{ds}[\\dfrac{P(s)}{Q(s)} (s-a_k)^m]\\}\\]\\[C_{m-1} = \\dfrac{1}{2!} \\lim_{s\\to a_k} \\{ \\dfrac{d^2}{ds^2}[\\dfrac{P(s)}{Q(s)} (s-a_k)^m]\\}\\]Case 3: complex conjugate roots\\[\\dfrac{As + B}{(s-a)^2+b^2}\\]\\[\\lim_{s\\to a+b j} \\{\\dfrac{P(s)}{Q(s)}[(s-a)^2+b^2]\\} = A(a+bj) + B\\]Solution of Initial Value Problems\\[\\mathscr{L}\\{f^{(n)}(t)\\} = s^n F(s) - s^{n-1} f(0) - s^{n-2}f'(0) - \\dots - f^{(n-1)}(0)\\]If all initial value are zero, then the Laplace transform of the impulse response can be easily calculated as \\(H(s)\\).Also, the value \\(H(j\\omega)\\) is very meanful. First\\[H(j\\omega) = \\int_{-\\infty}^{\\infty} h(t)e^{-j\\omega t} dt, \\quad \\text{(because h(t) = 0 when t is less than 0)}\\]which is the Fourier transform of the impulse response.Secondly, if the input is \\(e^{j\\omega t}\\) (for \\(t &gt; 0\\)), and all inital condition is still zero.\\[y(t) = \\int_{0}^{t} h(\\tau) e^{j\\omega (t-\\tau)} d\\tau = e^{j\\omega t} \\int_{0}^{t} h(\\tau) e^{-j\\omega \\tau} d\\tau\\]Thus, the response will approach to \\(H(j\\omega) e^{j\\omega t}\\) when \\(t \\to \\infty\\).ODE with Polynomial Coefficients\\[\\mathscr{L}\\{t^n f(t)\\} = (-1)^n\\dfrac{d^n F(s)}{ds^n}\\]Systems of Linear ODEs20ESeries SolutionsPower SeriesDefinition 4.4 (Power Series):\\[\\sum_{n=0}^{\\infty} a_n (x-x_0)^n\\]It is obvious it converges at \\(x = x_0\\).Theorem 4.5:If the power series converge at \\(x_1\\), then it converges absolutely for all \\(x\\) such that \\(\\vert x-x_0 \\vert &lt; \\vert x_1 - x_0\\vert\\).Theorem:There are three possibilities: It converges only at \\(x_0\\). It converges in a finite interval, the endpoints may or may not converge. \\((x_0-r, x_0+r)\\) is called the open interval of the convergence. It converges in \\(\\mathbb{R}\\).Theorem 4.6:For series \\(\\sum_{0}^{\\infty} b_n\\), where \\(b_n \\ne 0\\) for all \\(n\\). Then if \\(\\lim_{n\\to\\infty} \\vert \\dfrac{b_{n+1}}{b_n} \\vert = L\\) exists, if \\(L &lt; 1\\), then the series converges absolutely. if \\(L = 1\\), no conclusion. if \\(L &gt; 1\\), the series diverges.Taylor Series:\\[\\sum_{n=0}^{\\infty} \\dfrac{1}{n!}f^{(n)}(x_0)(x-x_0)^n\\]Definition 4.1 (Analytic Function):A function \\(f\\) is analytic at \\(x_0\\) if \\(f(x)\\) has a power series representation\\[f(x) = \\sum_{n=0}^{\\infty} a_n (x-x_0)^n\\]in a non-zero interval \\((x_0-r, x_0+r)\\).Solution of IVP (21E)Theorem 4.1:Let \\(p\\) and \\(q\\) be analytic at \\(x_0\\). Then the initial value problem\\[y' + p(x)y + q(x); \\quad y(x_0) = y_0\\]has a solution that is analytic at \\(x_0\\).Theorem 4.2:Let \\(p, q\\) and \\(f\\) be analytic at \\(x_0\\). Then the initial value problem\\[y'' + p(x)y' + q(x)y = f(x); \\quad y(x_0)=A, y'(x_0) = B\\]has a unique solution that is also analytic at \\(x_0\\).Recurrence Relations (22B)match of termsSingular Points (23A) Ordinary point:\\(x_0\\) is an ordinary point of equation\\[P(x)y'' + Q(x)y' + R(x)y = F(x)\\]if \\(P(x_0) \\ne 0\\) and \\(Q(x)/P(x), R(x)/P(x)\\) and \\(F(x)/P(x)\\) are analytic at \\(x_0\\). Singular point:If it is not ordinary point, it is singular point. Regular singular point:\\(x_0\\) is a regular singular point of equation\\[P(x)y'' + Q(x)y' + R(x)y = 0\\]if \\(x_0\\) is a singular point, and \\((x-x_0)\\dfrac{Q(x)}{P(x)}\\) and \\((x-x_0)^2\\dfrac{R(x)}{P(x)}\\) are analytic at \\(x_0\\). Irregular singular point:A singular point that is not regular is said to be an irregular singular point.Frobenius Series (23C)\\[y'' + \\dfrac{p(x)}{x}y' + \\dfrac{q(x)}{x^2} y = 0\\]\\[F(r) = r(r-1) + p_0 r + q_0 = 0\\]\\[\\begin{align}F(r+m) c_m &amp;= \\left[(m+r)(m+r-1) + (m+r)p_0 + q_0\\right] c_m\\\\&amp;= -\\sum_{k=0}^{m-1} \\left[(k+r)p_{m-k} + q_{m-k}\\right] c_k, \\quad m \\ge 1\\end{align}\\] If \\(r_1 - r_2\\) is not an (positive or negative or zero) integer.\\[\\begin{align}y_1(x) &amp;= \\vert x \\vert^{r_1} \\sum_{m=0}^{\\infty} c_{1,m} x^m\\\\y_2(x) &amp;= \\vert x \\vert^{r_2} \\sum_{m=0}^{\\infty} c_{2,m} x^m\\end{align}\\] If \\(r_1 - r_2 = 0\\)\\[\\begin{align}y_1(x) &amp;= \\vert x \\vert^{r} \\sum_{m=0}^{\\infty} c_m x^m\\\\y_2(x) &amp;= y_1(x) \\ln \\vert x \\vert + \\vert x \\vert ^{r} \\sum_{m=1}^{\\infty} d_m x^m\\end{align}\\]where\\[d_m = \\left( \\dfrac{\\partial c_m}{\\partial r}\\right)_{r=r_1}, \\quad m \\ge 1\\]The book Ordinary Differential Equations, Chapter 36 well explains why the formula takes the absolute value.It is just meant to maintain the function to be real value when \\(x&lt;x_0\\).If we only consider \\(x&gt;x_0\\), the absolute value is useless.Also, if \\(r\\) is a integer number, the absolute value is not necessary.Special Function\\[y(x) = \\sum_{m=0}^{\\infty} C_m x^m\\]Hermite Polynomials\\[y'' - 2x y' + 2ny = 0\\]\\[C_m = \\dfrac{2(m-2-n)}{m(m-1)}C_{m-2}, \\quad m\\ge 2\\]\\[y(x) = A_1 y_1(x) + A_2 y_2(x)\\]if \\(n\\) is some nonnegative integer (\\(n \\ge 0\\)), then one of the solution is a polynomial (with finite terms).Fourier SeriesFourier SeriesIf function \\(f(t)\\) has period \\(T\\), then \\(\\omega = 2\\pi/T\\)\\[f(t) \\sim A_0 + \\sum_{n=1}^{\\infty} A_n \\sin(n\\omega t + \\varphi_n)\\]or\\[\\begin{align}f(t) &amp;\\sim \\dfrac{a_0}{2} + \\sum_{n=1}^{\\infty} [a_n \\cos(n\\omega t) + b_n \\sin(n\\omega t)]\\\\a_0 &amp;= 2A_0 = \\dfrac{2}{T} \\int_T f(t)dt\\\\a_n &amp;= A_n \\sin\\varphi_n = \\dfrac{2}{T} \\int_T f(t)\\cos(n\\omega t)dt\\\\b_n &amp;= A_n \\cos\\varphi_n = \\dfrac{2}{T} \\int_T f(t)\\sin(n\\omega t)dt\\end{align}\\]or\\[f(t) \\sim \\sum_{n=-\\infty}^{\\infty} c_n e^{j n \\omega t}\\]\\[c_n = \\dfrac{1}{T}\\int_T f(t) e^{-j n \\omega t} dt\\]Examples Impulse Train\\[f(t) = \\sum_{k=-\\infty}^{\\infty} \\delta(t-kT)\\]\\[f(t) = \\dfrac{1}{T} \\sum_{n=-\\infty}^{\\infty} e^{-jn\\omega t}\\] Odd function square wave\\[f(t) =\\begin{cases}-1, \\quad -T/2 &lt; t &lt; 0\\\\1, \\quad 0 &lt; t &lt; T/2\\end{cases}\\]\\[\\begin{align}f(t) = \\dfrac{4}{\\pi} \\sum_{n=1,3,5,\\dots}^{\\infty} \\dfrac{1}{n} \\sin\\left(n\\omega t\\right)\\end{align}\\] Even function square wave\\[f(t) =\\begin{cases}-1, \\quad -T/2 &lt; t &lt; -T/4 \\quad \\text{ or } \\quad T/4 &lt; t &lt; T/2\\\\1, \\quad -T/4 &lt; t &lt; T/4\\end{cases}\\]\\[f(t) = \\dfrac{4}{\\pi} \\left( \\cos(\\omega t) - \\dfrac{1}{3} \\cos(3\\omega t) + \\dfrac{1}{5} \\cos(5\\omega t) - \\dfrac{1}{7} \\cos(7\\omega t) + \\dots \\right)\\]Convergence (25C)Fourier Transform\\[F(\\omega) = \\int_{-\\infty}^{\\infty} f(t) \\exp(-j\\omega t) dt\\]\\[f(t) = \\dfrac{1}{2\\pi} \\int_{-\\infty}^{\\infty} F(\\omega) \\exp(j\\omega t) d\\omega\\]Examples Convolution theorem\\[f(t)*g(t) \\leftrightarrow F(\\omega)G(\\omega)\\] FT of multiplication\\[f(t)g(t) \\leftrightarrow \\dfrac{1}{2\\pi} F(\\omega)*G(\\omega)\\] Time scaling\\[f(at) \\leftrightarrow \\dfrac{1}{\\vert a \\vert} F\\left(\\dfrac{\\omega}{a}\\right)\\] Frequency scaling\\[F(a\\omega) \\leftrightarrow \\dfrac{1}{\\vert a \\vert} f\\left(\\dfrac{t}{a}\\right)\\] finite unit step\\[f(t) = u(t) - u(t-T)\\]\\[F(\\omega) = T \\cdot \\exp(-j\\omega T / 2) \\cdot \\dfrac{\\sin (\\omega T/2)}{\\omega T/2}\\] comb function (impulse train)\\[\\mathrm{comb}(t) = \\sum_{n=-\\infty}^{\\infty} \\delta(t-n)\\]\\[\\mathrm{comb}(t) \\leftrightarrow \\mathrm{comb}\\left(\\dfrac{\\omega}{2\\pi}\\right)\\] Ideal low pass\\[\\mathrm{RECT}(\\omega) =\\begin{cases}1 &amp; \\vert \\omega \\vert &lt; \\dfrac{1}{2}\\\\0 &amp; \\vert \\omega \\vert &gt; \\dfrac{1}{2}\\end{cases}\\]\\[f(t) = \\dfrac{\\sin \\left(\\dfrac{t}{2}\\right)}{\\pi t} = \\dfrac{1}{2\\pi} \\mathrm{sinc}\\left(\\dfrac{t}{2\\pi}\\right)\\]\\[\\mathrm{sinc}(x) = \\dfrac{\\sin(\\pi x)}{\\pi x}\\]If the bandwidth is \\(\\omega_b\\), \\(\\mathrm{RECT}\\left(\\dfrac{\\omega}{2\\omega_b}\\right)\\)\\[f(t) = \\dfrac{\\omega_b}{\\pi} \\mathrm{sinc}\\left(\\dfrac{\\omega_b t}{\\pi}\\right)\\]Multi-Variable FunctionsFrom Prof. Yen’s LecturePartial DerivativesFor function \\(f(x,y)\\)\\[f_{xy} = \\dfrac{\\partial}{\\partial y} \\Big(\\dfrac{\\partial f}{\\partial x}\\Big) = \\dfrac{\\partial^2 f}{\\partial y \\partial x}\\]Theorem (Equalty of Derivatives): if \\(f_x, f_y, f_{xy}, f_{yx}\\) are all continuous in some neighborhood of \\((x_0,y_0)\\), then \\(f_{xy}=f_{yx}\\).We say \\(f\\) is \\(C^n\\) if all of the partial derivatives of \\(f\\), through \\(n\\)th order, are continuous.For example, \\(f\\) is \\(C^2\\) in \\(\\mathbb{R}\\) if \\(f_x, f_y, f_{xx}, f_{xy}, f_{yy}\\) are continuous in \\(\\mathbb{R}\\).Chain RuleFor function \\(f(x(t),y(t)) = R(t)\\)\\[\\dfrac{d R}{dt} = \\dfrac{\\partial f}{\\partial x} \\dfrac{d x}{dt} + \\dfrac{\\partial f}{\\partial y} \\dfrac{d y}{dt}\\]For function \\(f(u,v) = f(u(x,y), v(x,y)) = F(x,y)\\)\\[\\dfrac{\\partial F}{\\partial x} = \\dfrac{\\partial f}{\\partial u} \\dfrac{\\partial u}{\\partial x} + \\dfrac{\\partial f}{\\partial v} \\dfrac{\\partial v}{\\partial x}\\]Comments: we have to use a different symbol (e.g., \\(R\\) or \\(F\\)) for the composite function, otherwise it is very easy to make mistakes.Taylor’s FormulaFor single variable function\\[\\begin{align}f(x) &amp;= f(a) + \\dfrac{f'(a)}{1!}(x-a) + \\dfrac{f''(a)}{2!}(x-a)^2 + \\dots + \\dfrac{f^{(n-1)}(a)}{(n-1)!}(x-a)^{n-1} + \\dfrac{f^{(n)}(\\xi)}{n!}(x-a)^n\\end{align}\\]For multi-variable function\\[f(x_0,y_0) = f(a,b) + \\dfrac{1}{1!}D f\\vert_{a,b} + \\dots + \\dfrac{1}{(n-1)!} D^{n-1} f\\vert_{a,b} + \\dfrac{1}{n!}D^n f\\vert_{\\xi,\\eta}\\]\\[D = (x_0-a)\\dfrac{\\partial}{\\partial x} + (y_0-b)\\dfrac{\\partial}{\\partial y}\\]\\[Df = (x_0-a)f_x + (y_0-b)f_y\\]\\[D^2 f = (x_0-a)^2f_{xx} + 2(x_0-a)(y_0-b)f_{xy} + (y_0-b)^2 f_{yy}\\]or if it doesn’t cause confusion\\[f(x,y)=f(x_0,y_0) + f_x(x-x_0) + f_y(y-y_0) + \\dfrac{1}{2!}[f_{xx}(x-x_0)^2 + 2f_{xy}(x-x_0)(y-y_0) + f_{yy}(y-y_0)^2]\\]Mean Value TheoremFor single variable function\\[f(x) = f(a) + f'(\\xi)(x-a)\\]For multi-variable function\\[f(x,y) = f(a,b) + (x-a)f_x(\\xi,\\eta) + (y-b)f_y(\\xi,\\eta)\\]Implicit FunctionFor explicit function \\(y=y(x)\\), its implicit function counterpart is \\(f(x,y)=0\\).Theorem: For implicit function \\(f(x,y)=0\\), if it has explicit function \\(y=y(x)\\), then the corresponding \\(y'=-f_x/f_y\\).Theorem (Implicit Function Theorem): Let \\(f(x,y)=0\\) be satisfied by a pair of real numbers \\(x_0, y_0\\) so that \\(f(x_0,y_0)=0\\), and suppose that \\(f(x,y)\\) is \\(C^1\\) in some neighborhood of \\((x_0,y_0)\\) with\\[\\dfrac{\\partial f}{\\partial y}(x_0,y_0) \\ne 0\\]Then \\(f(x,y)=0\\) uniquely implies a function \\(y(x)\\) in some neighborhood \\(N\\) of \\(x_0\\) such that \\(y(x_0)=y_0\\), where \\(y(x)\\) is differentiable in \\(N\\).Define: Jacobian\\[\\dfrac{\\partial(f,g)}{\\partial(u,v)}=\\begin{vmatrix}f_u &amp; f_v\\\\g_u &amp; g_v\\end{vmatrix}\\]Theorem: For implicit function\\[\\begin{cases}f(x, y; u, v)=0\\\\g(x, y; u, v)=0\\end{cases}\\]the correspond explicit function\\[\\begin{cases}u=u(x,y)\\\\v=v(x,y)\\end{cases}\\]the derivatives \\(u_x,u_y,v_x,v_y\\) can be found by\\[\\begin{bmatrix}f_u &amp; f_v\\\\g_u &amp; g_v\\end{bmatrix}\\begin{bmatrix}u_x\\\\v_x\\end{bmatrix}=\\begin{bmatrix}-f_x\\\\-g_x\\end{bmatrix}\\]\\[\\begin{bmatrix}f_u &amp; f_v\\\\g_u &amp; g_v\\end{bmatrix}\\begin{bmatrix}u_y\\\\v_y\\end{bmatrix}=\\begin{bmatrix}-f_y\\\\-g_y\\end{bmatrix}\\]\\[u_x = -\\dfrac{\\dfrac{\\partial(f,g)}{\\partial(x,v)}}{\\dfrac{\\partial(f,g)}{\\partial(u,v)}}, \\quadu_y = -\\dfrac{\\dfrac{\\partial(f,g)}{\\partial(y,v)}}{\\dfrac{\\partial(f,g)}{\\partial(u,v)}}, \\quadv_x = -\\dfrac{\\dfrac{\\partial(f,g)}{\\partial(u,x)}}{\\dfrac{\\partial(f,g)}{\\partial(u,v)}}, \\quadv_y = -\\dfrac{\\dfrac{\\partial(f,g)}{\\partial(u,y)}}{\\dfrac{\\partial(f,g)}{\\partial(u,v)}}\\]For general implicit functions\\[\\begin{align}g_1(x_1,\\dots,x_m;y_1,\\dots,y_n)=0\\\\\\vdots\\\\g_n(x_1,\\dots,x_m;y_1,\\dots,y_n)=0\\end{align}\\]the derivatives can be calculated as\\[\\dfrac{\\partial y_j}{\\partial x_k} = -\\dfrac{J(y_1,\\dots,y_{j-1},x_k,y_{j+1},\\dots,y_n)}{J(y_1,\\dots,y_n)}\\]Jacobian MatrixTheorem: For functions\\[\\begin{align}u_1&amp;=u_1(x_1,\\dots,x_n)\\\\&amp;\\dots\\\\u_n&amp;=u_n(x_1,\\dots,x_n)\\end{align}\\]and its correspond implicit function\\[\\begin{align}x_1&amp;=x_1(u_1,\\dots,u_n)\\\\&amp;\\dots\\\\x_n&amp;=x_n(u_1,\\dots,u_n)\\end{align}\\]then the multiplication of two Jacobian matrix\\[\\dfrac{\\partial(u_1,\\dots,u_n)}{\\partial(x_1,\\dots,x_n)} \\dfrac{\\partial(x_1,\\dots,x_n)}{\\partial(u_1,\\dots,u_n)} = I\\]then their determinant has multiplication \\(1\\).PDE Example\\[\\begin{align}x &amp;= r\\cos\\theta\\\\y &amp;= r\\sin\\theta\\end{align}\\]\\[\\dfrac{\\partial r}{\\partial x} = \\cos\\theta, \\quad \\dfrac{\\partial r}{\\partial y} = \\sin\\theta\\]\\[\\dfrac{\\partial \\theta}{\\partial x} = -\\dfrac{\\sin\\theta}{r}, \\quad \\dfrac{\\partial \\theta}{\\partial y} = \\dfrac{\\cos\\theta}{r}\\]\\[T_{xx} + T_{yy} = 0\\]in polar coordinates\\[T(x(r,\\theta),y(r,\\theta)) = U(r,\\theta)\\]\\[? \\quad U_{rr} + \\dfrac{1}{r}U_r + \\dfrac{1}{r^2} U_{\\theta \\theta} = 0\\]in spherical coordinatesMaximum And MinimumFrom the Taylor’s formula, if all the first order partial derivatives are zero, and the second derivatives matrix is positive (or negative) definite, then the function is at maximum (or minimum).\\[f(x_1,y_1)\\]the necessary condition for it is at maximum or minimum\\[f_{x_1} = f_{x_2} = 0\\]if its second derivatives matrix is positive definite, it is at minimum point\\[\\begin{pmatrix}f_{x_1 x_1} &amp; f_{x_1 x_2}\\\\f_{x_2 x_1} &amp; f_{x_2 x_2}\\end{pmatrix}\\]Constrained ExtremaWe want to find extrema for\\[f(x_1,x_2,\\dots,x_n)\\]under the conditions\\[\\begin{align}g_1(x_1,x_2,\\dots,x_n) = 0\\\\\\vdots\\\\g_k(x_1,x_2,\\dots,x_n) = 0\\end{align}\\]Then we need to find the extrema for function\\[F(x_1,\\dots,x_{n-k}) = f\\left(x_1,\\dots,x_{n-k},x_{n-k+1}\\left(x_1,\\dots,x_{n-k}\\right),\\dots,x_n\\left(x_1,\\dots,x_{n-k}\\right)\\right)\\]if the functions \\(x_{n-k+1}(), \\dots\\) are easy to calculate, this method is easy and straightforward.Lagrange Method\\[f^{*} = f + \\lambda_1 g_1 + \\dots + \\lambda_k g_k\\]The necessary condition for extrema is\\[\\begin{align}f_{x_1} + \\lambda_1 g_{1_{x_1}} + \\dots + \\lambda_k g_{k_{x_1}} = 0\\\\\\vdots\\\\f_{x_n} + \\lambda_1 g_{1_{x_n}} + \\dots + \\lambda_k g_{k_{x_n}} = 0\\end{align}\\]then we can solve\\[\\begin{align}x_1 = x_1(\\lambda_1,\\dots,\\lambda_k)\\\\\\vdots\\\\x_n = x_n(\\lambda_1,\\dots,\\lambda_k)\\end{align}\\]then using the equations \\(g_1,\\dots,g_k\\), we can get the values of \\(\\lambda_1,\\dots,\\lambda_k\\).Famous examples including: Fermat’s principle of least time.Leibniz Rule\\[I(t) = \\int_{a(t)}^{b(t)} f(x,t)dx\\]\\[I'(t) = \\int_{a}^{b}\\dfrac{\\partial f}{\\partial t}dx + b' f(b,t) - a'f(a,t)\\]3D VectorsDot Product\\[\\vec{u}\\cdot\\vec{v} = \\left\\vert\\vec{u}\\right\\vert \\left\\vert\\vec{u}\\right\\vert \\cos\\theta, \\quad 0 \\le \\theta \\le \\pi\\]It is projection.\\[\\vec{u}\\cdot\\vec{v} = \\vec{v}\\cdot\\vec{u}\\]Cross Product\\[\\vec{u}\\times\\vec{v} = \\left\\vert\\vec{u}\\right\\vert \\left\\vert\\vec{v}\\right\\vert \\sin\\theta \\,\\,\\vec{e} , \\quad 0 \\le \\theta \\le \\pi, \\quad \\text{right-hand rule}\\]It is the area of parallelogram.\\[\\vec{u}\\times\\vec{v} = -\\vec{v}\\times\\vec{u}\\]Cartesian Coordinates\\[\\vec{u}=u_i\\, \\hat{i} + u_j\\,\\hat{j} + u_k\\,\\hat{k}\\]where \\(\\hat{i},\\hat{j},\\hat{k}\\) are orthonormal basis.\\[\\hat{i}\\times\\hat{j}=\\hat{k}, \\quad \\hat{j}\\times\\hat{k}=\\hat{i}, \\quad \\hat{k}\\times\\hat{i}=\\hat{j}\\]\\[\\vec{u}\\times\\vec{v}=\\begin{vmatrix}\\hat{i} &amp; \\hat{j} &amp; \\hat{k}\\\\u_i &amp; u_j &amp; u_k\\\\v_i &amp; v_j &amp; v_k\\end{vmatrix}\\]\\[\\begin{align}&amp;\\vec{u}\\times \\vec{v} = \\vert u \\vert \\vert v\\vert \\sin\\theta\\\\=&amp; \\vert u\\vert\\vert v\\vert \\sqrt{1-\\cos^2\\theta}\\\\=&amp; \\sqrt{\\vert u\\vert^2 \\vert v\\vert^2 - (\\vec{u}\\cdot\\vec{v})^2}\\end{align}\\]Multuple Products\\[\\vec{u}\\cdot\\left(\\vec{v}\\times\\vec{w}\\right)=\\vec{u}\\cdot\\vec{v}\\times\\vec{w}=\\begin{vmatrix}u_1 &amp; u_2 &amp; u_3\\\\v_1 &amp; v_2 &amp; v_3\\\\w_1 &amp; w_2 &amp; w_3\\end{vmatrix}=\\vec{u}\\times\\vec{v}\\cdot\\vec{w}\\]  \\(\\left\\vert \\vec{u}\\cdot\\vec{v}\\times\\vec{w}\\right\\vert\\) is the volume of the parallelepiped.\\[\\vec{u}\\times\\left(\\vec{v}\\times\\vec{w}\\right) = \\left(\\vec{u}\\cdot\\vec{w}\\right)\\vec{v} - \\left(\\vec{u}\\cdot\\vec{v}\\right) \\vec{w}\\]Differentiation\\[\\dfrac{d}{dt}\\left( \\vec{u} \\cdot \\vec{v} \\right) = \\dot{\\vec{u}} \\cdot \\vec{v} + \\vec{u} \\cdot \\dot{\\vec{v}}\\]Coordinates\\[\\begin{align}\\vec{R}(t) &amp;= x(t)\\hat{i} + y(t)\\hat{j} + z(t)\\hat{k}, \\quad \\text{in cartesian coordinate}\\\\&amp;= r(t)\\hat{e_r} + \\theta(t)\\hat{e_{\\theta}} + z(t)\\hat{k}, \\quad \\text{in cylindrical coordinate}\\\\&amp;= \\rho(t)\\hat{e_{\\rho}} + \\phi(t)\\hat{e_\\phi} + \\theta(t)\\hat{e_{\\theta}}, \\quad \\text{in spherical coordinate}\\end{align}\\]\\[\\vec{R}'(t) = x'(t)\\hat{i} + y'(t)\\hat{j} + z'(t)\\hat{k}\\]  \\(\\vec{R}'(t)\\) is at the tagent direction of the curve.\\[\\text{if } \\vert R(t)\\vert \\text{ is constant}, \\vec{R}(t) \\cdot \\vec{R}'(t) = 0\\]Polar Coordinates\\[\\begin{cases}x = r\\cos\\theta\\\\y = r\\sin\\theta\\end{cases}\\]\\[\\begin{cases}\\dfrac{\\partial r}{\\partial x} = -\\dfrac{J(x,\\theta)}{J(r,\\theta)} = \\cos\\theta\\\\\\dfrac{\\partial r}{\\partial y} = -\\dfrac{J(y,\\theta)}{J(r,\\theta)} = \\sin\\theta\\\\\\dfrac{\\partial \\theta}{\\partial x} = -\\dfrac{J(r,x)}{J(r,\\theta)} = -\\dfrac{\\sin\\theta}{r}\\\\\\dfrac{\\partial \\theta}{\\partial y} = -\\dfrac{J(r,y)}{J(r,\\theta)} = \\dfrac{\\cos\\theta}{r}\\end{cases}\\]\\[\\begin{cases}\\hat{e_r} = \\cos\\theta \\hat{i} + \\sin\\theta \\hat{j}\\\\\\hat{e_\\theta} = -\\sin\\theta \\hat{i} + \\cos\\theta\\hat{j}\\end{cases}\\]\\[\\begin{cases}\\hat{i} = \\cos\\theta \\hat{e_r} - \\sin\\theta \\hat{e_\\theta}\\\\\\hat{j} = \\sin\\theta \\hat{e_r} + \\cos\\theta\\hat{e_\\theta}\\end{cases}\\]\\[\\begin{cases}\\dfrac{\\partial \\hat{e_r}}{\\partial r} = 0, \\quad \\dfrac{\\partial \\hat{e_r}}{\\partial \\theta} = \\hat{e_\\theta}\\\\\\dfrac{\\partial \\hat{e_\\theta}}{\\partial r} = 0, \\quad \\dfrac{\\partial \\hat{e_\\theta}}{\\partial \\theta} = -\\hat{e_r}\\end{cases}\\]\\[\\dot{\\hat{e_r}} = \\dot{\\theta} \\hat{e_{\\theta}}, \\quad \\dot{\\hat{e_\\theta}} = - \\dot{\\theta} \\hat{e_r}\\]\\[\\begin{cases}\\vec{R}(t) = r \\hat{e_r}\\\\\\vec{R}'(t) = \\dot{r} \\hat{e_r} + r \\dot{\\theta} \\hat{e_\\theta}\\\\\\vec{R}''(t) = (\\ddot{r} - r\\dot{\\theta}^2) \\hat{e_r} + (2 \\dot{r}\\dot{\\theta} + r\\ddot{\\theta}) \\hat{e_\\theta}\\end{cases}\\]Cylindrical Coordinates\\[\\hat{e_r} \\times \\hat{e_\\theta} = \\hat{e_z}\\]\\[\\vec{R}(t) = r \\hat{e_r} + z \\hat{e_z}\\]\\[\\vec{R}'(t) = \\dot{r} \\hat{e_r} + r \\dot{\\theta} \\hat{e_\\theta} + \\dot{z} \\hat{e_z}\\]\\[\\vec{R}''(t) = (\\ddot{r} - r\\dot{\\theta}^2) \\hat{e_r} + (2 \\dot{r}\\dot{\\theta} + r\\ddot{\\theta}) \\hat{e_\\theta} + \\ddot{z} \\hat{e_z}\\]Spherical Coordinates\\[\\hat{e_\\rho} \\times \\hat{e_\\phi} = \\hat{e_\\theta}\\]\\[\\begin{cases}x = \\rho\\sin\\phi\\cos\\theta\\\\y = \\rho\\sin\\phi\\sin\\theta\\\\z = \\rho\\cos\\phi\\end{cases}\\]\\[\\hat{e_\\rho} = \\hat{e_\\rho}(\\phi,\\theta), \\quad \\hat{e_\\phi} = \\hat{e_\\phi}(\\phi,\\theta), \\quad \\hat{e_\\theta} = \\hat{e_\\theta}(\\theta)\\]\\[\\partial \\hat{e_\\rho} / \\partial \\rho = 0, \\quad \\partial \\hat{e_\\rho} / \\partial \\phi = \\hat{e_{\\phi}}\\]\\[...\\]\\[...\\]\\[R(t) = \\rho(t) \\hat{e_{\\rho}}\\]\\[v(t) = \\dot{\\rho} \\hat{e_{\\rho}} + \\rho\\dot{\\theta}\\sin\\theta \\hat{e_{\\theta}} + \\rho\\dot{\\phi}\\hat{e_{\\phi}}\\]Omega Method (4B,TBD)Curves, Surfaces, and VolumesLine IntegralArc LengthA curve can be described by position vector\\[\\vec{R}(\\tau) = x(\\tau)\\hat{i} + y(\\tau)\\hat{j} + z(\\tau)\\hat{k}\\]Terms Closed curve. Simple curve. Continuous. Smooth.Length of a curve\\[S = \\int ds = \\int_a^b \\sqrt{\\vec{R'}\\cdot \\vec{R'}} d\\tau\\]Line Integral\\[\\int_c f(x,y,z) ds = \\int_c f(x(\\tau),y(\\tau),z(\\tau)) \\sqrt{\\vec{R'}\\cdot \\vec{R'}} d\\tau\\]\\[\\int_c (\\alpha f + \\beta g) ds = \\alpha\\int_c f ds + \\beta\\int_c g ds\\]\\[\\oint\\]Surface IntegralDouble IntegralIf we are doing double integral in \\(x,y\\) axis\\[\\int\\int f(x,y) dA\\]\\[\\int_{0}^{2} \\int_{y/2}^1 xy^2 dxdy = \\dfrac{8}{15}\\]Surface Integral\\[\\vec{R}(u,v) = x(u,v)\\hat{i} + y(u,v)\\hat{j} + z(u,v)\\hat{k}\\]\\[dA = (R_u \\times R_v)du dv\\]\\[R_u = x_u \\hat{i} + y_u \\hat{j} + z_u \\hat{k}\\]\\[R_v = x_v \\hat{i} + y_v \\hat{j} + z_v \\hat{k}\\]if \\(z=\\mathrm{const}\\)\\[dA = \\left\\vert \\dfrac{\\partial(x,y)}{\\partial(u,v)} \\right\\vert du dv\\]Volumn Integral\\[\\begin{align}dV &amp;= \\left\\vert \\vec{R_u} \\times \\vec{R_v} \\cdot \\vec{R_\\omega} \\right\\vert du dv d\\omega\\\\&amp;= \\left\\vert \\dfrac{\\partial(x,y,z)}{\\partial(u,v,\\omega)} \\right\\vert du dv d\\omega\\end{align}\\]Field TheoryDivergence\\[\\nabla \\cdot \\vec{V} = \\left( \\dfrac{\\partial V_x}{\\partial x} + \\dfrac{\\partial V_y}{\\partial y} + \\dfrac{\\partial V_z}{\\partial z} \\right)\\]Gradient\\[\\nabla u = \\dfrac{\\partial u}{\\partial x} \\hat{i} + \\dfrac{\\partial u}{\\partial y} \\hat{j} + \\dfrac{\\partial u}{\\partial z} \\hat{k}\\]Curl\\[\\nabla \\times \\vec{V} =\\begin{vmatrix}\\hat{i} &amp; \\hat{j} &amp; \\hat{k}\\\\\\dfrac{\\partial}{\\partial x} &amp; \\dfrac{\\partial}{\\partial y} &amp; \\dfrac{\\partial}{\\partial z}\\\\V_x &amp; V_y &amp; V_z\\end{vmatrix}\\]Combination and LaplacianIf \\(\\alpha,\\beta\\) are scalars, then\\[\\nabla \\cdot (\\alpha \\vec{u} + \\beta \\vec{v}) = \\alpha \\nabla \\cdot \\vec{u} + \\beta \\nabla \\cdot \\vec{v}\\]\\[\\nabla (\\alpha u + \\beta v) = \\alpha \\nabla u + \\beta \\nabla v\\]\\[\\nabla \\times (\\alpha \\vec{u} + \\beta\\vec{v}) = \\alpha \\nabla \\times \\vec{u} + \\beta \\nabla \\times \\vec{v}\\]Furthermore\\[\\nabla \\cdot (u\\vec{v}) = \\nabla u \\cdot \\vec{v} + u \\nabla \\cdot \\vec{v}\\]\\[\\nabla \\times (u\\vec{v}) = \\nabla u \\times \\vec{v} + u \\nabla \\times \\vec{v}\\]\\[\\nabla \\cdot (\\vec{u}\\times \\vec{v}) = \\vec{v} \\cdot \\nabla \\times \\vec{u} - \\vec{u}\\cdot \\nabla\\times\\vec{v}\\]\\[\\nabla \\times (\\vec{u}\\times \\vec{v}) = \\vec{u} \\nabla \\cdot \\vec{v} - \\vec{v} \\nabla \\cdot \\vec{u} + (\\vec{v} \\cdot \\nabla) \\vec{u} - (\\vec{u} \\cdot \\nabla) \\vec{v}\\]\\[\\nabla (\\vec{u}\\cdot\\vec{v}) = (\\vec{u}\\cdot \\nabla)\\vec{v} + (\\vec{v}\\cdot \\nabla)\\vec{u} + \\vec{u}\\times (\\nabla \\times \\vec{v}) + \\vec{v} \\times (\\nabla \\times \\vec{u})\\]Combination of Operators\\[\\nabla \\cdot \\nabla = \\nabla^2 = \\left( \\dfrac{\\partial^2}{\\partial x^2} + \\dfrac{\\partial^2}{\\partial y^2} + \\dfrac{\\partial^2}{\\partial z^2} \\right) \\quad \\text{(it can be applied to either scalar or vector field)}\\]\\[\\nabla \\cdot \\nabla \\times \\vec{v} = 0\\]\\[\\nabla \\times \\nabla u = 0\\]\\[\\nabla \\times \\nabla \\times \\vec{v} = \\nabla(\\nabla \\cdot \\vec{v}) - \\nabla^2 \\vec{v}\\]Non-Cartesian CoordinatesCylindrical Coordinates\\[\\nabla = \\hat{e_r} \\dfrac{\\partial}{\\partial r} + \\hat{e_\\theta} \\dfrac{1}{r} \\dfrac{\\partial}{\\partial\\theta} + \\hat{e_z}\\dfrac{\\partial}{\\partial z}\\]\\[\\nabla u = \\dfrac{\\partial u}{\\partial r} \\hat{e_r} + \\dfrac{1}{r}\\dfrac{\\partial u}{\\partial \\theta} \\hat{e_\\theta} + \\dfrac{\\partial u}{\\partial z} \\hat{k}\\]\\[\\nabla \\cdot \\vec{v} = \\left(\\hat{e_r}\\dfrac{\\partial}{\\partial r} + \\hat{e_\\theta} \\dfrac{1}{r}\\dfrac{\\partial}{\\partial \\theta} + \\hat{e_z} \\dfrac{\\partial}{\\partial z}\\right) \\cdot \\left(v_r \\hat{e_r} + v_\\theta \\hat{e_\\theta} + v_z \\hat{e_z}\\right)\\]Carrying out the dot product on the right-hand side yields nine terms: the first term in \\(\\nabla\\) dotted with the first term in \\(\\vec{v}\\), the first with the second, the first with the third, the second with the first, and so on.As representative, consider the first term dotted with the first,\\[\\left(\\hat{e_r}\\dfrac{\\partial}{\\partial r}\\right) \\cdot \\left(v_r \\hat{e_r}\\right)\\]Notice carefully that the latter is ambiguous because there are two operations to carry out, a dot product and a derivative, and we are not told which to do first and which to do second (in the equation above we actually get the same result, but this is not generally true).We state, without proof, that the correct answer is always obtained if we do the differentation first and then the dot product (or cross product if we are computing the curl rather than the divergence).\\[\\left(\\hat{e_r}\\dfrac{\\partial}{\\partial r}\\right) \\cdot \\left(v_r \\hat{e_r}\\right) = \\hat{e_r} \\cdot \\dfrac{\\partial}{\\partial r} \\left(v_r \\hat{e_r}\\right) = \\hat{e_r} \\cdot \\left(\\dfrac{\\partial v_r}{\\partial r} \\hat{e_r} + v_r \\dfrac{\\partial \\hat{e_r}}{\\partial r}\\right)\\]In the lecture he states that, as long as we choose the right-handed orthogonal basis, we can use the determind to calculate the curl.Is it correct? No, it is wrong, as explained in the book page 789.Spherical CoordinatesDivergence Theorem\\[\\int_\\nu \\nabla \\cdot \\vec{v} \\, dV = \\int_S \\hat{n} \\cdot \\vec{v} \\, dA\\]where \\(\\vec{n}\\) is the vector pointing outside of the surface.2D case of divergence theorem (11B).\\[\\int_S \\nabla \\cdot \\vec{v} \\, dA = \\oint_c \\hat{n} \\cdot \\vec{v} \\, ds\\]He states that\\[\\int_\\nu \\nabla \\times \\vec{v} \\, dV = \\int_s \\hat{n} \\times \\vec{v} \\, dA\\]\\[\\int_\\nu \\nabla u \\, dV = \\int_s u \\, dA\\]Is these correct?Stokes’ Theorem\\[\\int_s \\hat{n} \\cdot \\left(\\nabla \\times \\vec{v}\\right) \\, dA = \\oint_c \\vec{v} \\cdot \\, d \\vec{R}\\]where \\(\\hat{n}\\) is determined by the right hand rule from closed curve \\(c\\).Irrotational FieldsThe four statements are equivalent: There exists a \\(c^2\\) (the second derivatives are continuous) scaler function in \\(D\\), such that \\(\\nabla \\phi = \\vec{v}\\).   \\(\\nabla \\times \\vec{v} = 0\\).   \\(\\oint_c \\vec{v}\\cdot \\, d\\vec{R} = 0\\).   \\(\\int_c \\vec{V} \\cdot d\\vec{R}\\) is independent with \\(c\\).If we know some field \\(\\vec{v}\\) is irrotational, then we can find \\(\\phi(x,y,z)\\) by partial integrals in terms of \\(x,y\\) and \\(z\\).Fourier MethodHalf and Quarter Range ExpansionsFor a function defined from \\(0\\) to \\(L\\), you can extend it to half range sin or half range cos or quarter range sin or quarter range cos.Sturm-Liouville Problem\\[\\begin{cases}\\left[ p(x)y' \\right]' + q(x) y + \\lambda w(x) y = 0\\\\x \\in [a,b]\\\\\\alpha y(a) + \\beta y'(a) = 0\\\\\\gamma y(b) + \\delta y'(b) = 0\\end{cases}\\]where \\(p, p', q, w\\) are continuous, and \\(p(x) &gt; 0, w(x) &gt; 0\\).For any \\(\\lambda\\) provides the corresponding solution \\(\\phi\\), we call \\(\\lambda\\) eigenvalue, \\(\\phi(x)\\) eigenfxn.The eigenvalue needs to be determined.Strum-Liouville Theorem All \\(\\lambda\\)’s are real. The eigenvalues are simple. That is, to each eigenvalue there corresponds only one linearly independent eigenfunction. Further, there are an infinite number of eigenvalues, and they can be ordered so that \\(\\lambda_1 &lt; \\lambda_2 &lt; \\lambda_3 &lt; \\dots\\), where \\(\\lambda_n \\to \\infty\\) as \\(n \\to \\infty\\). If \\(\\lambda_i \\ne \\lambda_j\\), then \\(\\int_a^b \\phi_j \\phi_k w(x) \\, dx = 0\\).These three properties are exactly the same as symmetry matrix.Non-nagative \\(\\lambda\\)’sIf \\(q(x) \\le 0\\) on \\([a,b]\\), and \\(\\left[p(x)\\phi_n(x)\\phi_n'(x) \\right] \\vert_a^b \\le 0\\), then not only is \\(\\lambda_n\\) real, but also \\(\\lambda_n \\ge 0\\).Periodic and Singular SL ProblemsPeriodic boundary conditions\\[\\begin{cases}y(a) = y(b)\\\\y'(a) = y'(b)\\end{cases}\\]Singular case: In this case \\(p(x)\\) (and possibly \\(w(x)\\)) vanishes at one or both endpoints, so that \\(p(x) &gt; 0\\) and \\(w(x)&gt;0\\) holds on the open interval \\((a,b)\\) rather than on the closed interval \\([a,b]\\).Further, the boundary conditions are modified as follows,  \\(p(a) = 0\\) and \\(p(b) \\ne 0\\)\\[\\begin{cases}y \\text{ bounded at } a\\\\\\gamma y(b) + \\delta y'(b) = 0\\end{cases}\\]  \\(p(b) = 0\\) and \\(p(a) \\ne 0\\)\\[\\begin{cases}\\alpha y(a) + \\beta y'(a) = 0\\\\y \\text{ bounded at } b\\end{cases}\\]  \\(p(a)=p(b)=0\\)\\[\\begin{cases}y \\text{ bounded at } a\\\\y \\text{ bounded at } b\\end{cases}\\]By \\(y\\) being bounded at \\(a\\), for example, we mean that \\(\\lim_{x\\to a} y(x)\\) exists and is therefore finite.Fourier Integral\\[f(x) \\sim \\int_0^{\\infty} \\left[a(\\omega) \\cos\\omega x + b(\\omega) \\sin\\omega x\\right] \\, d\\omega\\]where\\[\\begin{align}a(\\omega) &amp;= \\dfrac{1}{\\pi}\\int_{-\\infty}^{\\infty}f(x)\\cos\\omega x\\, dx\\\\b(\\omega) &amp;= \\dfrac{1}{\\pi}\\int_{-\\infty}^{\\infty}f(x)\\sin\\omega x\\, dx\\end{align}\\]Fourier Transform 横看成岭侧成峰\\[F(\\omega) = \\int_{-\\infty}^{\\infty} f(x)e^{-i\\omega x} \\, dx\\]\\[f(x) = \\dfrac{1}{2\\pi}\\int_{-\\infty}^{\\infty} F(\\omega) e^{i\\omega x} \\, d\\omega\\] Linearity.   \\(F\\left\\{f^{(n)}(x)\\right\\} = (i\\omega)^n \\hat{f}(\\omega)\\). Fourier convolution: \\((f*g)(x) = \\int_{-\\infty}^{\\infty} f(x-\\xi)g(\\xi)\\,d\\xi\\).   \\(F\\{f*g\\}=\\hat{f}(\\omega)\\hat{g}(\\omega)\\).   \\(F\\left\\{f(x-a)\\right\\} = e^{-ia\\omega} \\hat{f}(\\omega)\\).   \\(F^{-1}\\{\\hat{f}(\\omega-a)\\} = e^{iax}f(x)\\).Diffusion EquationIntroduction\\[\\alpha^2 \\nabla^2 u = \\dfrac{\\partial u}{\\partial t}\\]For linear homogeneous PDE (without considering the initial or boundary condition), if \\(y_1, y_2\\) is solution, then \\(A y_1 + B y_2\\) is also solution.However, when solve PDE, we usually are not trying to find general solution.Instead, we usually just find some solution sufficiently robusts to handle the BCs and ICs.Separation of Variables\\[\\begin{cases}L[u] = a^2 u_{xx} - u_t = 0, \\quad (0 &lt; x &lt; L, 0 &lt; t &lt; \\infty)\\\\u(0,t) = u_1, \\quad u(L,t) = u_2, \\quad (0 &lt; t &lt; \\infty)\\\\u(x,0) = f(x), \\quad (0 &lt; x &lt; L)\\end{cases}\\]Fourier Transform\\[\\begin{cases}\\alpha^2 u_{xx} = u_{t}\\\\-\\infty &lt; x &lt; \\infty, 0 &lt; t &lt; \\infty\\\\u(\\pm \\infty, t) = u_{x}(\\pm \\infty, t) = 0\\\\u(x,0) = f(x)\\end{cases}\\]\\[u(x,t) = \\int_{-\\infty}^{\\infty} f(\\xi) K(x-\\xi,t)\\, d\\xi\\]\\[K(\\xi-x;t) = \\dfrac{e^{-(x-\\xi)^2/(4\\alpha^2 t)}}{2\\alpha \\sqrt{\\pi t}}\\]We can do a sanity check. \\(K(\\xi-x,0) = \\delta(\\xi-x)\\), thus natually the integration when \\(t=0\\) gives \\(f(x)\\).If \\(f(x) = \\delta(x)\\), \\(u(x,t) = K(x,t)\\).Laplace Transform\\[\\begin{cases}\\alpha^2 u_{xx} = u_{t}\\\\0 \\le x &lt; \\infty, \\quad 0 \\le t &lt; \\infty\\\\u(0,t) = g(t), \\quad 0 \\le t &lt; \\infty\\\\u(\\infty, t) = 0, \\quad 0 \\le t &lt; \\infty\\\\u(x,0) = 0, \\quad 0 \\le x &lt; \\infty\\end{cases}\\]Numerical SolutionFinite difference time domain.Wave Equation\\[c^2 \\nabla^2 u = u_{tt}\\]Separation of Variable\\[\\begin{cases}c^2 y_{xx} = y_{tt}\\\\0 \\le x \\le L, \\quad 0 \\le t &lt; \\infty\\\\y(0,t) = 0\\\\y(L,t) = 0\\\\y(x,0) = f(x)\\\\y_{t}(x,0) = g(x)\\end{cases}\\]Two Dimentional Case\\[\\begin{cases}c^2 (w_{xx} + w_{yy}) = w_{tt}\\\\0 \\le x \\le a\\\\0 \\le y \\le b\\\\0 \\le t &lt; \\infty\\\\w(0,y,t) = w(a,y,t) = w(x,0,t) = w(x,b,t) = 0\\\\w(x,y,0) = f(x,y)\\\\w_t(x,y,0) = 0\\end{cases}\\]d’Alembert’s Solution\\[c^2 y_{xx} = y_{tt}\\]Let\\[x-ct = \\xi, \\quad x + ct = \\eta\\]\\[\\dfrac{\\partial}{\\partial x} = \\dfrac{\\partial}{\\partial \\xi} \\dfrac{\\partial \\xi}{\\partial x} + \\dfrac{\\partial}{\\partial \\eta} \\dfrac{\\partial \\eta}{\\partial x} = \\dfrac{\\partial}{\\partial \\xi} + \\dfrac{\\partial}{\\partial \\eta}\\]\\[\\dfrac{\\partial}{\\partial t} = -c \\dfrac{\\partial}{\\partial \\xi} + c \\dfrac{\\partial}{\\partial \\eta}\\]\\[c^2 \\left( \\dfrac{\\partial}{\\partial \\xi} + \\dfrac{\\partial}{\\partial \\eta}\\right) \\left( \\dfrac{\\partial}{\\partial \\xi} + \\dfrac{\\partial}{\\partial \\eta}\\right) y = \\left( -c \\dfrac{\\partial}{\\partial \\xi} + c \\dfrac{\\partial}{\\partial \\eta}\\right) \\left( -c \\dfrac{\\partial}{\\partial \\xi} + c \\dfrac{\\partial}{\\partial \\eta}\\right) y\\]\\[y_{\\xi \\eta} + y_{\\eta \\xi} = 0\\]assume \\(y_{\\xi \\eta} = y_{\\eta\\xi}\\)\\[y_{\\xi \\eta} = y_{\\eta \\xi} = 0\\]\\[y_{\\xi} = A(\\xi)\\]\\[\\begin{align}y &amp;= F(\\xi) + G(\\eta)\\\\&amp;= F(x-ct) + G(x+ct)\\end{align}\\]once the initial condition is given\\[\\begin{cases}-\\infty &lt; x &lt; \\infty\\\\y(x,0) = f(x)\\\\y_t(x,0) = g(x)\\end{cases}\\]\\[y(x,t) = \\dfrac{f(x-ct) + f(x+ct)}{2} + \\dfrac{1}{2c}\\int_{x-ct}^{x+ct} g(\\xi)\\, d\\xi\\]Now 21A" }, { "title": "Advanced Differential Equations 003", "url": "/posts/advanced-differential-equations-003/", "categories": "math", "tags": "math", "date": "2023-07-23 06:00:00 +0000", "snippet": "From Prof. J. Nathan Kutz’ Lecture 003Linear Operators And Thier AdjointsFredholm Alternative TheoremGiven a matrix \\(\\mathbf{A} \\in \\mathbb{C}^{m\\times n}\\), then the vector \\(\\mathbf{Ax}\\), where \\(x \\in \\mathbb{C}^n\\) must be orthogonal to the null space of \\(\\mathbf{A}^{\\mathsf{H}}\\). First recall the definition of inner product is\\[\\braket{\\mathbf{a},\\mathbf{b}} = \\mathbf{b}^{\\mathsf{H}} \\mathbf{a}\\]thus\\[\\begin{align}\\braket{\\mathbf{Ax}, \\mathbf{y}} &amp;= \\braket{\\mathbf{x},\\mathbf{A}^{\\mathsf{H}}\\mathbf{y}}\\\\\\braket{\\mathbf{x},\\mathbf{Ay}} &amp;= \\braket{\\mathbf{A}^{\\mathsf{H}}\\mathbf{x}, \\mathbf{y}}\\end{align}\\]Assume \\(\\mathbf{A}^{\\mathsf{H}} \\mathbf{y}=0\\), then\\[\\braket{\\mathbf{Ax},\\mathbf{y}} = \\braket{\\mathbf{x},\\mathbf{A}^{\\mathsf{H}}\\mathbf{y}} = 0\\]Linear Operators\\[Lu=f\\]where \\(L\\) will be a linear, differential operator on the domain \\(x\\in [0,l]\\) and with boundary conditions specified at \\(x=0\\) and \\(x=l\\) which will be specified shortly.The definition of inner product is given by\\[\\braket{u,v} = \\int_{0}^{l} uv^{*}dx\\]we want to find the adjoint operator such that\\[\\braket{v,Lu}=\\braket{L^{\\dagger}v,u}\\]For example\\[L = a(x)\\dfrac{d^2}{dx^2} + b(x)\\dfrac{d}{dx} + c(x)\\]on the domain \\(x\\in[a,b]\\) with boundary conditions\\[\\begin{align}\\alpha_1 u(a) + \\beta_1 \\dfrac{d u(a)}{dx} &amp;= 0\\\\\\alpha_2 u(b) + \\beta_2 \\dfrac{d u(b)}{dx} &amp;= 0\\end{align}\\]assume we are working on real functions \\(u,v\\) we can calculate\\[\\begin{align}\\braket{v,Lu} &amp;=\\int_{a}^b v \\bigg( a(x)\\dfrac{d^2 u}{dx^2} + b(x)\\dfrac{du}{dx} + c(x)u \\bigg)dx\\\\&amp;=\\int_a^b \\bigg( a(x)v\\dfrac{d^2 u}{dx^2} + b(x)v\\dfrac{du}{dx} + c(x)vu \\bigg)dx\\end{align}\\]using integration by part\\[\\int udv = uv - \\int vdu\\]\\[u= av, \\quad v =u_x\\]\\[du = (av)_x dx, \\quad dv = u_{xx}dx\\]\\[\\begin{align}\\int_{a}^b avu_{xx}dx &amp;= avu_x\\vert_{a}^b - \\int_a^b u_x (av)_x dx\\\\&amp;= avu_x\\vert_{a}^b - (av)_x u\\vert_{a}^b + \\int_a^b u(av)_{xx} dx\\end{align}\\]\\[\\int_a^b bvu_x dx = buv\\vert_a^b - \\int_a^b u(bv)_x dx\\]\\[\\braket{v,Lu} = (avu_x - (av)_x u + uvb)\\vert_a^b + \\int_a^b [(av)_{xx} u - (bv)_x u + cvu] dx\\]Thus the formal adjoint is given by\\[L^{\\dagger}v = \\dfrac{d^2}{dx^2}(a(x)v) - \\dfrac{d}{dx}(b(x)v) + c(x)v\\]also the boundary condition for operator \\(L^{\\dagger}\\) can be given by\\[(avu_x - (av)_x u + uvb)\\vert_a^b = 0\\]" }, { "title": "Advanced Differential Equations 002", "url": "/posts/advanced-differential-equations-002/", "categories": "math", "tags": "math", "date": "2023-07-23 05:00:00 +0000", "snippet": "From Prof. J. Nathan Kutz’ Lecture 002" }, { "title": "Advanced Differential Equations 001", "url": "/posts/advanced-differential-equations-001/", "categories": "math", "tags": "math", "date": "2023-07-22 05:00:00 +0000", "snippet": "From Prof. J. Nathan Kutz’ Lecture 001Phase-Plane Analysis For Nonlinear DynamicsWe begin by reviewing linear system\\[\\mathbf{x}'=\\mathbf{Ax}\\]where\\[\\mathbf{x} \\in \\mathbb{R}^{2}, \\mathbf{A} \\in \\mathbb{R}^{2 \\times 2}\\]The equilibrium points of this system are determined by setting \\(\\mathbb{x}'=0\\).\\[\\mathbf{x}'=\\mathbf{Ax}=0 \\implies \\mathbf{x}=0\\]Here we assume \\(A\\) is not singular.The differential equation can be solved by\\[\\mathbf{x}=\\mathbf{v}e^{\\lambda t} \\implies (\\mathbf{A}-\\lambda \\mathbf{I})\\mathbf{v}=0\\]There are five cases Cases 1: eigenvalues are real, unequal, same sign\\[\\mathbf{x} = c_1 \\mathbf{v}^{(1)}e^{\\lambda_1 t} + c_2 \\mathbf{v}^{(2)} e^{\\lambda_2 t}\\] Cases 2: eigenvalues are real, opposite sign\\[\\mathbf{x} = c_1 \\mathbf{v}^{(1)}e^{\\lambda_1 t} + c_2 \\mathbf{v}^{(2)} e^{-\\lambda_2 t}\\] Cases 3: eigenvalues are real and equalFor the case of a double root, two possibilities exist: either we can find two linearly independent eigenvectors so that our solution is\\[\\mathbf{x} = c_1 \\mathbf{v}^{(1)}e^{\\lambda_1 t} + c_2 \\mathbf{v}^{(2)} e^{\\lambda_1 t}\\]or, there is only one eigenvector, and we must generate a generalized eigenvector via the methods\\[\\mathbf{x} = c_1 \\mathbf{v}^{(1)}e^{\\lambda_1 t} + c_2 [\\mathbf{v}^{(1)}t e^{\\lambda_1 t}+\\boldsymbol{\\eta}e^{\\lambda_1 t}]\\] Cases 4: eigenvalues are complex eigenvalues\\[\\lambda_{\\pm} = \\beta \\pm i\\mu\\] Cases 5: eigenvalues are purely imaginary\\[\\lambda_{\\pm} = \\pm i\\mu\\]" }, { "title": "Computer Aided Circuit Analysis 002", "url": "/posts/computer-aided-circuit-analysis-002/", "categories": "analog-circuit", "tags": "analog", "date": "2023-07-14 05:00:00 +0000", "snippet": "In this post, the detail of linear DC nodal analysis is discussed.The material is from Book: Electronic Circuit and System Simulation Methods.Most of the content are from chapter 2 in the book.Linear DC Nodal AnalysisFor the linear DC nodal analysis perspective, the circuits can have different components: Resistor Independent voltage source Independent current source Voltage-controlled current source Current-controlled current source Voltage-controlled voltage source Current-controlled current sourceWe have seen that nodal analysis can easily deal with resistor and independent current source.Now we will see how to deal with the others.We start from the nodal equations with only resistors and independent current sources.If there are \\(N+1\\) nodes, we can write down \\(N+1\\) equations\\[YV=I\\]The equations will be indefinite. We can make it definite by picking any of the node as ground reference, then the corresponding row and column can be removed in \\(Y\\), such that the equations are definite.However, in this post, we will keep the original \\(N+1\\) equtions in the form, for the symmetry of the equations.Matrix StampsResistorFor a resistor connecting from node \\(i\\) to node \\(j\\), it will give the matrix stamps in \\(Y\\) matrix.Independent Current SourceFor a independent current source connecting from node \\(i\\) to node \\(j\\), it will give the matrix stamps in current vectorIndependent Voltage SourcesVoltage-Controlled Current SourceVoltage-controlled current source is a four-terminal element.It will give matrix stamp in \\(Y\\) matrix.Current-Controlled Current SourceVoltage-Controlled Voltage SourceCurrent-Controlled Voltage SourceWhen Do Nodal Equations Fail?DC Solution of Circuits with Energy Storage" }, { "title": "Computer Aided Circuit Analysis 001", "url": "/posts/computer-aided-circuit-analysis-001/", "categories": "analog-circuit", "tags": "analog", "date": "2023-07-11 05:00:00 +0000", "snippet": "In this post, the principle of DC analysis, AC analysis and transient analysis are explained.We will focus on the minimum examples, such that the algorithms can be simplified as much as possible.In the later posts, the algorithms for more complicated scenarios will be introduced.References: Paper: Elements of Computer-Aided Circuit Analysis Book: Electronic Circuit and System Simulation MethodsDC AnalysisFor Linear CircuitsLinear circuits are composed of resistors, capacitors, inductors, independent and dependent sources.In the minimum example in this section, we assume the circuits are only compsed of linear resistors and independent current sources, and they are time-invariant.Nodal analysis are applied to the linear DC analysis.For a circuit of \\(N\\) nodes, the number of equations (or the dimension of the voltage vector) will be \\(N-1\\), since one of the node will be selected as the ground reference.Assume we lable the nodes from \\(0\\) to \\(N-1\\), where node \\(0\\) is the ground reference (\\(V_0 = 0\\)).For node \\(i (i \\ne 0)\\), let \\(I_i\\) be the total current flowing into the node \\(i\\) from the independent current sources.Let \\(V_i\\) be the voltage level at node \\(i\\).The equation for node \\(i\\) is\\[I_i + \\sum_{0 \\le k \\le N-1, k \\ne i} G_{ik} (V_k - V_i) = 0\\]That is\\[(\\sum_{0 \\le k \\le N-1, k\\ne i} G_{ik}) V_i - \\sum_{1 \\le k \\le N-1, k\\ne i} G_{ik} V_k = I_i\\]The \\(N-1\\) equations will be\\[\\begin{bmatrix}Y_{11} &amp; Y_{12} &amp; \\dots &amp; Y_{1(N-1)}\\\\Y_{21} &amp; Y_{22} &amp; \\dots &amp; Y_{2(N-1)}\\\\\\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\Y_{(N-1)1} &amp; Y_{(N-1)2} &amp; \\dots &amp; Y_{(N-1)(N-1)}\\end{bmatrix}\\begin{bmatrix}V_{1}\\\\V_{2}\\\\\\vdots\\\\V_{N-1}\\end{bmatrix}=\\begin{bmatrix}I_{1}\\\\I_{2}\\\\\\vdots\\\\I_{N-1}\\end{bmatrix}\\]or in short\\[YV=I\\]The \\(Y\\) matrix is called nodal admittance matrix.Let \\(G_{ik}\\) be the conductance of the resistor connected between node \\(i\\) and \\(k\\) (\\(i\\ne k\\)), then the \\(Y\\) matrix can be calculated as\\[Y_{ij}=\\begin{cases}\\sum\\limits_{0\\le k \\le N-1, k\\ne i} G_{ik}, \\quad &amp;\\text{ if } i = j\\\\-G_{ij}, \\quad &amp;\\text{ if } i \\ne j\\end{cases}\\]Each \\(V_i\\) is the voltage level (to be determined) at the node \\(i\\). \\(I_i\\) is the total current flowing into the node \\(i\\) from the independent current sources.The circuit is solved by\\[V = Y^{-1}I\\]Example:\\[\\begin{bmatrix}G_{01} + G_{12} + G_{13} &amp; -G_{12} &amp; -G_{13}\\\\-G_{12} &amp; G_{12}+G_{23} &amp; -G_{23}\\\\-G_{13} &amp; -G_{23} &amp; G_{13} + G_{23}\\end{bmatrix}\\begin{bmatrix}V_1\\\\V_2\\\\V_3\\end{bmatrix}=\\begin{bmatrix}-I_{12}\\\\I_{12}\\\\I_{03}\\end{bmatrix}\\]For Nonlinear CircuitsTake the following example.The diode can be viewed as a nonliear voltage controlled resistor.\\[\\begin{align}I_D &amp;\\approx I_s(e^{V_{k}/V_T} - 1) + (V-V_{k}) \\dfrac{\\partial I_{D}}{\\partial V} \\bigg\\vert_{V=V_k}\\\\&amp;= I_{k} + V \\cdot g_{k}\\end{align}\\]where\\[\\begin{align}I_{k} &amp;= I_s(e^{V_k/V_T}-1) - V_k g_k\\\\g_k &amp;= \\dfrac{\\partial I_{D}}{\\partial V} \\bigg\\vert_{V=V_k} = \\dfrac{I_s}{V_T}e^{V_k/V_T}\\end{align}\\]Then by nodal analysis\\[(G+g_k) V = I - I_k\\]This is an iterative procedure\\[\\begin{align}V_{k+1} &amp;= \\dfrac{I-I_k}{G+g_k}\\\\&amp;= \\dfrac{I-I_s(e^{V_k/V_T}-1) - V_k \\dfrac{I_s}{V_T} e^{V_k/V_T}}{G + \\dfrac{I_s}{V_T} e^{V_k/V_T}}\\end{align}\\]Given some appropriate initial value, it should converge to the solution.This method is called Newton-Raphson iteration.If there are multiple nonlinear voltage controlled resistors, each of them is linearized at the initial (guessed) voltage value across the nonlinear resistor.The nonlinear voltage controlled resistors are converted into the linearized current source and linearized conductance.\\[\\begin{align}I_{linear} &amp;= I(V_{init}) - V_{init} \\cdot g_{linear}\\\\g_{linear} &amp;= \\dfrac{\\partial I}{\\partial V} \\bigg\\vert_{V=V_{init}}\\end{align}\\]Then nodal analysis is applied to find the next voltage values.Iteratively, the solution will converge to the DC operating point.AC AnalysisFor Linear CircuitNow assume the circuits are composed of linear time-invariant resistors, capacitors and some special time-varying independent current sources.The “special” time-varying will be explained later.When doing DC analysis, the capacitors are open circuits (\\(G=0\\)).In AC analysis, the effects of capacitors will be taken into account.\\[i(t) = C \\dfrac{dv}{dt}\\]Then the capacitor can be viewed as the conductance of \\(C\\frac{d}{dt}\\).Then the nodal equations can be expressed as\\[Y \\cdot V(t) = I(t)\\]note that the dimension will be the same in the DC analysis.Now \\(Y\\) is the complete nodal admittance matrix, with the components of \\(G\\) and \\(C\\frac{d}{dt}\\).In principle, the equations can be solved as long as we specify the initial condition of the capacitor voltages.However, solving such equations is much more difficult than in the DC analysis, since it is a differential equation.AC analysis assumes the independent current sources are some “special” time-varying current sources.In AC analysis, each independent sources are assumed to be, e.g., \\(I_{i}(t) = I_{i,dc} + A_{i}\\cos(\\omega t)\\).Then, the current vector can be expressed as\\[I(t) = I_{dc} + I_{ac}\\cos(\\omega t)\\]Where \\(I_{dc}\\) includes the DC parts of each current sources.And \\(I_{ac}\\) is the current vector for all the currents with \\(I_i = A_i\\).Thus\\[Y \\cdot V(t) = I_{dc} + I_{ac}\\cos(\\omega t)\\]Similarly, \\(V(t)\\) can be represented as \\(V(t) = V_{dc} + V_{ac}(t)\\), where\\[Y \\cdot V_{dc} = I_{dc}\\]then\\[Y \\cdot V_{ac}(t) = I_{ac} \\cos(\\omega t)\\]It can be easily solved by replace \\(C\\dfrac{d}{dt}\\) as \\(j\\omega C\\) in the \\(Y\\) matrix\\[V_{ac}(t) = \\mathrm{Re}[Y^{-1}\\cdot I_{ac}\\cdot e^{j\\omega t}]\\]In the phasor representation, it is enough to write\\[V_{ac} = Y^{-1} \\cdot I_{ac}\\]For Nonlinear CircuitFor nonlinear circuit, we assume all the \\(A_i\\) are extremely small, such that \\(V(t)\\) are extremely close to \\(V_{dc}\\).Then the linearization at \\(V_{dc}\\) is (almost) equivalent to the original circuit, thus AC analysis can be done in the same way as above.Note that now \\(Y\\) will includes the linearized conductance, but \\(I_{ac}\\) only includes the independent current sources.The linearized current sources will not be inlcuded in \\(I_{ac}\\).Transient AnalysisFor Linear CircuitTransient analysis computes the waveform in time domain.Suppose that we know the solution of the circuit at time \\(t\\), and we seek to find the solution at some subsequent time \\(t+\\Delta t\\).Consider the example of linear time-invariant capacitor\\[i = C\\dfrac{dv}{dt}\\]\\[v(t+\\Delta t) = v(t) + \\dfrac{1}{C}\\int_{t}^{t+\\Delta t} i(\\tau)d\\tau\\]If we approximate the integratoin by trapezoidal approximation\\[\\int_{t}^{t+\\Delta t} i(\\tau) d\\tau \\approx \\dfrac{\\Delta t}{2}[i(t)+i(t+\\Delta t)]\\]thus\\[v(t+\\Delta t) \\approx v(t) + \\dfrac{\\Delta t}{2C}[i(t)+i(t+\\Delta t)]\\]or\\[i(t+\\Delta t) \\approx \\dfrac{2C}{\\Delta t} v(t+\\Delta t) - \\Big( i(t) + \\dfrac{2C}{\\Delta t} v(t) \\Big)\\]The equation can be viewed as the following circuit.Thus \\(v(t+\\Delta t)\\) can be solved by DC analysis.For Nonlinear CircuitSimilarly, the nonlinear component will be linearized at \\(v(t)\\), and the capacitor will be convert to the circuits shown above.Then \\(v(t+\\Delta t)\\) can be solved by DC analysis." }, { "title": "5T OTA Design", "url": "/posts/5tota-design/", "categories": "analog-circuit", "tags": "analog, amplifier", "date": "2023-06-14 05:00:00 +0000", "snippet": "Assume we need to design an amplifier with the following specification. Load capacitance \\(C_L\\). Gain-bandwidth product \\(f_u\\).Choose The TopologyAssume we choose the simple 5 transistor amplifier as the topology.Also we commonly use PMOS as the input differential pair because in many technologies: With same \\(g_m\\) and \\(I_d\\), PMOS will have larges size, thus smaller offset. N-well is more clean than the substrate. PMOS have less flicker noise, because of the larger size and holes are far from surface or interface compared with electrons.Choose The \\(g_m/I_d\\) Ratio For Each TransistorIf the focus is low noise (instead of small input capacitance), we will use larger \\(g_m/I_d\\) (e.g., 25) for the input differential pair.Since with the same current, they give us larger \\(g_m\\), thus smaller input referred voltage noise \\(4kT\\gamma / g_m\\).For the current mirror, we will use smaller \\(g_m/I_d\\), to reduce the current noise \\(4kT \\gamma g_m\\). However, if \\(g_m/I_d\\) is too small, the transistor will need a large overdrive voltage, then a small head room for the amplifier. As a example we may choose \\(g_m/I_d=16\\) for the current mirror transistors.Choose The Channel Length For Each TransistorThe choice of channel length is a bit arbitrary.In general longer channel will give higher intrinsic gain (\\(g_m/g_{ds}\\)).Usually we simulate the PMOS and NMOS seperately, to find the relation between channel length and intrinsic gain.In many technologies, intrinsic gain will increase until a certain channel length.We may choose such channel length as the initial design, to have enough gain without having too much parasitics.Find The \\(I_d/W\\) Ratio For Each TransistorAt the same time, simulate the \\(\\dfrac{I_D}{W}\\) vs. \\(\\dfrac{g_m}{I_D}\\) plot, for the later width determination.Since we have decided the \\(g_m/I_d\\) ratio for each transistor, the \\(I_d/W\\) value can be found through the plot.Find The First \\(g_m\\)For the 5 transistor amplifier, the \\(g_{mp}\\) of the input differential pair can be determined by\\[2\\pi f_u = A_v \\cdot \\omega_p = g_{mp} R_o \\cdot \\dfrac{1}{R_o C_L} = \\dfrac{g_{mp}}{C_L}\\]\\[g_{mp} = 2\\pi f_u C_L\\]Find All The CurrentsWe can use\\[I_{D} = \\dfrac{g_{m}}{g_m/I_D}\\]to determine the current for the input differential pair.Then all the currents can be determined by the topology.Find All the WidthWe can use\\[W = \\dfrac{I_D}{I_D/W}\\]to determine all the width of each transistor." }, { "title": "Delta Sigma Modulator", "url": "/posts/delta-sigma-modulator/", "categories": "math", "tags": "math", "date": "2023-06-08 05:00:00 +0000", "snippet": "Referenceslides_an_introduction_to_digital_delta_sigma_modulatorspaper a multiple modulator fractional dividerpaper A Calibration-Free Fractional-N Analog PLL With Negligible DSM Quantization Noise1st Order Error Feedback Modulator (EFM1)The dsim netlist example for \\(m=10\\)# 1st Order Error Feedback Modulator (EFM1)# The output average should be input / 1024--- Netlistx, s --&gt; add --&gt; vv, c --&gt; sub --&gt; ee --&gt; delay(1) --&gt; sy --&gt; scaler(pow(2,10)) --&gt; cv --&gt; integer_divide(pow(2,10)) --&gt; y--- Pinin: xout: e, yThe dsim maestro example for a single tone input--- Testdsim: /home/longhe/repos/dsimschematic: efm1runtime: yrunnum: 1000000saveall: True--- Parametersx_mem = []x = dsim.sig_init(lambda: lambda n: int(1024*1024*sin(0.02*n*pi)), x_mem)--- PlotsFs = 1e6ntf = lambda z: 1 - pow(z,-1)S_n_out = lambda f: frac(1/12, Fs/2) * plot.tf2psd(ntf, var='z', Fs=Fs)(f)fig, ax = plt.subplots(1,2)plot.plot_psd(fig,ax[0],y_mem,Fs=Fs,color='C0',sumover_N=1,label='output')plot.plot_psd(fig,ax[0],y_mem,Fs=Fs,color='C1',sumover_N=10000,label='averaged output')plot.plot_psd(fig, ax[0], S_n_out, Fs=Fs, fstart = 1e3, fend=Fs/2,color='C2',label='NTF')ax[0].legend(fontsize=16)plot.plot_psd(fig,ax[1],y_mem,Fs=Fs,color='C1',sumover_N=10000,label='averaged output')plot.plot_psd(fig, ax[1], S_n_out, Fs=Fs, fstart = 1e3, fend=Fs/2,color='C2',label='NTF')ax[1].legend(fontsize=16)ax[1].set_ylim(-100,-60)ax[1].set_xlim(3e3,Fs/2)plt.show()the classical model of quantization (CMQ)The \\(e_q\\) is assumed to be uniform distributed in \\([-1/2,1/2]\\), thus its variance\\[\\int_{-1/2}^{1/2} x^2 dx = \\dfrac{1}{12}\\]also it is assumed to have flat psd in \\([0, F_s/2]\\).It can be calculated the NTF (noise transfer function) to the output \\(y\\) is\\[\\begin{align}NTF(z) &amp;= 1 - z^{-1}\\end{align}\\]Thus the output noise psd is\\[\\begin{align}S_{n,out}(f) &amp;= \\dfrac{1}{12} \\cdot \\dfrac{1}{F_s/2} \\cdot \\vert 1 - e^{-j 2\\pi f/Fs} \\vert^2\\\\&amp;= \\dfrac{1}{6} \\cdot \\dfrac{1}{Fs} \\cdot \\left( 1 - e^{-j 2\\pi f/Fs} \\right) \\left( 1 - e^{j 2\\pi f/Fs} \\right)\\\\&amp;= \\dfrac{1}{6} \\cdot \\dfrac{1}{Fs} \\cdot e^{-j\\pi f/Fs} \\cdot \\left( e^{j\\pi f/Fs} - e^{-j \\pi f/Fs} \\right) \\cdot e^{j \\pi f/Fs} \\cdot \\left( e^{-j \\pi f/Fs} - e^{j \\pi f/Fs} \\right)\\\\&amp;= \\dfrac{1}{6} \\cdot \\dfrac{1}{Fs} \\cdot 2 j \\sin(\\pi f/Fs) \\cdot 2 j \\sin(-\\pi f/Fs)\\\\&amp;= \\dfrac{1}{6 Fs} \\cdot \\left( 2 \\sin\\left(\\dfrac{\\pi f}{Fs}\\right)\\right)^2\\\\&amp; \\approx \\dfrac{1}{6} \\cdot \\dfrac{(2\\pi f)^2}{F_s^3}\\end{align}\\]MASHMASH stands for Multi-stAge noise SHaping.MASH 1-1The dsim netlist example for MASH 1-1# MASH-1-1--- Netlistx --&gt; efm1 --&gt; e1, y1e1 --&gt; efm1 --&gt; e2, y2y2 --&gt; tf([1,-1],[1,0],1) --&gt; net1y1, net1 --&gt; add --&gt; y--- Pinin: xout: yThe dsim maestro exampel for MASH 1-1--- Testdsim: /home/longhe/repos/dsimschematic: mash_1_1runtime: yrunnum: 1000000saveall: True--- Parametersx_mem = []x = dsim.sig_init(lambda: lambda n: int(1024*sin(0.02*n*pi)), x_mem)--- PlotsFs = 1e6ntf = lambda z: 1 - pow(z,-1)S_n_out = lambda f: frac(1/12, Fs/2) * plot.tf2psd(ntf, var='z', Fs=Fs)(f)fig, ax = plt.subplots(1,2)plot.plot_psd(fig,ax[0],y_mem,Fs=Fs,color='C0',sumover_N=1,label='output')ax[0].legend(fontsize=16)plot.plot_psd(fig,ax[1],y_mem,Fs=Fs,color='C1',sumover_N=10000,label='averaged output')plot.plot_psd(fig, ax[1], S_n_out, Fs=Fs, fstart = 1e3, fend=Fs/2,color='C2',label='NTF')ax[1].legend(fontsize=16)ax[1].set_ylim(-100,-60)ax[1].set_xlim(3e3,Fs/2)plt.show()The four transfer function is EFM1 is\\[\\begin{align}H_{x,y}(z) &amp;= \\dfrac{1}{2^m}\\\\H_{e_q,y}(z) &amp;= 1-z^{-1} \\\\H_{x,e}(z) &amp;= 0\\\\H_{e_q,e} (z) &amp;= - 2^m\\end{align}\\]The MASH 1-1 STF\\[\\begin{align}y_1 &amp;= \\dfrac{1}{2^m} x\\\\e_1 &amp;= 0\\\\y_2 &amp;= 0\\\\y &amp;= \\dfrac{1}{2^m} x\\end{align}\\]The MASH 1-1 NTF\\[\\begin{align}y_1 &amp;= (1-z^{-1}) e_{q1}\\\\e_1 &amp;= -2^m e_{q1}\\\\y_2 &amp;= \\dfrac{1}{2^m} e_1 + (1-z^{-1}) e_{q2}\\\\&amp;= -e_{q1} + (1-z^{-1}) e_{q2}\\\\y &amp;= y_1 + (1-z^{-1})y_2\\\\&amp;= (1-z^{-1}) e_{q1} - (1-z^{-1}) e_{q1} + (1-z^{-1})^2 e_{q2}\\\\&amp;= (1-z^{-1})^2 e_{q2}\\end{align}\\]MASH 1-1-1The dsim netlist example for MASH 1-1-1# MASH-1-1-1--- Netlistx --&gt; efm1 --&gt; e1, y1e1 --&gt; efm1 --&gt; e2, y2e2 --&gt; efm1 --&gt; e3, y3y3 --&gt; diff --&gt; net3net3, y2 --&gt; add --&gt; net2net2 --&gt; diff --&gt; net1y1, net1 --&gt; add --&gt; y--- Pinin: xout: yThe dsim maestro example for MASH 1-1-1--- Testdsim: /home/longhe/repos/dsimschematic: mash_1_1_1runtime: yrunnum: 1000000save: x, y--- Parametersx_mem = []x = dsim.sig_init(lambda: lambda n: int(1024*sin(0.02*n*pi)), x_mem)--- PlotsFs = 1e6ntf = lambda z: pow(1 - pow(z,-1),3)S_n_out = lambda f: frac(1/12, Fs/2) * plot.tf2psd(ntf, var='z', Fs=Fs)(f)y_np = np.array(y_mem)y_np = y_np - np.mean(y_mem)fig, ax = plt.subplots(1,2)plot.plot_psd(fig,ax[0],y_np.tolist(),Fs=Fs,color='C0',sumover_N=1,label='output')plot.plot_psd(fig,ax[0],y_np.tolist(),Fs=Fs,color='C1',sumover_N=1000,label='averaged output')plot.plot_psd(fig, ax[0], S_n_out, Fs=Fs, fstart = 1e4, fend=Fs/2,color='C2',label='NTF')ax[0].legend(fontsize=16)plot.plot_psd(fig,ax[1],y_np.tolist(),Fs=Fs,color='C1',sumover_N=10000,label='averaged output')plot.plot_psd(fig, ax[1], S_n_out, Fs=Fs, fstart = 1e4, fend=Fs/2,color='C2',label='NTF')ax[1].legend(fontsize=16)# ax[1].set_ylim(-100,-60)# ax[1].set_xlim(3e3,Fs/2)plt.show()\\[\\begin{align}NTF(z) &amp;= (1-z^{-1})^3\\\\y &amp;= (1-z^{-1})^3 e_{q3}\\end{align}\\]" }, { "title": "Calculus 006", "url": "/posts/calculus-006/", "categories": "math", "tags": "math", "date": "2023-05-29 05:00:00 +0000", "snippet": "From Zhenyu Qi’ Lecture 006Seperated SequencesGiven a sequence \\(a_n (n \\in \\mathbb{N})\\), we can seperate all terms into two sequences:\\[a_{n_1}, a_{n_2}, \\dots \\text{ and } a_{n_1'}, a_{n_2'}, \\dots\\]with\\[n_1 &lt; n_2 &lt; \\dots \\text{ and } n_1' &lt; n_2' &lt; \\dots\\]also\\[\\begin{align}\\{n_1, n_2, \\dots\\} \\cup \\{n_1', n_2', \\dots\\} &amp;= \\mathbb{N}\\\\\\{n_1, n_2, \\dots\\} \\cap \\{n_1', n_2', \\dots\\} &amp;= \\emptyset\\end{align}\\]s.t. \\(a_{n_j} \\ge 0 (j \\in \\mathbb{N})\\) and \\(a_{n_k'} \\le 0 (k \\in \\mathbb{N})\\)Let \\(p_j := a_{n_j} (j \\in \\mathbb{N})\\) and \\(q_k := - a_{n_k'} (k \\in \\mathbb{N})\\), we know \\(p_j \\ge 0, q_k \\ge 0\\).Exercise 3: Show that\\[\\sum_{n} \\vert a_n \\vert &lt; \\infty \\iff \\sum_{j} p_j &lt; \\infty \\land \\sum_{k} q_k &lt; \\infty\\]Moreover, if and side holds, then\\[\\sum_{n} \\vert a_n \\vert = \\sum_{j} p_j + \\sum_k q_k\\]\\[\\sum_{n} a_n = \\sum_{j} p_j - \\sum_k q_k\\]Proof:  \\(\\implies\\): Let the partial sum of \\(\\sum_n \\vert a_n \\vert\\) has upper bound M.The partial sum of \\(\\sum_j p_j\\) is less or equal to the partial sum of \\(\\sum_n \\vert a_n \\vert\\)\\[\\begin{align}&amp;p_1 + \\dots + p_k = a_{n_1} + \\dots + a_{n_k}\\\\\\le &amp; \\vert a_1 \\vert + \\vert a_2 \\vert + \\dots + \\vert a_{n_k} \\vert \\le M\\end{align}\\]Thus the right side has upper bound, thus converges.  \\(\\Longleftarrow\\): Let the partial sum of \\(\\sum_j p_j, \\sum_k q_k\\) has uppber bound \\(U, V\\) respectively. Consider the partial sum of \\(\\sum_n \\vert a_n \\vert\\)\\[\\vert a_1 \\vert + \\dots + \\vert a_k \\vert \\le p_1 + \\dots + p_k + q_1 + \\dots + q_k \\le U + V\\]To prove \\(\\sum_{n} \\vert a_n \\vert = \\sum_{j} p_j + \\sum_k q_k\\), let \\(s_k = \\vert a_1 \\vert + \\dots + \\vert a_k \\vert\\), \\(u_k = p_1 + \\dots + p_k\\), \\(v_k = q_1 + \\dots + q_k\\), and let\\[\\lim_{k\\to \\infty} u_k = L_p\\]\\[\\lim_{k\\to\\infty} v_k = L_q\\]We know\\[s_k \\le u_k + v_k \\implies \\lim_{k \\to \\infty} s_k \\le L_p + L_q\\]also\\[u_k + v_k \\le s_{\\max(n_k, n'_k)}\\]It is easy to see \\(\\lim_{k\\to\\infty} s_{\\max(n_k,n'_k)} = \\lim_{k\\to\\infty} s_k\\), thus\\[\\lim_{k\\to\\infty} s_k \\ge L_p + L_q\\]Thus \\(\\sum_{n} \\vert a_n \\vert = \\sum_{j} p_j + \\sum_k q_k\\).To prove \\(\\sum_{n} a_n = \\sum_j p_j - \\sum_k q_k\\), now let \\(s_k = a_1 + \\dots + a_k\\)\\[\\forall \\varepsilon &gt; 0 \\exists N \\in \\mathbb{N} [k \\ge N \\implies \\vert u_k - L_p \\vert &lt; \\varepsilon \\land \\vert v_k - L_q \\vert &lt; \\varepsilon]\\]\\[k \\ge \\max(n_N, n_N') \\implies s_{k} = u_{N_1} - v_{N_2}, \\text{ where } N_1 \\ge N, n_2 \\ge N\\]\\[k \\ge \\max(n_N, n_N') \\implies \\vert s_{k} - (L_p - L_q) \\vert = \\vert (u_{N_1} - L_p) + (v_{N_2} - L_q) \\vert \\le 2 \\varepsilon\\]Thus \\(\\sum_{n} a_n = \\sum_j p_j - \\sum_k q_k\\).Two Convergent Series   \\(\\sum_n a_n\\) and \\(\\sum_n b_n\\) converge \\(\\implies \\sum_{n} (a_n \\pm b_n) = \\sum_n a_n \\pm \\sum_n b_n\\)\\[A_n = a_1 + \\dots + a_n, B_n = b_1 + \\dots + b_n, S_n = a_1 + b_1 + \\dots + a_n + b_n\\]\\[\\forall \\varepsilon &gt; 0 \\exists N \\in \\mathbb{N} [n \\ge N \\implies \\vert A_n - A \\vert &lt; \\varepsilon \\land \\vert B_n - B \\vert &lt; \\varepsilon]\\]\\[\\forall \\varepsilon &gt; 0 \\exists N \\in \\mathbb{N} [n \\ge N \\implies \\vert S_n - (A + B)\\vert &lt; 2 \\varepsilon]\\] Inserting finite number of 0s into all the two neighbor terms, will not affect its convergence or divergence and its sum (if the sum exists).16min" }, { "title": "Calculus 005", "url": "/posts/calculus-005/", "categories": "math", "tags": "math", "date": "2023-05-27 05:00:00 +0000", "snippet": "From Zhenyu Qi’ Lecture 005Series Convergent Criterion\\[\\sum_{n} a_n \\text{ converges} \\iff s_{n} (n \\in \\mathbb{N}) \\text{ converges } \\iff s_n (n \\in \\mathbb{N}) \\text{ is Cauchy}\\]\\[\\forall \\varepsilon &gt; 0, \\exists N \\in \\mathbb{N}, [k &gt; N, l \\ge 0 \\implies \\vert a_k + \\dots + a_{k+l} \\vert &lt; \\varepsilon ]\\]since\\[\\vert a_{k} + \\dots + a_{k+l} \\vert \\le \\vert a_k \\vert + \\dots + \\vert a_{k+l} \\vert\\]thus if we have \\(\\sum_n \\vert a_n \\vert\\) convergent, we are sure \\(\\sum_{n} a_n\\) convergent.Since the partial sum of \\(\\sum_{n} \\vert a_n \\vert\\) monotone, it converges if and only if its partial sum has a upper bound.\\[\\exists M &gt; 0 \\forall n \\in \\mathbb{N} [\\vert a_1 \\vert + \\dots \\vert a_n \\vert \\le M]\\]Notation: If \\(a_n \\ge 0 (n \\in \\mathbb{N})\\), then we write \\(\\sum_{n} a_n &lt; \\infty\\) to mean that \\(\\sum_n a_n\\) converges.We write \\(\\sum_{n} a_n = \\infty\\) to mean that it diverges.Examples (1) \\(\\sum_{n=1}^{\\infty} \\frac{1}{n}\\)\\[S_1 = 1\\]\\[S_2 = 1 + \\frac{1}{2}\\]\\[S_4 = 1 + \\frac{1}{2} + \\frac{1}{3} + \\frac{1}{4} &gt; 1 + \\frac{1}{2} + \\frac{1}{2}\\]\\[\\begin{align}\\forall m \\in \\mathbb{N}, S_{2^m} &amp;= 1 + \\frac{1}{2} + \\frac{1}{3} + \\frac{1}{4} + \\dots + \\frac{1}{2^{m-1}+1} + \\dots + \\frac{1}{2^{m-1}+2^{m-1}}\\\\&amp; &gt; 1 + \\frac{m}{2}\\end{align}\\](2) \\(\\sum_{n=1}^{\\infty} \\frac{1}{n^2}\\)\\[\\frac{1}{n^2} &lt; \\frac{1}{n(n-1)} = \\frac{1}{n-1} - \\frac{1}{n}\\]\\[\\begin{align}S_n &amp;= 1 + \\frac{1}{2^2} + \\dots + \\frac{1}{n^2} &lt; 1 + (\\frac{1}{1} - \\frac{1}{2}) + (\\frac{1}{2}-\\frac{1}{3}) + \\dots + (\\frac{1}{n-1} - \\frac{1}{n})\\\\&amp; &lt; 1 + 1 - \\frac{1}{n} &lt; 2\\end{align}\\](3) \\(\\sum_{n=1}^{\\infty} \\dfrac{\\sin(n)}{n^k}\\), for \\(k \\in \\mathbb{N}, k \\ge 2\\)\\[\\vert \\dfrac{\\sin(1)}{1^k} \\vert + \\dots + \\vert \\dfrac{\\sin(n)}{n^k} \\vert &lt; 1 + \\dots + \\frac{1}{n^2} &lt; 2\\]thus \\(\\sum_{n=1}^{\\infty} \\dfrac{\\sin(n)}{n^k}\\) converges.Ex2. For \\(a &gt; 1\\) and \\(k \\in \\mathbb{N}\\), show that \\(\\sum_{n=1}^{\\infty} \\frac{n^k}{a^n} &lt; \\infty\\)It is obvious all terms are non-negative.Similar from the exercise in Calculus 003, for any \\(n &gt; 2k+ 4 &gt; k+2\\)\\[a^n = (1+b)^n \\ge \\binom{n}{k+2} b^{k+2} \\ge \\dfrac{b^{k+2}}{(k+2)!}(n-k-1)^{k+2} \\ge \\dfrac{b^{k+2}}{2^{k+2}(k+2)!} n^{k+2}\\]thus for any \\(n &gt; 2k + 4\\)\\[\\dfrac{n^k}{a^n} \\le \\dfrac{2^{k+2}(k+2)!}{b^{k+2}} \\dfrac{1}{n^2}\\]thus the partial sum has upper bound.Def: Given a sequence \\(a_n (n \\in \\mathbb{N})\\), we say that(1) \\(\\sum_{n} a_n\\) converges absolutely if \\(\\sum_{n} \\vert a_n \\vert &lt; \\infty\\).(2) \\(\\sum_{n} a_n\\) converges conditionally if \\(\\sum_{n} \\vert a_n \\vert = \\infty\\) but \\(\\sum_{n=1}^{\\infty} a_n\\) converges.Comparison TestIf \\(a_n, b_n (n \\in \\mathbb{N}) \\ge 0\\)(1) If we have\\[\\exists C &gt; 0 \\text{ and } N \\in \\mathbb{N} [n \\ge N \\implies a_n \\le C b_n]\\]then\\[\\sum_n b_n &lt; \\infty \\implies \\sum_n a_n &lt; \\infty\\]Proof:Since they are non-negative series, their convergence are equivalent to that the partial sum has upper bound.From the partial sum of \\(b_n\\) has upper bound, it is obvious that the partial sum of \\(a_n\\) has upper bound.(2) If we know\\[\\lim_{n\\to\\infty} \\dfrac{a_n}{b_n} \\text{ exists }\\]then\\[\\exists C &gt; 0 \\text{ and } N \\in \\mathbb{N} [n \\ge N \\implies a_n \\le C b_n]\\]Proof:Let \\(l = \\lim_{n\\to\\infty} \\dfrac{a_n}{b_n}\\) and \\(\\varepsilon=1\\)\\[\\exists N \\in \\mathbb{N} [n \\ge N \\implies l-1 &lt; \\dfrac{a_n}{b_n} &lt; l+1]\\]\\[\\exists N \\in \\mathbb{N} [n \\ge N \\implies a_n &lt; (l+1)b_n]\\]where \\(l+1\\) must be positive \\((C &gt; 0)\\), since \\(l \\ge 0\\).If \\(\\lim_{n\\to\\infty} \\dfrac{a_n}{b_n}\\) exists and not equal \\(0\\), it is easy to see \\(\\sum_{n} a_n &lt; \\infty \\iff \\sum_{n} b_n &lt; \\infty\\), since \\(a_n, b_n\\) can be bounded by each other. Actually, \\(\\lim_{n\\to\\infty} \\dfrac{a_n}{b_n}\\) exists can be replaced by \\(\\overline{\\lim_{n\\to\\infty}} \\dfrac{a_n}{b_n}\\) exists.Proof: Let \\(u_n = \\sup \\{\\dfrac{a_m}{b_m} \\vert m \\ge n\\}\\), \\(\\lim_{n\\to\\infty}u_n = l\\).\\[\\exists N \\in \\mathbb{N} [u_N &lt; l+1]\\]\\[\\exists N \\in \\mathbb{N} [l+1 \\text{ is an upper bound of } \\{\\dfrac{a_m}{b_m} \\vert m \\ge N\\}]\\]\\[\\exists N \\in \\mathbb{N} [n \\ge N \\implies \\dfrac{a_n}{b_n} \\le l + 1]\\]Ratio and Root TestsExercise 3 (the ratio and the root tests):Let \\(a_n (n \\in \\mathbb{N}) \\ge 0\\). Then(1) If \\(\\lim_{n\\to\\infty} \\dfrac{a_{n+1}}{a_n} &lt; 1\\), then\\[\\exists N \\in \\mathbb{N} \\text{ and } C &lt; 1 [n \\ge N \\implies a_{n+1} \\le C a_n]\\]then, \\(\\sum_{n} a_n &lt; \\infty\\)Proof: the idea is that\\[\\exists N \\in \\mathbb{N} \\text{ and }0 &lt; C &lt; 1 [n \\ge N \\implies a_{n} \\le C^{n-N} a_N]\\]Since the series \\({a_1, a_2, \\dots, a_{N-1}, C^{0} a_{N}, C^{1} a_{N}, \\dots}\\) converges, we know \\(a_n (n\\in\\mathbb{N})\\) converges.Similarly, if \\(\\lim_{n\\to\\infty} \\dfrac{a_{n+1}}{a_n} &gt; 1\\), we have \\(\\sum_n a_n = \\infty\\).If \\(\\lim_{n\\to\\infty} \\dfrac{a_{n+1}}{a_n} = 1\\), it may converge or diverge.(2) If \\(\\lim_{n\\to\\infty} (a_n)^{1/n} &lt; 1\\), then\\[\\exists N \\in \\mathbb{N} \\text{ and } C &lt; 1 [n \\ge N \\implies a_n \\le C^n]\\]thus \\(\\sum_{n} a_n &lt; \\infty\\).If \\(\\lim_{n\\to\\infty} (a_n)^{1/n} &gt; 1\\), then \\(\\sum_{n} a_n = \\infty\\).Alternating SeriesQuestion: does \\(\\sum_{n=1}^{\\infty} \\dfrac{(-1)^{n-1}}{n}\\) converges (conditionally)?Def: A series \\(\\sum_n a_n\\) is an alternating series if\\[\\exists b_n &gt; 0 (n \\in \\mathbb{N}) \\text{ s.t. } a_n = (-1)^{n-1} b_n (n \\in \\mathbb{N})\\]Confusion: \\(b_n &gt; 0\\) or \\(b_n \\ge 0\\)?Leibniz’s CriterionIf \\(\\sum_n a_n\\) is an alternating series, and \\(b_n (=\\vert a_n \\vert) \\searrow 0\\) as \\(n \\to \\infty\\) (monotonically non-increasing, and converges to \\(0\\)), then \\(\\sum_{n} a_n\\) converges.Proof:We know \\(b_n = (-1)^{n-1} a_n\\). Consider the tail\\[\\vert a_k + \\dots + a_{k+l} \\vert = \\vert (-1)^{k-1} (b_k - b_{k+1} + \\dots + (-1)^l b_{k+l}) \\vert\\]It is easy to see that \\(b_k - b_{k+1} + \\dots + (-1)^{l} b_{k+l} \\ge 0\\), no matter \\(l\\) is even or odd.\\[\\begin{align}&amp;\\vert a_k + \\dots + a_{k+l} \\vert = \\vert (-1)^{k-1} (b_k - b_{k+1} + \\dots + (-1)^l b_{k+l}) \\vert\\\\=&amp; b_k - b_{k+1} + \\dots + (-1)^l b_{k+l} \\le b_k\\end{align}\\]It is smaller than \\(b_k\\) since you can calculate it as \\(b_k - (b_{k+1}-b_{k+2}) - \\dots\\), no matter \\(l\\) is even or odd.Plus we know \\(\\lim_{n\\to\\infty} b_n = 0\\), the tail can be controlled, and the series \\(\\sum_{n} a_n\\) converges." }, { "title": "逻辑学 005", "url": "/posts/logics-005/", "categories": "math", "tags": "math", "date": "2023-05-21 06:00:00 +0000", "snippet": "来自 傅皓政 逻辑课程 005" }, { "title": "逻辑学 004", "url": "/posts/logics-004/", "categories": "math", "tags": "math", "date": "2023-05-21 05:00:00 +0000", "snippet": "来自 傅皓政 逻辑课程 004 语句：由符号组成的序列 命题：符号序列的意义或内容 语词论证：S是M，所有的P都是M，所以S是P 命题论证：如果P，则Q，P，所以Q命题逻辑语言 命题逻辑的字符集包含下列几个部分： 语句（或命题）符号：\\(P, Q, R, \\dots\\) 连接词：\\(\\lnot, \\land, \\lor, \\to, \\leftrightarrow\\) 辅助符号：\\((,)\\) 命题逻辑语言中的句式都可以经由下列的形式规则构建之。（\\(\\varphi\\)与\\(\\psi\\)为句式的变量） 每个语句符号都是句式。 如果\\(\\varphi\\)是一个句式，那么\\(\\lnot\\varphi\\)也是句式。 如果\\(\\varphi\\)和\\(\\psi\\)都是句式，那么\\(\\varphi\\land\\psi,\\varphi\\lor\\psi,\\varphi\\to\\psi,\\varphi\\leftrightarrow\\psi\\)也都是句式。 除了经由规则(1)-(3)所构建的称为句式外，没有其他句式。 每个语句符号（如\\(P,Q,R\\)）成为原子句式。 直接子句式（immediate subformulae）的定义： 语句符号没有直接子句式。 句式\\(\\lnot\\varphi\\)的直接子句式只有\\(\\varphi\\)。 句式\\(\\varphi\\land\\psi, \\varphi\\lor\\psi, \\varphi\\to\\psi\\)以及\\(\\varphi\\leftrightarrow\\psi\\)的直接子句式为\\(\\varphi\\)和\\(\\psi\\)。 主要连接词（main connective）的定义： 连接词范围（scope）的定义： 优先级：   \\(\\lnot\\)   \\(\\lor, \\land\\)   \\(\\to, \\leftrightarrow\\) 语义值 语句的语义值为真假值。 语义学预设： 二值原则：命题具有真假值。但不能既真又假，也不能不真也不假。 真值函映原则：在古典逻辑中出现的语句连接词均为真值函映的关键词。 外延原则：复合语句的真假值是由原子语句的真假值决定，与语句本身的内容或意义无关。 基本真值表 \\(P\\) \\(\\lnot P\\) T F F T \\(P\\) \\(Q\\) \\(P \\lor Q\\) T T T T F T F T T F F F \\(P\\) \\(Q\\) \\(P \\land Q\\) T T T T F F F T F F F F \\(P\\) \\(Q\\) \\(P \\to Q\\) T T T T F F F T T F F T \\(P\\) \\(Q\\) \\(P \\leftrightarrow Q\\) T T T T F F F T F F F T 恒真句（tautology）：考虑语句的所有可能情况，如果该语句在所有可能情况中均为真，若且唯若，该语句为恒真句。   \\(P\\to P\\)   \\(P \\lor \\lnot P\\)   \\(P \\to (Q \\to P)\\) 矛盾句（contradiction）：   \\(P \\land \\lnot P\\)   \\(\\lnot(P\\to(Q \\to P))\\)   \\(P \\leftrightarrow \\lnot P\\) 偶真句命题之间的关系 蕴含关系（implication）：   \\(\\varphi_1, \\varphi_2, \\dots, \\varphi_n \\models \\psi\\) 若且唯若 \\((\\varphi_1 \\land \\varphi_2 \\land \\dots \\land \\varphi_n) \\to \\psi\\) 为恒真句。   \\(\\models \\psi\\) 若且唯若 \\(\\psi\\)为恒真句。   \\(\\varphi_1, \\varphi_2, \\dots, \\varphi_n \\models\\) 若且唯若 \\(\\varphi_1 \\land \\varphi_2 \\land \\dots \\land \\varphi_n\\) 为矛盾句。 等值关系（equivalence）:   \\(\\models \\varphi \\leftrightarrow \\psi\\)   \\(\\models (P \\to Q) \\leftrightarrow (\\lnot P \\lor Q)\\)   \\(\\models (P \\lor Q) \\leftrightarrow \\lnot(\\lnot P \\land \\lnot Q)\\) 不一致（inconsistency） 对句式集合\\(\\Gamma\\), \\(\\Gamma\\)是不一致的，若且唯若，集合中所有句式的连言是矛盾句。记为\\(\\Gamma \\models\\) 一致性： 集合中所有句式的连言不是矛盾句。记为\\(\\Gamma \\not\\models\\) " }, { "title": "Calculus 004", "url": "/posts/calculus-004/", "categories": "math", "tags": "math", "date": "2023-05-18 06:00:00 +0000", "snippet": "From Zhenyu Qi’ Lecture 004Some comments on our usage of logic termsStandard logic terms\\[\\forall \\varepsilon \\in \\mathbb{R} \\, [\\varepsilon &gt; 0 \\implies \\exists N \\in \\mathbb{N} \\, \\forall n \\in \\mathbb{N} \\, [ n \\ge N \\implies \\vert a_n - L \\vert &lt; \\varepsilon]]\\]informal way\\[\\forall \\varepsilon &gt; 0 \\, \\exists N \\in \\mathbb{N} \\, [n \\ge N \\implies \\vert a_n - L \\vert &lt; \\varepsilon]\\]Ex. Let \\(a_n, b_n (n \\in \\mathbb{N})\\) be two bounded sequence.Then\\[\\overline{\\lim_{n\\to\\infty}} (a_n + b_n) \\le \\overline{\\lim_{n\\to\\infty}} a_n + \\overline{\\lim_{n\\to\\infty}}b_n\\]\\[\\underline{\\lim}\\limits_{n\\to\\infty} (a_n + b_n) \\ge \\underline{\\lim}\\limits_{n\\to\\infty} a_n + \\underline{\\lim}\\limits_{n\\to\\infty} b_n\\]Let \\(u_n = \\sup \\{a_m \\vert m \\ge n\\}, v_{n} = \\sup \\{b_m \\vert m \\ge n\\}, y_{n} = \\sup \\{a_m + b_m \\vert m \\ge n\\}\\).Since \\(u_n\\) is a upper bound of \\(\\{a_m \\vert m \\ge n\\}\\), \\(v_n\\) is a upper bound of \\(\\{b_m \\vert m \\ge n\\}\\).We have \\(\\forall m \\ge n, u_n \\ge a_m, v_n \\ge b_m\\). Thus\\[\\forall m \\ge n, u_n + v_n \\ge a_m + b_m\\]Thus \\(u_n + v_n\\) is a upper bound of \\(\\{a_m + b_m \\vert m \\ge n\\}\\).Since \\(y_n\\) is the least upper bound\\[y_n \\le u_n + v_n\\]\\[\\lim_{n\\to\\infty} y_n \\le \\lim_{n\\to\\infty} u_n + \\lim_{n\\to\\infty} v_n\\]Ex. What about \\(\\overline{\\lim_{n\\to\\infty}} (a_n \\times b_n)\\)? \\(\\overline{\\lim_{n\\to\\infty}} (a_n \\div b_n)\\)?If the sequences are all positive, we can have the same conclusion.Cauchy Sequence ConvergentSince Cauchy sequence is bounded, to prove a bounded sequence convergent, we only need to show \\(\\lim_{n\\to\\infty} u_n = \\lim_{n\\to\\infty} l_n\\), thus we only need to show \\(\\lim_{n\\to\\infty} (u_n - l_n) = 0\\).For Cauchy sequence\\[\\forall \\varepsilon &gt; 0, \\exists N \\in \\mathbb{N}, [n \\ge N \\implies a_N - \\varepsilon &lt; a_n &lt; a_{N} + \\varepsilon]\\]\\[\\forall \\varepsilon &gt; 0, \\exists N \\in \\mathbb{N}, [n \\ge N \\implies a_N - \\varepsilon \\le l_N \\le l_n \\le a_n \\le u_n \\le u_N \\le a_{N} + \\varepsilon]\\]\\[\\forall \\varepsilon &gt; 0, \\exists N \\in \\mathbb{N}, [n \\ge N \\implies \\vert u_n - l_n - 0 \\vert \\le 2 \\varepsilon]\\]thus we finish the proof.Ex 1. Let \\(S \\subseteq \\mathbb{R}\\). Prove that if \\(\\vert s - s' \\vert \\le 3\\) for all \\(s, s'\\), then(1) \\(S\\) is bounded.(2) \\(\\sup S - \\inf S \\le 3\\).What if \\(\\vert s - s' \\vert &lt; 3\\)? We still only have \\(\\sup S - inf S \\le 3\\).Consider the example of \\(S = (0,3)\\).(1) is obvious. (2) Were this false, \\(\\sup S - \\inf S = a &gt; 3\\).thus \\(\\sup S = \\inf S + a\\), thus let \\(b = \\inf S + a - (a-3)/2 &lt; \\sup S\\), \\(c = \\inf S + (a-3)/2 &gt; \\inf S\\).We know \\(c &lt; b\\) and \\(b\\) is not upper bound of \\(S\\), \\(c\\) is not lower bound of \\(S\\).\\[\\exists x, y \\in S, [y &lt; c &lt; b &lt; x]\\]\\[\\vert x - y \\vert &gt; b - c = 3\\]SeriesQuestion: What does \\(1 + \\frac{1}{2} + \\frac{1}{2^2} + \\dots = 2\\) mean?Def: Let \\(a_n (n \\in \\mathbb{N})\\) be a sequence of \\(\\mathbb{R}\\).We say that the series \\(\\sum_{n=1}^{\\infty} a_n\\) converges to a real number \\(S\\), if\\[\\lim_{n\\to\\infty} s_n = S, \\text{ where } s_n := \\sum_{j=1}^{n} a_j\\]we usually call \\(s_n\\) the \\(n\\)=th partial sum of the series.What if it is not easy to calculate the closed form of \\(s_n\\)?Take the examples\\[1 + \\frac{1}{2} + \\frac{1}{3} + \\dots\\]\\[1 - \\frac{1}{2} + \\frac{1}{3} - \\frac{1}{4} + \\dots\\]Notation: For a series \\(\\sum_{n} a_n\\) and \\(l, m \\in \\mathbb{N}, l &lt; m\\), we call\\[s_{l,m} := \\sum_{j=l}^{m} a_j\\]the \\((l,m)\\)-tail.   \\(\\sum_{n} a_n\\) converges \\(\\implies \\lim_{n\\to\\infty} a_n = 0\\).We know \\(s_n\\) is Cauchy sequence\\[\\forall \\varepsilon &gt; 0, \\exists N \\in \\mathbb{N}, [m,n &gt;= N \\implies \\vert s_m - s_n \\vert &lt; \\varepsilon]\\]\\[\\forall \\varepsilon &gt; 0, \\exists N \\in \\mathbb{N}, [n \\ge N \\implies \\vert s_{n+1} - s_n \\vert = \\vert a_{n+1} \\vert &lt; \\varepsilon]\\]Or\\[\\lim_{n\\to\\infty} s_n = \\lim_{n\\to\\infty} s_{n-1} = S\\]\\[\\lim_{n\\to\\infty} (s_n - s_{n-1}) = \\lim_{n\\to\\infty} a_n = 0\\]" }, { "title": "Calculus 003", "url": "/posts/calculus-003/", "categories": "math", "tags": "math", "date": "2023-05-18 05:00:00 +0000", "snippet": "From Zhenyu Qi’ Lecture 003Example: If \\(a&gt;1\\), then \\(\\lim_{n\\to\\infty} \\dfrac{1}{a^n} = 0\\).\\[\\dfrac{1}{(1+(a-1))^n} = \\dfrac{1}{(1+b)^n} \\le \\dfrac{1}{1 + nb}\\]Ex1 (squeeze theorem): \\(\\lim_{n\\to\\infty}a_n = \\lim_{n\\to\\infty}b_n = L\\), and \\(a_n \\le c_n \\le b_n\\), then \\(\\lim_{n\\to\\infty}c_n = L\\).\\[\\forall \\varepsilon &gt; 0, \\exists N_a, N_b \\in \\mathbb{N} [n \\ge \\max(N_a, N_b) \\implies L-\\epsilon &lt; a_n \\le b_n &lt; L + \\epsilon]\\]\\[\\forall \\varepsilon &gt; 0, \\exists N_a, N_b \\in \\mathbb{N} [n \\ge \\max(N_a, N_b) \\implies L-\\epsilon &lt; a_n \\le c_n \\le b_n &lt; L + \\epsilon]\\]Ex: prove for any \\(k \\in \\mathbb{N}\\), and real number \\(a &gt; 1\\), \\(\\lim_{n\\to\\infty} \\dfrac{n^k}{a^n} = 0\\)Let \\(a = 1+b\\) where \\(b &gt; 0\\).\\[\\text{ for any } n &gt; k+1, \\quad a^n = (1+b)^n \\ge \\binom{n}{k+1}b^{k+1} \\ge \\dfrac{b^{k+1}}{(k+1)!} (n-k)^{k+1}\\]\\[0 \\le \\dfrac{n^k}{a^n} \\le \\dfrac{n^k}{(n-k)^k} \\dfrac{1}{n-k} \\dfrac{k!}{b^k}\\]By squeeze theorem, we have \\(\\lim_{n\\to\\infty} \\dfrac{n^k}{a^n} = 0\\).Monotone(1) nondecreasingly monotone / increasing: \\(\\forall n \\in N, a_n \\le a_{n+1}\\)nonincreasingly monotone / decreasing: \\(\\forall n \\in N, a_n \\ge a_{n+1}\\)\\[a_n \\nearrow \\quad a_n \\searrow\\](2) strictly increasing (resp. strictly decreasing)\\[a_n \\nearrow\\nearrow \\quad a_n \\searrow\\searrow\\]Theorem: (boundedness from above \\(+ \\nearrow \\implies\\) convergence)Since it is boundedness from above, let \\(L = \\sup \\{a_n\\}\\).\\[\\forall \\varepsilon&gt; 0, L - \\varepsilon \\text{ is not upper bound}, \\exists N \\in \\mathbb{N}, [n \\ge N \\implies a_n \\ge a_{N} &gt; L - \\varepsilon]\\]plus \\(a_n \\le L\\), we have \\(\\vert a_n - L \\vert &lt; \\epsilon\\).The Decimal ExpressionExamples: (1) A decimal expression gives a real number.\\[0.d_1 d_2 d_3 \\dots\\]\\[a_1 = 0.d_1, a_2 = 0.d_1 d_2, \\dots\\]\\[\\forall n \\in \\mathbb{N} \\, [a_n \\in \\mathbb{Q}]\\]\\[a_n \\nearrow, \\text{ it has uppber bound } 1\\]one real number may have different decimal expression, 1, 0.999999The Natural Base(2) The natural base \\(e\\)\\[e = \\lim_{n\\to\\infty} (1 + \\dfrac{1}{n})^n\\]\\[\\begin{align}a_n &amp;= \\binom{n}{0} + \\binom{n}{1}\\dfrac{1}{n} + \\binom{n}{2}\\dfrac{1}{n^2} + \\dots + \\binom{n}{n}\\dfrac{1}{n^n}\\\\&amp;= \\sum_{j=0}^n \\binom{n}{j}\\dfrac{1}{n^j}\\end{align}\\]\\[\\binom{n}{j} = \\dfrac{n!}{j!(n-j)!}\\]\\[\\begin{align}&amp;\\dfrac{n!}{j!(n-j)!} \\dfrac{1}{n^j} = \\dfrac{1}{j!} \\dfrac{n (n-1) \\dots (n-j+1)}{n^j}\\\\=&amp; \\dfrac{1}{j!} (1-\\frac{1}{n})(1-\\frac{2}{n})\\dots (1 - \\dfrac{j-1}{n})\\end{align}\\]\\[a_n = 1 + 1 + \\dfrac{1}{2!}(1-\\frac{1}{n}) + \\dfrac{1}{3!}(1-\\frac{1}{n})(1-\\frac{2}{n}) + \\dots + \\dfrac{1}{n!}(1-\\frac{1}{n})\\dots (1-\\frac{n-1}{n})\\]\\[a_{n+1} = 1 + 1 + \\dfrac{1}{2!}(1-\\frac{1}{n+1}) + \\dfrac{1}{3!}(1-\\frac{1}{n+1})(1-\\frac{2}{n+1}) + \\dots + \\dfrac{1}{n!}(1-\\frac{1}{n+1})\\dots (1-\\frac{n-1}{n+1}) + \\dfrac{1}{(n+1)!}(1-\\frac{1}{n+1})\\dots (1-\\frac{n}{n+1})\\]thus\\[a_{n+1} &gt; a_{n} \\quad a_{n} \\nearrow\\nearrow\\]and\\[\\begin{align}a_n &amp;&lt; 1 + 1 + \\frac{1}{2!} + \\dots + \\frac{1}{n!}\\\\&amp;&lt; 1 + 1 + \\frac{1}{2} + \\frac{1}{2^2} + \\dots \\frac{1}{2^{n-1}}\\\\&amp;= 1 + \\dfrac{1-\\frac{1}{2^n}}{1-\\frac{1}{2}} &lt; 3\\end{align}\\]in the near future we will see that\\[e = \\lim_{n\\to\\infty}(1 + 1 + \\frac{1}{2!} + \\dots + \\frac{1}{n!})\\]Nested IntervalsDefinition: A sequence of intervals \\(I_n (n \\in \\mathbb{N})\\) is nested if \\(I_n \\ne \\emptyset\\) and \\(I_{n+1} \\subseteq I_n\\) for all \\(n \\in \\mathbb{N}\\).Question: \\(\\bigcap\\limits_{n\\in\\mathbb{N}} I_n \\ne \\emptyset\\)?Sometimes it becomes emptyset.(1) \\(I_n = (0, \\frac{1}{n})\\)(2) \\(I_n = [n, \\infty)\\)Theorem (of nested intervals)If \\(I_{n} (n \\in \\mathbb{N})\\) is a sequence of bounded closed nested intervals (\\([a_n, b_n]\\)), then \\(\\bigcap\\limits_{n\\in\\mathbb{N}}I_n \\ne \\emptyset\\).Proof:Write \\(I_n = [a_n, b_n] (n \\in \\mathbb{N})\\), we know \\(a_n \\le b_n, \\quad a_n \\nearrow, \\quad b_n \\searrow\\).We know\\[\\forall n, m \\in \\mathbb{N}, a_n \\le a_{\\max{(n,m)}} \\le b_{\\max_{(n,m)}} \\le b_{m}\\]In other words, for every \\(m \\in \\mathbb{N}\\), \\(b_m\\) is a upper bound of \\(\\{a_n \\vert n \\in \\mathbb{N}\\}\\).Let \\(c=\\lim_{n\\to\\infty}a_n\\).Then \\(c\\) is the least uppber bound, \\(c \\ge a_n\\) for all \\(n\\).For all \\(m\\), \\(a_n \\le b_m\\), take the limits \\(\\lim_{n\\to\\infty} a_n \\le \\lim_{n\\to\\infty} b_m\\), thus \\(c \\le b_m\\) for all \\(m\\).Ex2: (1) What if \\(I_n = (a_n, b_n)\\) nested, but \\(a_n \\nearrow\\nearrow\\) and \\(b_n \\searrow\\searrow\\)?In this case the intersections are non-empty.\\[\\forall n, m \\in \\mathbb{N}, a_n &lt; a_{\\max(n,m)} &lt; b_{\\max(n,m)} &lt; b_{m}\\]Let \\(c = \\lim_{n\\to\\infty} a_n\\), then \\(c\\) is the least uppber bound, \\(c \\ge a_{n+1} &gt; a_{n}\\) for all \\(n\\).Also \\(c \\le b_{m+1} &lt; b_m\\)(2) \\(I_n = (a_n, \\infty)\\), nested and \\(\\{a_n\\}\\) bounded above.In this case the intersections are non-empty. Let \\(c\\) be an upper bound. For all \\(n\\), \\(c \\ge a_{n}\\), thus \\(c + 1 \\ge a_{n} + 1 &gt; a_n\\)Equivalencegapless is equivalent to Weierstrass, to \\((\\text{ mono } + \\text{ Archimedean })\\) to \\((\\text{ nexted interval } + \\text{ Archimedean })\\).These expressions are used to determine if we can define something and it is a real number.For example, if we can find a bounded increasing sequence, then we know there is a real number to be its limit.Cauchy SequenceA sequence \\(a_n (n \\in \\mathbb{N})\\) in \\(\\mathbb{R}\\) is a Cauchy sequence if\\[\\forall \\varepsilon &gt; 0\\, \\exists N \\in \\mathbb{N} \\, [n, m \\ge N \\implies \\vert a_n - a_m \\vert &lt; \\varepsilon]\\]It is easy to see convergent \\(\\implies\\) Cauchy Cauchy \\(\\implies\\) bounded Theorem: Cauchy \\(\\iff\\) convergent.To prove the theorem, we need the definition of upper and lower sequence.Upper And Lower SequenceDefinition:For a bounded sequence \\(a_n\\), let\\[u_n = \\sup \\{a_m \\vert m \\ge n\\}\\]\\[l_n = \\inf \\{a_m \\vert m \\ge n\\}\\]we call \\(u_n\\) the upper sequence of \\(a_n\\), \\(l_n\\) the lower sequence of \\(a_n\\).We want to show for the bounded sequence \\(a_n\\), the upper sequence and lower sequence converge: \\(\\lim_{n\\to\\infty} u_n, \\lim_{n\\to\\infty} l_n\\) exists.It is easy to see \\(u_n \\searrow, \\, l_n \\nearrow\\).Also \\(l_n \\le a_n \\le u_n\\).For example, to show \\(u_n \\searrow\\), we need to show \\(\\forall i \\ge 1, u_{i+1} \\le u_{i}\\).Were this false, \\(\\exists i \\ge 1, u_{i} &lt; u_{i+1}\\).\\[u_{i} &lt; u_{i+1} = \\sup \\{a_m \\vert m \\ge i + 1\\}\\]\\[u_{i} \\text{ is not upper bound of } \\{a_m \\vert m \\ge i + 1\\}\\]\\[u_{i} \\text{ is upper bound of } \\{a_m \\vert m \\ge i\\} \\implies u_{i} \\text{ is upper bound of } \\{a_m \\vert m \\ge i+1\\}\\]Also, the sequence \\(\\{u_n\\}\\), \\(\\{l_n\\}\\) is bounded.For example, to show \\(\\{u_n\\}\\) is bounded, since \\(u_n \\searrow\\), we only need to show \\(\\{u_n\\}\\) has lower bound.Let \\(A\\) be a lower bound of \\(\\{a_n\\}\\), we want to show \\(A\\) is also a lower bound of \\(\\{u_n\\}\\). Were this false, \\(A\\) is not a lower bound of \\(\\{u_n\\}\\)\\[\\exists i, u_i &lt; A\\]\\[\\exists i, a_i \\le \\sup \\{a_m \\vert m \\ge i\\} = u_i &lt; A\\]Then we have \\(a_i &lt; A\\), this is impossible, since \\(A\\) is lower bound of \\(\\{a_n\\}\\).\\[\\overline{\\lim_{n\\to\\infty}} a_{n}=\\limsup_{n\\to\\infty} a_n := \\lim_{n\\to\\infty} u_n\\]\\[\\underline{\\lim}\\limits_{n\\to\\infty} \\liminf_{n\\to\\infty} a_n := \\lim_{n\\to\\infty} l_n\\]Ex4: \\(a_n\\) converges \\(\\iff\\) \\(\\limsup_{n\\to\\infty} a_n = \\liminf_{n\\to\\infty} a_n \\ne \\infty\\).And if it is the case, \\(\\lim_{n\\to\\infty}a_n = \\limsup_{n\\to\\infty} a_n = \\limsup_{n\\to\\infty} a_n\\)We have \\(l_n \\le a_n \\le u_n\\)  \\(\\Leftarrow\\) is from squeeze theorem.  \\(\\implies\\), Let \\(\\lim_{n\\to\\infty}a_n\\) = A\\[\\forall \\epsilon &gt; 0\\, \\exists N \\in\\mathbb{N}\\, [n \\ge N \\implies A - \\varepsilon &lt; a_n &lt; A + \\varepsilon ]\\]thus \\(A - \\varepsilon\\) is a lower bound of \\(\\{a_m \\vert m \\ge N\\}\\), and \\(l_N\\) is the greatest lower bound of \\(\\{a_m \\vert m \\ge N\\}\\), thus \\(A - \\varepsilon \\le l_N \\le l_n\\) for all \\(n \\ge N\\). Similarly\\[\\forall \\epsilon &gt; 0\\, \\exists N \\in\\mathbb{N}\\, [n \\ge N \\implies A - \\varepsilon \\le l_{N} \\le l_n \\le a_n \\le u_n \\le u_N \\le A + \\varepsilon ]\\]thus \\(\\limsup_{n\\to\\infty} a_n = \\liminf_{n\\to\\infty} a_n = A\\)." }, { "title": "Stochastic Process 15", "url": "/posts/stochastic-process-15/", "categories": "math", "tags": "math", "date": "2023-05-13 05:00:00 +0000", "snippet": "From Hao Zhang’s Lecture 15Markov chain\\[\\{X_n\\}_{n=0}^{\\infty}, \\quad X_k \\in S = \\{x_1, x_2, \\dots \\} \\text{ (finite or countably infinite) }\\]C-K equatoin\\[\\forall m &lt; n, \\quad P_{ij}(n) = \\sum_{k} P_{ik}(m)P_{kj}(n-m)\\]First passage probability\\[P_{ij}(n) = \\sum_{k=1}^n f_{ij}(k) P_{jj}(n-k)\\]Recurrent States  state \\(i\\) is recurrent \\(\\iff \\sum_{k=1}^{\\infty} f_{ii}(k) = 1\\)  state \\(i\\) is non-recurrent \\(\\iff \\sum_{k=1}^{\\infty} f_{ii}(k) &lt; 1\\)Confusion: how to show \\(\\sum_{k=1}^{\\infty} \\le 1\\)?\\[P_{ij}(n) = \\sum_{k=1}^n f_{ij}(k) P_{jj}(n-k)\\]The equation above is in convolution form. We define\\[\\begin{align}P_{ij}(z) &amp;= \\sum_{n=0}^{\\infty} P_{ij}(n) z^{n}\\\\f_{ij}(z) &amp;= \\sum_{n=1}^{\\infty} f_{ij}(n) z^{n}\\end{align}\\]we use the convention\\[\\begin{align}P_{ij}(0) &amp;= \\delta_{ij}\\\\f_{ij}(0) &amp;= 0\\end{align}\\]we have\\[\\begin{align}P_{ij}(z)=&amp;\\sum_{n=0}^{\\infty} P_{ij}(n) z^n = P_{ij}(0) + \\sum_{n=1}^{\\infty} \\Big( \\sum_{k=1}^{n} f_{ij}(k) P_{jj}(n-k) \\Big) z^{n}\\\\=&amp; P_{ij}(0) + \\sum_{k=1}^{\\infty} \\sum_{n=k}^{\\infty} f_{ij}(k) z^{k} P_{jj}(n-k) z^{n-k}\\\\=&amp; P_{ij}(0) + \\sum_{k=1}^{\\infty} f_{ij}(k) z^k \\sum_{n=k}^{\\infty} P_{jj}(n-k)z^{n-k}\\\\=&amp; P_{ij}(0) + \\sum_{k=1}^{\\infty} f_{ij}(k) z^k P_{jj}(z)\\\\=&amp; \\delta_{ij} + F_{ij}(z)P_{jj}(z)\\end{align}\\]if \\(i=j\\)\\[P_{ii}(z) = 1 + F_{ii}(z)P_{ii}(z)\\]\\[P_{ii}(z) = \\dfrac{1}{1-F_{ii}(z)}\\]thus criterion for recurrent:  state \\(i\\) is recurrent \\(\\iff \\sum_{k=0}^{\\infty} P_{ii}(k) = \\infty\\)  state \\(i\\) is non-recurrent \\(\\iff \\sum_{k=0}^{\\infty} P_{ii}(k) &lt; \\infty\\) If \\(i \\leftrightarrow j\\), then \\(i, j\\) are both recurrent or non-recurrent.\\[\\exists m, n, \\quad P_{ij}(m) &gt; 0, P_{ji}(n) &gt; 0\\]\\[\\begin{align}&amp;\\sum_{k=0}^{\\infty} P_{ii}(k) \\ge \\sum_{k=0}^{\\infty} P_{ii}(m+n+k)\\\\\\ge &amp; \\sum_{k=0}^{\\infty} P_{ij}(m) P_{jj}(k) P_{ji}(n)\\\\\\ge &amp; P_{ij}(m) \\Big( \\sum_{k=0}^{\\infty} P_{jj}(k) \\Big) P_{ji}(n)\\end{align}\\] If \\(i\\) is non-recurrent, \\(P_{ii}(n) \\to 0 (n \\to \\infty)\\). If \\(i\\) is non-recurrent, \\(\\forall k , P_{ki}(n) \\to 0 (n \\to \\infty)\\). This is because\\[P_{ki}(z) = \\delta_{ki} + F_{ki}(z)P_{ii}(z)\\]and for non-recurrent \\(i\\), both \\(F_{ki}(0)\\) and \\(P_{ii}(0)\\) converges. If finite states, there must be recurrent state.This is because the summation of row is 1: \\(\\sum_{j} P_{ij}(n) = 1\\), for finite states, it is impossible for all \\(i\\) to be non-recurrent states.\\[\\lim_{n\\to \\infty} \\sum_{j}^{N} P_{ij}(n) = 0 = 1\\] Finite states + irreducible: all states are recurrent.This is because if it is irreducible, all states are commutatable.Example: random walk with equal probability. For 1-d and 2-d, all the states are recurrent; for 3-d and more, all the states are non-recurrent.   If \\(i\\) is recurrent, and \\(i \\to j\\), then \\(j \\to i\\). Thus \\(j\\) is recurent states. Recurrent states only goes to recurrent states.To prove that, we first define the probability, starting from \\(i\\), then come back to \\(i\\) at least \\(m\\) times\\[g_{ii}(m) = P(\\#\\{k: X_k=i, k \\ge 1\\} \\ge m \\vert X_0 = i)\\]we want to find how to derive \\(g_{ii}(m)\\) from \\(g_{ii}(m-1)\\)\\[g_{ii}(m) = \\sum_{k=1}^{\\infty} f_{ii}(k) g_{ii}(m-1) = g_{ii}(m-1) \\Big( \\sum_{k=1}^{\\infty} f_{ii}(k) \\Big)\\]\\[g_{ii}(1) = F_{ii}(0) = f_{ii}\\]\\[g_{ii}(m) = (f_{ii})^m\\]\\[g_{ii} = \\lim_{m \\to \\infty} g_{ii}(m) =\\begin{cases}1&amp; \\quad i \\text{ recurrent }\\\\0&amp; \\quad i \\text{ non-recurrent }\\end{cases}\\]We define\\[g_{ij}(m) = P(\\#\\{k: X_k=j, k \\ge 1\\} \\ge m \\vert X_0 = i)\\] If \\(i\\) is recurrent, and \\(\\exists m \\ge 1, P_{ij}(m) &gt; 0\\), then \\(\\lim_{n \\to \\infty} g_{ji}(n) = 1\\)\\[1 = g_{ii} = \\sum_{j} P_{ij}(m) g_{ji}\\]\\[1 = \\sum_{j} P_{ij}\\]\\[0 = \\sum_{j} P_{ij}(m)(g_{ji} - 1)\\]We not only know \\(j \\to i\\), we even know \\(\\lim_{n\\to \\infty} g_{ji} = 1\\)" }, { "title": "Markov Chain", "url": "/posts/markov-chain/", "categories": "math", "tags": "math", "date": "2023-05-10 07:00:00 +0000", "snippet": "From Hao Zhang’s Lecture 13-18Markov ChainMarkov property:We discuss Distrete time, discrete states random process\\[\\{X_n\\}_{n=0}^{\\infty}, \\quad X_k \\in S = \\{x_1, x_2, \\dots \\} \\text{ (finite or countably infinite) }\\]Markov property assumes\\[P(X_n = x_n \\vert X_{n-1} = x_{n-1}, \\dots, X_0 = x_0) = P(X_{n}=x_n \\vert X_{n-1}=x_{n-1})\\]then\\[\\begin{align}&amp;P(X_n=x_n, \\dots, X_0=x_0) = P(X_n=x_n \\vert X_{n-1}=x_{n-1}, \\dots, X_0=x_0) P(X_{n-1}=x_{n-1}, \\dots, X_0=x_0)\\\\=&amp; P(X_n=x_n \\vert X_{n-1}=x_{n-1}, \\dots, X_0=x_0) P(X_{n-1}=x_{n-1} \\vert X_{n-2}=x_{n-2}, \\dots, X_0=x_{0}) P(X_{n-2}=x_{n-1}, \\dots, X_0=x_0)\\\\=&amp; \\Big( \\prod_{k=1}^n P(X_k=x_k \\vert X_{k-1}=x_{k-1}, \\dots, X_0=x_0) \\Big) P(X_0 = x_0)\\\\=&amp; \\Big(\\prod_{k=1}^n P(X_k = x_k \\vert X_{k-1} = x_{k-1})\\Big) P(X_0=x_0)\\end{align}\\]" }, { "title": "Stochastic Process 14", "url": "/posts/stochastic-process-14/", "categories": "math", "tags": "math", "date": "2023-05-10 06:00:00 +0000", "snippet": "From Hao Zhang’s Lecture 14From last time Markov property:  \\(A: \\text{ past } \\quad B: \\text{ Now } \\quad C: \\text{ Future }\\)\\[P(C \\vert BA) = P(C \\vert B) \\iff P(CA \\vert B) = P(C \\vert B) P(A \\vert B)\\]We discuss Distrete time, discrete states random process\\[\\{X_n\\}_{n=0}^{\\infty}, \\quad X_k \\in S = \\{x_1, x_2, \\dots \\} \\text{ (finite or countably infinite) }\\]Markov property assumes\\[P(X_n = x_n \\vert X_{n-1} = x_{n-1}, \\dots, X_0 = x_0) = P(X_{n}=x_n \\vert X_{n-1}=x_{n-1})\\]then\\[\\begin{align}&amp;P(X_n=x_n, \\dots, X_0=x_0) = P(X_n=x_n \\vert X_{n-1}=x_{n-1}, \\dots, X_0=x_0) P(X_{n-1}=x_{n-1}, \\dots, X_0=x_0)\\\\=&amp; P(X_n=x_n \\vert X_{n-1}=x_{n-1}, \\dots, X_0=x_0) P(X_{n-1}=x_{n-1} \\vert X_{n-2}=x_{n-2}, \\dots, X_0=x_{0}) P(X_{n-2}=x_{n-1}, \\dots, X_0=x_0)\\\\=&amp; \\Big( \\prod_{k=1}^n P(X_k=x_k \\vert X_{k-1}=x_{k-1}, \\dots, X_0=x_0) \\Big) P(X_0 = x_0)\\\\=&amp; \\Big(\\prod_{k=1}^n P(X_k = x_k \\vert X_{k-1} = x_{k-1})\\Big) P(X_0=x_0)\\end{align}\\]The transition probability \\(P(X_n = x_n \\vert X_{n-1} = X_{n-1})\\) is important, more general, for \\(n &gt; m\\)\\[P(X_n = x_n \\vert X_m = x_{m}) = P(X_n = j \\vert X_m = i) = P_{ij}(n, m)\\]It can be understood as, transit from state \\(i\\) to \\(j\\), from time \\(m\\) to time \\(n\\).Stationary AssumptionWe assume \\(\\forall n &gt; m \\ge 0\\)\\[P_{i,j}(n,m) = P_{i,j}(n-m)\\]Chapman-Kolmogorov Equation (C-K)\\[\\forall m &lt; n, \\quad P_{ij}(n) = \\sum_{k} P_{ik}(m)P_{kj}(n-m)\\]Proof:\\[\\begin{align}P_{ij}(n) &amp;= P(X_n = j \\vert X_0 = i)\\\\&amp;= \\sum_{k} P(X_n = j, X_m = k \\vert X_0 = i)\\\\&amp;= \\sum_{k} P(X_n = j \\vert X_m = k, X_0 = i) P(X_m=k \\vert X_0 = i)\\\\&amp;= \\sum_{k} P(X_n = j \\vert X_m = k) P(X_m = k \\vert X_0 = i)\\\\&amp;= \\sum_{k} P_{kj}(n-m) P_{ik}(m)\\end{align}\\]Matrix Form\\[P(n) = P(m)P(n-m) = (P(1))^n\\]To diagnolize such general matrix, we need to use Jordan canonical form.Qualitative Understanding Reachable: \\(i \\to j: \\quad \\exists n, P_{ij}(n) &gt; 0\\). Commutatable: \\(i \\leftrightarrow j \\quad \\iff \\quad i \\to j, j \\to i\\) Closed Set: let \\(S\\) be the set of all states. \\(C \\subseteq S\\) is closed set \\(\\iff\\) \\(i \\in C, j \\not\\in C \\implies i \\not\\to j\\). After it goes into \\(C\\), it will never left \\(C\\). Recude \\(S\\) to \\(C\\) is called reduction. It is a complete Markov chain in \\(C\\). However, it is possible for \\(S\\) to have several non-overlaping closed sets. Irreducible \\(S\\): no closed true subsets.Theorem: Irreducible \\(\\iff \\forall i, j, i \\leftrightarrow j\\).  \\(\\Leftarrow\\) is trival.To prove \\(\\implies\\). We define \\(\\forall i, A_i = \\{j: i \\to j\\}\\). First we prove \\(A_i\\) is closed. If this is true, for irreducible \\(S\\), we know any \\(A_i\\) should be \\(S\\), thus any two states are commutatable.If \\(A_i\\) is not closed, then \\(\\exists j \\in A_i, k \\not\\in A_i\\) such that \\(j \\to k\\). Then we know \\(i \\to j, j \\to k\\), thus \\(i \\to k\\), thus \\(k \\in A_i\\), we have the contracdiction.\\[P(1) =\\begin{pmatrix}P_{11} &amp; \\dots &amp; P_{1n}\\\\\\dots &amp; \\dots &amp; \\dots\\\\P_{n1} &amp; \\dots &amp; p_{nn}\\end{pmatrix}\\]If it is reductable, we can convert \\(P(1)\\) by interchange rows and columns (by label states by different numbers)\\[\\to\\begin{pmatrix}A &amp; B\\\\0 &amp; C\\end{pmatrix}\\]\\[P(1)^n =\\begin{pmatrix}* &amp; *\\\\0 &amp; C^n\\end{pmatrix}\\]Then similarly, \\(C\\) may be convert to the form if \\(C\\) is reductable.Currently people didn’t have easy ways to determine if \\(P\\) is reductable or not, we need to traverse all the possible interchanging.First Passage First passage: \\(f_{ij}(n) = P(X_n=j, X_{n-1}\\ne j, X_{n-2}\\ne j, \\dots, X_1\\ne j \\vert X_0 = i)\\)We have\\[P_{ij}(n) = \\sum_{k=1}^n f_{ij}(k) P_{jj}(n-k)\\]To prove it we first define first passage time: \\(T = \\min_{r} \\{X_r = j \\vert X_0 = i\\}\\). \\(T = k\\) means \\(\\{X_k=j, X_{k-1} \\ne j, \\dots X_{1} \\ne j \\vert X_0=i\\}\\).\\[\\begin{align}P_{ij}(n) &amp;= P(X_n=j \\vert X_0 = i)\\\\&amp;= \\sum_{k=1}^{n} P(X_n=j, T=k \\vert X_0=i)\\\\&amp;= \\sum_{k=1}^{n} P(X_n=j \\vert T=k, X_0=i) P(T=k \\vert X_0=i)\\\\&amp;= \\sum_{k=1}^{n} P(X_n=j \\vert X_k=j, X_{k-1} \\ne j, \\dots, X_{1} \\ne j , X_0 = i) P(X_k=j, X_{k-1}\\ne j, \\dots, X_{1}\\ne j \\vert X_0 = i)\\\\&amp;= \\sum_{k=1}^n P(X_n=j \\vert X_k = j) f_{ij}(k)\\\\&amp;= \\sum_{k=1}^n f_{ij}(k)P_{jj}(n-k)\\end{align}\\]" }, { "title": "Stochastic Process 13", "url": "/posts/stochastic-process-13/", "categories": "math", "tags": "math", "date": "2023-05-10 05:00:00 +0000", "snippet": "From Hao Zhang’s Lecture 131 hr 33 minMarkov PropertyWe know\\[\\begin{align}&amp;P(X_n=x_n, \\dots, X_0=x_0) = P(X_n=x_n \\vert X_{n-1}=x_{n-1}, \\dots, X_0=x_0) P(X_{n-1}=x_{n-1}, \\dots, X_0=x_0)\\\\=&amp; P(X_n=x_n \\vert X_{n-1}=x_{n-1}, \\dots, X_0=x_0) P(X_{n-1}=x_{n-1} \\vert X_{n-2}=x_{n-2}, \\dots, X_0=x_{0}) P(X_{n-2}=x_{n-1}, \\dots, X_0=x_0)\\\\=&amp; \\Big( \\prod_{k=1}^n P(X_k=x_k \\vert X_{k-1}=x_{k-1}, \\dots, X_0=x_0) \\Big) P(X_0 = x_0)\\end{align}\\]If we assume\\[P(X_n = x_n \\vert X_{n-1} = x_{n-1}, \\dots, X_0 = x_0) = P(X_{n}=x_n \\vert X_{n-1}=x_{n-1})\\]\\[\\begin{align}P(X_n=x_n, \\dots, X_0=x_0) = \\Big(\\prod_{k=1}^n P(X_k = x_k \\vert X_{k-1} = x_{k-1})\\Big) P(X_0=x_0)\\end{align}\\]Random process can be catagorized as discrete time or continuous time; discrete states or continuous statesFirst we discuss discrete-time Markov chain (discrete-time and discrete states)Formally, Markov property can be state as\\[\\forall n \\in \\mathbb{N} \\quad P(X_n=x_n \\vert X_{n-1}=x_{n-1}, \\dots, X_1 = x_1, X_0 = x_0) = P(X_{n}=x_n \\vert X_{n-1}=x_{n-1})\\]If we note \\(A, B, C\\) to be past, current, future, then\\[\\begin{align}A &amp;= X_{n-2}, \\dots X_{1}\\\\B &amp;= X_{n-1}\\\\C &amp;= X_{n}\\end{align}\\]Then Markov property is\\[P(C \\vert B A) = P(C \\vert B)\\]It is equivalent to\\[P(CA \\vert B) = P(C \\vert B) P(A \\vert B)\\]with the interpretation of when current is known, past and future are independent.\\[[P(C \\vert B A) = P(C \\vert B)] \\implies P(CA \\vert B) = P(C \\vert B) P(A \\vert B)\\]\\[\\begin{align}&amp;P(C A \\vert B) = \\dfrac{P(CAB)}{P(B)} = \\dfrac{P(CAB)}{P(AB)} \\dfrac{P(AB)}{P(B)}\\\\=&amp; \\dfrac{P(CBA)}{P(BA)} \\dfrac{P(AB)}{P(B)} = P(C \\vert BA) P(A \\vert B)\\\\=&amp; P(C \\vert B) P(A \\vert B)\\end{align}\\]\\[[P(CA \\vert B) = P(C \\vert B) P(A \\vert B)] \\implies [P(C \\vert B A) = P(C \\vert B)]\\]\\[\\begin{align}&amp;P(C \\vert B A) = \\dfrac{P(CBA)}{P(BA)} = \\dfrac{P(CAB)}{P(B)} \\dfrac{P(B)}{P(BA)}\\\\=&amp; P(CA \\vert B) / P(A \\vert B) = P(C \\vert B)\\end{align}\\]Other EquationsObviously\\[P(X_n \\in A \\vert X_{n-1}=x_{n-1}, \\dots, X_1 = x_1) = P(X_n \\in A \\vert X_{n-1}=x_{n-1})\\]\\[\\sum_{x_n \\in A} P(X_n = x_n \\vert X_{n-1}=x_{n-1}, \\dots, X_{1} = X_1) = \\sum_{x_n \\in A} P(X_n = x_n \\vert X_{n-1}=x_{n-1}) = P(X_n \\in A \\vert X_{n-1}=x_{n-1})\\]We also have\\[P(X_{n}=x_n \\vert X_{n-1}=x_{n-1}, X_{n-2}\\in S_{n-2}, \\dots, X_{1} \\in S_1) = P(X_n = x_n \\vert X_{n-1}=x_{n-1})\\]using the equivalence of \\(P(C \\vert BA) = P(C \\vert B) \\iff P(CA \\vert B) = P(C \\vert B) P(A \\vert B)\\), we only need to show\\[P(X_n = x_n, X_{n-2}\\in S_{n-2}, \\dots X_{1} \\in S_1 \\vert X_{n-1} = x_{n-1}) = P(X_n=x_n \\vert X_{n-1}=x_{n-1} ) P(X_{n-2} \\in S_{n-2} \\dots X_{1} \\in S_1 \\vert X_{n-1}=x_{n-1})\\]The importance thing is we must exactly know \\(X_{n-1}=x_{n-1}\\), this cannot be a set." }, { "title": "Stochastic Process 09", "url": "/posts/stochastic-process-09/", "categories": "math", "tags": "math", "date": "2023-05-07 05:00:00 +0000", "snippet": "From Hao Zhang’s Lecture 09" }, { "title": "Calculus 002", "url": "/posts/calculus-002/", "categories": "math", "tags": "math", "date": "2023-05-06 05:00:00 +0000", "snippet": "From Zhenyu Qi’ Lecture 002Upper and Lower BoundDef: Let \\(S \\subseteq \\mathbb{R}\\) and \\(r \\in \\mathbb{R}\\). We say that(1.1) \\(r\\) is an upper bound of \\(S\\) : \\(\\quad \\forall s \\in S \\, [s \\le r]\\)(1.2) \\(r\\) is an lower bound of \\(S\\) : \\(\\quad \\forall s \\in S \\, [s \\ge r]\\)(2) \\(r\\) is the greatest element of \\(S\\) (notation \\(\\max S\\)):   1. \\(r\\) is an upper bound of \\(S\\).   2. \\(r \\in S\\) (3.1) \\(r\\) is the least upper bound of \\(S\\) (notation \\(\\sup S\\)): \\(\\quad r = \\min \\{\\text{ upper bounds of } S\\}\\)(3.2) \\(r\\) is the greatest lower bound of \\(S\\) (notation \\(\\inf S\\)): \\(\\quad r = \\max \\{\\text{ lower bounds of } S\\}\\)We have Any \\(u &lt; \\sup S\\), \\(u\\) is not upper bound of \\(S\\). Any \\(u &gt; \\inf S\\), \\(u\\) is not lower bound of \\(S\\). Also, every \\(r \\in \\mathbb{R}\\) is a upper and lower bound of \\(\\emptyset\\).Convention: We write \\(\\sup S = \\infty\\) if and only if \\(S\\) has no upper bound.If this is the case we say \\(\\sup S\\) doesn’t exist. We way \\(S\\) is bounded from above iff \\(S\\) has a upper bound. Dedekind CutIf we split the set \\(\\mathbb{R}\\) to two non-empty set, lower part \\(A\\) and upper part \\(B\\).Then either \\(\\max A\\) exists or \\(\\min B\\) exists.Def (Dedekind Cut): Let \\(A, B \\subseteq \\mathbb{R}\\), we say that \\((A,B)\\) is a Dedekind cut (of \\(\\mathbb{R}\\)) if   \\(A \\ne \\emptyset \\ne B\\)   \\(A \\cup B = \\mathbb{R}\\)   \\(\\forall a \\in A, b \\in B, [a &lt; b]\\) We usually call \\(A\\) (resp. \\(B\\)) the lower (resp. upper) part of \\((A,B)\\).From now on (until he say stop) we assume that \\(\\mathbb{R}\\) has the following property:(Dedekind’s gapless property) If \\((A,B)\\) is a D-cut of \\(\\mathbb{R}\\), then exactly one of the following happens:   \\(\\max A\\) exists but \\(\\min B\\) doesn’t.   \\(\\min B\\) exists but \\(\\max A\\) doesn’t Ex 1: We may define Dedekind cuts of \\(\\mathbb{Q}\\) similarly.\\[B = \\{x \\in \\mathbb{Q} \\vert x &gt; 0 \\land x^2 &gt; 2\\}, A = Q \\backslash B\\]We can verify it is D-cut.We can prove \\(\\max A, \\min B\\) doesn’t exists.If \\(\\min B\\) exists, let \\(\\min B = p\\).\\[\\min B = p \\implies p \\in B \\implies p &gt; 0 \\land p^2 &gt; 2\\]Let \\(q = p - \\dfrac{p^2-2}{2p} \\implies 0 &lt; q &lt; p\\)\\[q^2 = \\Big( p - \\dfrac{p^2-2}{2p}\\Big)^2 = 2 + \\Big(\\dfrac{p^2-2}{2p}\\Big)^2 &gt; 2\\]\\[q &gt; 0 \\land q^2 &gt; 2 \\land q \\in \\mathbb{Q} \\implies q \\in B\\]contradicts with \\(q &lt; p = \\min B\\)If \\(\\max A\\) exists, let \\(\\max A = p\\)\\[\\max A = p \\implies p \\in A \\implies p^2 &lt; 2\\]\\[1 \\in A \\implies p \\ge 1 &gt; 0\\]Let \\(h = \\min\\Big( \\dfrac{1}{2}, \\dfrac{2-p^2}{2p+2}\\Big)\\)\\[h \\le \\dfrac{1}{2} &lt; 1\\]\\[h \\le \\dfrac{2-p^2}{2p+2} &lt; \\dfrac{2-p^2}{2p+1}\\]\\[(p+h)^2 = p^2 + (2p+h)h &lt; p^2 + (2p+1)h &lt; p^2 + (2-p^2) &lt; 2\\]\\[p + h \\in Q \\land (p+h)^2 &lt; 2 \\implies p+h \\in A\\]contraditcs with \\(p+h &gt; p = \\max A\\)WeierstrassTheorem (Weierstrass): Let \\(\\emptyset \\ne S \\subseteq \\mathbb{R}\\). If \\(S\\) has a upper bound, then \\(\\sup S\\) exists.Proof:\\[\\sup S = \\min \\{\\text{upper bounds of } S\\}\\]The idea is to do a Dedekind’s cut \\(( \\mathbb{R} \\backslash \\{\\text{ upper bounds of } S\\} ,\\{\\text{upper bounds of } S \\})\\), then we prove the other case is impossible.Let \\(B := \\{b \\in \\mathbb{R} \\vert b \\text{ is a upper bound of } S\\}, A:= \\mathbb{R} \\backslash B\\)We need to show that \\(\\min B\\) exists.Step 1: \\((A,B)\\) is a D-cut of \\(\\mathbb{R}\\)   \\(A \\ne \\emptyset \\ne B\\)\\[S \\ne \\emptyset \\implies s \\in S \\implies s-1 \\not\\in B \\implies s - 1 \\in A \\implies A \\ne \\emptyset\\]\\[S \\text{ has a upper bound } \\implies B \\ne \\emptyset\\]   \\(A \\cup B = \\mathbb{R}\\)   \\(\\forall a \\in A, b \\in B, [a &lt; b]\\). Were this false, \\(a \\ge b\\)\\[b \\in B \\implies a \\in B \\implies a \\in A \\cap B \\implies a \\in \\emptyset\\]Step 2: We need to show that \\(\\max A\\) doesn’t exists. Were this false.\\[\\max A = a_0 \\implies a_0 \\in A \\implies a_0 \\text{ is not an upper bound of} S \\implies \\exists s_0 \\in S \\text{ s.t. } s_0 &gt; a_0\\]Let \\(h = \\dfrac{a_0 + s_0}{2}, a_0 &lt; h &lt; s_0\\)\\[h &lt; s_0 \\implies h \\text{ is not an upper bound of } S \\implies h \\in A\\]contracts with \\(h &gt; a_0 = \\max A\\)The Archimedean PropertyEx2: prove the following statement(The Archimedean property) \\(\\forall r \\in \\mathbb{R} [r &gt; 0 \\implies \\exists n \\in N [\\frac{1}{n} &lt; r]]\\)Step 1: We need to show \\(\\mathbb{N}\\) doesn’t have upper bound. Were this false, \\(\\mathbb{N}\\) has upper bound, then from Weierstrass \\(\\sup \\mathbb{N}\\) exists, \\(b = \\sup \\mathbb{N} = \\min \\{\\text{ uppber bound of } \\mathbb{N}\\}\\).\\[\\forall n \\in \\mathbb{N}, [n+1 \\in \\mathbb{N}]\\]\\[n+1 \\in \\mathbb{N} \\implies n + 1 \\le b \\implies n \\le b - 1\\]\\[\\forall n \\in \\mathbb{N}, [n \\le b - 1]\\]thus \\(b - 1\\) is an uppber bound of \\(\\mathbb{N}\\), \\(b - 1 \\in \\{ \\text{ upper bound of } \\mathbb{N}\\}\\) contracdicts with\\[b - 1 &lt; b = \\min \\{ \\text{ uppber bound of } \\mathbb{N} \\}\\]Step 2: Were Archimedean property false\\[\\exists r \\in \\mathbb{R} \\land r &gt; 0 [\\forall n \\in \\mathbb{N} [\\frac{1}{n} \\ge r]]\\]thus\\[\\exists r \\in \\mathbb{R} \\land r &gt; 0 [\\forall n \\in \\mathbb{N} [n \\le \\frac{1}{r}]]\\]thus \\(1/r\\) is an upper bound of \\(\\mathbb{N}\\)Limit of SequenceDef: Let \\(a_n (n \\in \\mathbb{N})\\) be a sequence in \\(\\mathbb{R}\\), and \\(L \\in \\mathbb{R}\\).We say that \\(a_n\\) converges to \\(L\\) (as \\(n \\to \\infty\\)) if\\[\\forall \\varepsilon &gt; 0 \\quad \\exists N \\in \\mathbb{N} \\quad [n \\ge N \\implies \\vert a_n - L \\vert &lt; \\varepsilon]\\]Terminology: If such \\(L\\) exists (doesn’t exists), we call it the limit of \\(a_n\\) and we call \\(a_n\\) a convergent (divergent) sequence. the \\(n \\ge N\\) can be freely changed to \\(n &gt; N\\). the \\(\\vert a_n - L \\vert &lt; \\varepsilon\\) can be freely changed to \\(\\le\\) and \\(K \\varepsilon\\), where \\(K \\in \\mathbb{R}\\) is a constant not depending on \\(\\varepsilon, N\\). Notation: \\(\\lim_{n \\to \\infty} a_n = L\\)Some generalized notations: \\(\\lim_{n\\to\\infty} a_n = \\infty \\quad \\iff \\quad\\) \\(\\forall M \\in \\mathbb{R}\\, \\exists n \\in \\mathbb{N} \\, [n \\ge N \\implies a_n \\ge M]\\), similarly for \\(-\\infty\\).In this case we usually doesn’t say the limit exists or converge.Ex3: (1) \\(\\lim_{n \\to \\infty} a_n = L, \\lim_{n \\to \\infty} a_n = M \\quad \\implies \\quad L = M\\)wlog, assume \\(M &gt; L\\)\\[\\exists N_1 \\in \\mathbb{N} \\, [n \\ge N_1 \\implies L - \\dfrac{M - L}{2} &lt; a_n &lt; L + \\dfrac{M-L}{2}]\\]\\[\\exists N_2 \\in \\mathbb{N} \\, [n \\ge N_2 \\implies M - \\dfrac{M - L}{2} &lt; a_n &lt; M + \\dfrac{M-L}{2}]\\]\\[n \\ge \\max(N_1, N_2) \\implies \\dfrac{M+L}{2} &lt; a_n &lt; \\dfrac{M+L}{2}\\]we have the contracdiction.(2) \\(a_n (n \\in \\mathbb{N})\\) is convergent \\(\\implies \\{a_n \\vert n \\in \\mathbb{N}\\}\\) is bounded.\\[\\exists N \\in \\mathbb{N}, [n \\ge N \\implies L - 1 &lt; a_n &lt; L + 1]\\]let \\(L_1 = \\max \\{a_n \\vert n \\in \\mathbb{N}, n &lt; N\\}\\), then\\[\\forall n \\in \\mathbb{N} \\, [a_n \\le \\max(L+1, L_1)]\\]lower bound is similar.(3) If \\(\\forall n \\in \\mathbb{N} a_n \\le b_n, \\lim_{n\\to\\infty}a_n = L, \\lim_{n\\to\\infty}b_n=M\\), then \\(L \\le M\\). What if \\(\\le\\) is replaced by $&lt;$?Were this false, \\(L &gt; M\\). We can find \\(N_a, N_b \\in \\mathbb{N}\\) such that \\(a_n, b_n\\) all lies in the nonoverlaping region. Then we find \\(n \\ge \\max(N_a, N_b) \\implies a_n &gt; b_n\\).If \\(\\le\\) is replaced by \\(&lt;\\), we still only have the conclusion \\(L \\le M\\). Assume the examlpe \\(a_n = \\dfrac{1}{n+1}\\), \\(b_n = \\dfrac{1}{n}\\). By Archimedean property, limit is \\(L=M=0\\).Remark: changing or removing finitely many terms in \\(a_n\\) doesn’t affect the existance of limit or the value of the limit.Elementary Arithmetic on LimitIf \\(\\lim_{n\\to\\infty} a_n = L\\) and \\(\\lim_{n\\to\\infty} b_n = M\\), then(1) \\(\\lim_{n\\to\\infty} (a_n \\pm b_n) = L \\pm M\\)(2) \\(\\lim_{n\\to\\infty} a_n b_n = L \\cdot M\\)(3) if \\(M \\ne 0\\), then \\(b_n \\ne 0\\) for all but finitely many \\(n\\), and \\(\\lim_{n\\to\\infty} \\dfrac{a_n}{b_n} = \\dfrac{L}{M}\\)" }, { "title": "Stochastic Process 08", "url": "/posts/stochastic-process-08/", "categories": "math", "tags": "math", "date": "2023-05-04 10:00:00 +0000", "snippet": "From Hao Zhang’s Lecture 08Nonlinear vs. GaussianOne Random VariableFor \\(Y \\sim N(0, \\sigma^2)\\), let’s calculate \\(E(Y^n)\\)\\[E(Y^n) =\\begin{cases}0, \\quad n = 2k-1\\\\(2k-1)!! \\sigma^{2k}, \\quad n = 2k\\end{cases}\\]\\[\\begin{align}&amp;\\int_{-\\infty}^{\\infty} y^{2k} \\exp\\big(-\\dfrac{y^2}{2\\sigma^2} \\big) dy\\\\=&amp; -\\sigma^2 \\int_{-\\infty}^{\\infty} y^{2k-1} d\\Big( \\exp\\big( - \\dfrac{y^2}{2\\sigma^2}\\big) \\Big)\\\\=&amp; -\\sigma^2 y^{2k-1} \\exp \\big( -\\dfrac{y^2}{2\\sigma^2} \\big) \\Big\\vert_{-\\infty}^{\\infty} + (2k-1)\\sigma^2 \\int_{-\\infty}^{\\infty} \\exp\\big(-\\dfrac{y^2}{2\\sigma^2} \\big) y^{2k-2} dy\\\\=&amp; (2k-1)\\sigma^2 \\int_{-\\infty}^{\\infty} \\exp\\big(-\\dfrac{y^2}{2\\sigma^2} \\big) y^{2k-2} dy\\end{align}\\]Thus\\[E(Y^{2k}) = (2k-1)\\sigma^2 E(Y^{2k-2})\\]Thus\\[E(Y^{2k}) = (2k-1)!! \\sigma^{2k}\\]where\\[(2k-1)!! = (2k-1) (2k-3) \\dots 1\\]\\[\\begin{align}E(Y^2) &amp;= \\sigma^2\\\\E(Y^4) &amp;= 3 \\sigma^4\\\\E(Y^6) &amp;= 15 \\sigma^6\\end{align}\\]Multiple Random Variables\\[(X_1, X_2, X_3, X_4) \\sim N(0, \\Sigma), X \\in \\mathbb{R}\\]then\\[E(X_1 X_2 X_3 X_4) = E(X_1 X_2) E(X_3 X_4) + E(X_1 X_3) E(X_2 X_4) + E(X_1 X_4) E(X_2 X_3)\\]This is for real variables.For complex variables it is slightly different.To prove this we use characteristic function\\[E(X_1^{\\alpha_{1}} \\dots X_n^{\\alpha_n}) = \\dfrac{1}{j^{\\alpha_1 + \\dots + \\alpha_n}} \\frac{\\partial^{\\alpha_1 + \\dots + \\alpha_n}}{\\partial \\omega_1^{\\alpha_1} \\dots \\partial \\omega_n^{\\alpha_n}} \\Phi_{X_1, \\dots, X_n}(\\omega_1, \\dots, \\omega_n) \\Big\\vert_{\\omega_1=\\dots=\\omega_n = 0}\\]this is because\\[\\Phi_{X_1,\\dots,X_n}(\\omega_1, \\dots, \\omega_n) = E(\\exp(j(\\omega_1 X_1 + \\dots + \\omega_n X_n)))\\]If we have 5 random variables with \\(N(0, \\Sigma)\\), we will have\\[E(X_1 X_2 X_3 X_4 X_5) = 0\\]If we have 6 random variables with \\(N(0, \\Sigma)\\), we will have 15 terms, as in the one random variable cases.Square DeviceFor W.S.S. Gaussian process \\(X(t)\\), if \\(Y(t) = (X(t))^2\\). How to compute \\(R_Y(t,s)\\)?\\[R_Y(t,s) = E(Y(t)Y(s))= E(X^2(t)X^2(s))\\]Since we know \\((X(t), X(t), X(s), X(s))\\) is Gaussian\\[\\begin{align}R_Y(t,s) &amp;= E(X(t)X(t))E(X(s)X(s)) + E(X(t)X(s))E(X(t)X(s)) + E(X(t)X(s)) + E(X(t)X(s))\\\\&amp;= (R_X(0))^2 + 2 (R_X(t-s))^2\\end{align}\\]Hard Limiter\\[g(x) = \\mathrm{sgn}(x) =\\begin{cases}1, \\quad x &gt; 0\\\\0, \\quad x = 0\\\\-1, \\quad x &lt; 0\\end{cases}\\]We want to calculate \\(R_Y(t,s)\\) for \\(Y = g(X)\\), assuming \\(X(t)\\) is a Gaussian process with zero mean.\\[\\begin{align}R_Y(t,s) &amp;= E(Y(t)Y(s)) = E(g(X(t)) \\cdot g(X(s)))\\\\&amp;= 1 \\cdot P(X(t)X(s) &gt; 0) + (-1) \\cdot P(X(t)X(s)) &lt; 0\\end{align}\\]we only need to calculate \\(P(X(t)X(s) &gt; 0)\\)\\[\\begin{align}&amp;\\Big( \\int_{0}^{\\infty}\\int_{0}^{\\infty} + \\int_{-\\infty}^{0} \\int_{-\\infty}^{0} \\Big) f(x_1, x_2) d x_1 d x_2\\\\=&amp; \\dfrac{1}{2\\pi \\sigma_1 \\sigma_2 \\sqrt{1 - \\rho^2}} \\Big( \\int_{0}^{\\infty}\\int_{0}^{\\infty} + \\int_{-\\infty}^{0} \\int_{-\\infty}^{0} \\Big) \\exp \\bigg( -\\dfrac{1}{2(1-\\rho^2)} \\Big(\\big(\\dfrac{x_1}{\\sigma_1}\\big)^2 + \\big(\\dfrac{x_2 }{\\sigma_2}\\big)^2 - 2\\rho \\big( \\dfrac{x_1}{\\sigma_1} \\big) \\big( \\dfrac{x_2}{\\sigma_2} \\big) \\Big) \\bigg) d x_1 d x_2\\\\=&amp; \\dfrac{1}{\\pi \\sigma_1 \\sigma_2 \\sqrt{1 - \\rho^2}} \\int_{0}^{\\infty}\\int_{0}^{\\infty} \\exp \\bigg( -\\dfrac{1}{2(1-\\rho^2)} \\Big(\\big(\\dfrac{x_1}{\\sigma_1}\\big)^2 + \\big(\\dfrac{x_2 }{\\sigma_2}\\big)^2 - 2\\rho \\big( \\dfrac{x_1}{\\sigma_1} \\big) \\big( \\dfrac{x_2}{\\sigma_2} \\big) \\Big) \\bigg) d x_1 d x_2\\end{align}\\]Let \\(y_1 = x_1/\\sigma_1, y_2 = x_2/\\sigma_2\\)\\[\\dfrac{1}{\\pi \\sqrt{1 - \\rho^2}} \\int_{0}^{\\infty}\\int_{0}^{\\infty} \\exp \\Big( -\\dfrac{1}{2(1-\\rho^2)} (y_1^2 + y_2^2 - 2\\rho y_1 y_2 ) \\Big) d y_1 d y_2\\]Let \\(y_1 = u_1 + u_2, y_2 = u_1 - u_2\\)\\[\\begin{align}&amp;(u_1 + u_2)^2 + (u_1 - u_2)^2 - 2 \\rho (u_1 + u_2)(u_1 - u_2)\\\\=&amp; 2 (u_1^2 + u_2^2) - 2 \\rho(u_1^2 - u_2^2)\\end{align}\\]\\[d y_1 d y_2 = \\big \\vert \\mathrm{det} \\dfrac{\\partial y}{\\partial u} \\big\\vert d u_1 d u_2 = 2 d u_1 d u_2\\]integration region becomes a right angle.\\[\\begin{align}&amp;\\dfrac{2}{\\pi \\sqrt{1 - \\rho^2}} \\int\\int \\exp \\Big( -\\dfrac{1}{2(1-\\rho^2)} (2(u_1^2 + u_2^2) - 2\\rho (u_1^2 - u_2^2)) \\Big) d u_1 d u_2\\\\=&amp;\\dfrac{2}{\\pi \\sqrt{1 - \\rho^2}} \\int\\int \\exp \\Big(- \\big( \\dfrac{u_1^2}{1+\\rho} + \\dfrac{u_2^2}{1-\\rho}\\big) \\Big) d u_1 d u_2\\end{align}\\]Let \\(u_1' = u_1/\\sqrt{1+\\rho}, u_2' = u_2/\\sqrt{1-\\rho}\\)\\[d u_1 d u_2 = \\sqrt{1-\\rho^2} d u_1' d u_2'\\]integration region becomes a slightly modified angle, with points \\((1/\\sqrt{1+\\rho},1/\\sqrt{1-\\rho}), (1/\\sqrt{1+\\rho}, -1/\\sqrt{1-\\rho})\\)\\[\\dfrac{2}{\\pi} \\int\\int \\exp \\big(- (u_1'^2 + u_2'^2) \\big) d u_1' d u_2'\\]then change to polar axis\\[\\dfrac{2}{\\pi} \\int_{0}^{\\infty}\\int_{-\\phi}^{\\phi} \\exp \\big(- r^2 \\big) r dr d \\theta = \\dfrac{2\\phi}{\\pi}\\]\\[\\tan \\phi = \\dfrac{1/\\sqrt{1-\\rho}}{1/\\sqrt{1+\\rho}} = \\sqrt{\\dfrac{1+\\rho}{1-\\rho}}\\]\\[P(X(t)X(s) &gt; 0) = \\dfrac{2}{\\pi} \\arctan \\sqrt{\\dfrac{1+\\rho}{1-\\rho}}\\]use tangent half-angle formula\\[\\cos(2\\phi) = \\dfrac{1 - \\tan^2\\phi}{1 + \\tan^2\\phi} = \\dfrac{1 - \\dfrac{1+\\rho}{1-\\rho}}{1 + \\dfrac{1+\\rho}{1-\\rho}} = -\\rho\\]\\[P(X(t)X(s) &gt; 0) = \\dfrac{2}{\\pi} \\arctan \\sqrt{\\dfrac{1+\\rho}{1-\\rho}} = \\dfrac{1}{\\pi} \\arccos(-\\rho)\\]using \\(\\arcsin(x) + \\arccos(x) = \\dfrac{\\pi}{2}\\)\\[P(X(t)X(s) &gt; 0) = \\dfrac{1}{\\pi} \\arccos(-\\rho) = \\dfrac{1}{\\pi} (\\dfrac{\\pi}{2} - \\arcsin(-\\rho)) = \\dfrac{1}{2} + \\dfrac{1}{\\pi} \\arcsin(\\rho)\\]\\[P(X(t)X(s) &lt; 0) = \\dfrac{1}{2} - \\dfrac{1}{\\pi} \\arcsin(\\rho)\\]\\[R_Y(t,s) = \\dfrac{2}{\\pi} \\arcsin(\\rho) = \\dfrac{2}{\\pi} \\arcsin \\Big( \\dfrac{R_X(t-s)}{R_X(0)} \\Big)\\]It is easy to see \\(Y\\) is W.S.S.Price’s Theorem  \\((X_1, X_2) \\sim N(0, 0, \\sigma_1^2, \\sigma_2^2, \\rho)\\).\\[\\dfrac{\\partial E(g(X_1, X_2))}{\\partial \\rho} = \\sigma_1 \\sigma_2 E \\Big( \\dfrac{\\partial^2 g(X_1, X_2)}{\\partial X1 \\partial X_2} \\Big)\\]Price’s Theorem for Hard Limiter\\[g(X_1, X_2) = \\mathrm{sgn}(X_1) \\mathrm{sgn}(X_2)\\]\\[E(g(X_1, X_2)) = E(\\mathrm{sgn}(X_1) \\mathrm{sgn}(X_2))\\]\\[\\dfrac{\\partial^2 g(X_1, X_2)}{\\partial X1 \\partial X_2} = 4 \\delta(X_1) \\delta(X_2)\\]\\[\\dfrac{\\partial E}{\\partial \\rho} = 4\\sigma_1 \\sigma_2 E(\\delta(X_1) \\delta(X_2))\\]\\[\\begin{align}E(\\delta(X_1) \\delta(X_2)) &amp;= \\dfrac{1}{2\\pi \\sigma_1 \\sigma_2 \\sqrt{1 - \\rho^2}} \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty} \\exp \\bigg( -\\dfrac{1}{2(1-\\rho^2)} \\Big(\\big(\\dfrac{x_1}{\\sigma_1}\\big)^2 + \\big(\\dfrac{x_2 }{\\sigma_2}\\big)^2 - 2\\rho \\big( \\dfrac{x_1}{\\sigma_1} \\big) \\big( \\dfrac{x_2}{\\sigma_2} \\big) \\Big) \\bigg) \\delta(x_1) \\delta(x_2) d x_1 d x_2\\\\&amp;= \\dfrac{1}{2\\pi \\sigma_1 \\sigma_2 \\sqrt{1 - \\rho^2}}\\end{align}\\]\\[\\dfrac{\\partial E}{\\partial \\rho} = \\dfrac{2}{\\pi} \\dfrac{1}{\\sqrt{1-\\rho^2}}\\]using\\[\\dfrac{d}{dx} \\arcsin x = \\dfrac{1}{\\sqrt{1-x^2}}\\]\\[E(\\rho) = \\dfrac{2}{\\pi} \\arcsin(\\rho) + C\\]when \\(\\rho = 0, E(0) = 0, \\arcsin(0) = 0\\)\\[E(\\rho) = \\dfrac{2}{\\pi} \\arcsin(\\rho)\\]Price’s Theorem for Square Deviceon the videoProof of Price’s Theorem\\[\\begin{align}&amp; E(g(X_1, X_2)) = \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} g(X_1, X_2) f_{X_1, X_2}(x_1, x_2) d x_1 d x_2\\\\=&amp; \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} g(X_1, X_2) \\Bigg( \\dfrac{1}{(2\\pi)^2} \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} \\Phi(\\omega_1, \\omega_2) \\exp(-j(\\omega_1 x_1 + \\omega_2 x_2)) d \\omega_1 d \\omega_2 \\Bigg) d x_1 d x_2\\end{align}\\]\\[\\Phi(\\omega_1, \\omega_2) = \\exp\\Big( -\\frac{1}{2} \\omega^\\intercal \\Sigma \\omega \\Big)\\]\\[\\Sigma =\\begin{pmatrix}\\sigma_1^2 &amp; \\rho \\sigma_1 \\sigma_2\\\\\\rho \\sigma_1 \\sigma_2 &amp; \\sigma_2^2\\end{pmatrix}\\]\\[\\Phi(\\omega_1, \\omega_2) = \\exp\\Big( -\\frac{1}{2} (\\sigma_1^2 \\omega_1^2 + \\sigma_2^2 \\omega_2^2 + 2 \\rho \\sigma_1 \\sigma_2 \\omega_1 \\omega_2) \\Big)\\]\\[\\dfrac{\\partial}{\\partial \\rho} \\Phi(\\omega_1, \\omega_2) = - \\sigma_1 \\sigma_2 \\omega_1 \\omega_2 \\Phi(\\omega_1, \\omega_2)\\]\\[\\begin{align}&amp;\\dfrac{\\partial}{\\partial \\rho} E(g(X_1, X_2)) = \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} g(X_1, X_2) \\Bigg( \\dfrac{1}{(2\\pi)^2} \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} (-\\sigma_1 \\sigma_2 \\omega_1 \\omega_2) \\Phi(\\omega_1, \\omega_2) \\exp(-j(\\omega_1 x_1 + \\omega_2 x_2)) d \\omega_1 d \\omega_2 \\Bigg) d x_1 d x_2\\\\=&amp; \\sigma_1 \\sigma_2 \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} g(X_1, X_2) \\Bigg( \\dfrac{1}{(2\\pi)^2} \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} (-\\omega_1 \\omega_2) \\Phi(\\omega_1, \\omega_2) \\exp(-j(\\omega_1 x_1 + \\omega_2 x_2)) d \\omega_1 d \\omega_2 \\Bigg) d x_1 d x_2\\\\=&amp; \\sigma_1 \\sigma_2 \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} g(X_1, X_2) \\dfrac{\\partial^2}{\\partial X_1 \\partial X_2} \\Bigg( \\dfrac{1}{(2\\pi)^2} \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} \\Phi(\\omega_1, \\omega_2) \\exp(-j(\\omega_1 x_1 + \\omega_2 x_2)) d \\omega_1 d \\omega_2 \\Bigg) d x_1 d x_2\\\\=&amp; \\sigma_1 \\sigma_2 \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} g(X_1, X_2) \\dfrac{\\partial^2}{\\partial X_1 \\partial X_2} f_{X_1, X_2} (x_1, x_2) d x_1 d x_2\\end{align}\\]if \\(g(X_1, X_2)\\) grows slower than \\(\\exp\\big(-\\frac{1}{2}(\\sigma_1^2 x_1^2 + \\sigma_2^2 x_2^2)\\big)\\)\\[\\begin{align}\\dfrac{\\partial}{\\partial \\rho} E(g(X_1, X_2)) &amp;= \\sigma_1 \\sigma_2 \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty}\\dfrac{\\partial^2}{\\partial X_1 \\partial X_2} g(X_1, X_2) f_{X_1, X_2} (x_1, x_2) d x_1 d x_2\\\\&amp;= \\sigma_1 \\sigma_2 E \\Big( \\dfrac{\\partial^2 g(X_1, X_2)}{\\partial X1 \\partial X_2} \\Big)\\end{align}\\]Example without Price’s Theorem\\[\\cos(X)\\]It doesn’t have to use Price’s theorem. We can directly apply trigeometry formulas, then use characteristic function to get the result." }, { "title": "Stochastic Process 2023 08", "url": "/posts/stochastic-process-2023-08/", "categories": "math", "tags": "math", "date": "2023-05-04 05:00:00 +0000", "snippet": "From Hao Zhang’s 2023 Lecture 08Gaussian ProcessesSample Mean and Variance  \\(X_1, X_2, \\dots, X_n \\overset{\\text{i.i.d.}}{\\sim} N(\\mu, \\sigma^2)\\), we look at \\(\\overline{X} = \\dfrac{1}{n} \\sum_{k=1}^{n} X_k, \\quad \\overline{S} = \\dfrac{1}{n-1} \\sum_{k=1}^n (X_k - \\overline{X})^2\\), we will prove \\(\\overline{X}, \\overline{S}\\) are independent.\\[\\begin{align}&amp; (n-1) \\overline{S} = \\sum_{k=1}^n (X_k - \\overline{X})^2\\\\=&amp; \\sum_{k=1}^n (X_k^2 - 2X_k \\overline{X} + (\\overline{X})^2)\\\\=&amp; \\sum_{k=1}^n X_k^2 - n (\\overline{X})^2\\end{align}\\]we design a matrix\\[A =\\begin{pmatrix}\\frac{1}{\\sqrt{n}} &amp; \\frac{1}{\\sqrt{n}} &amp; \\dots &amp; \\frac{1}{\\sqrt{n}}\\\\\\dots &amp; \\dots &amp; \\dots &amp; \\dots\\\\\\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\\\dots &amp; \\dots &amp; \\dots &amp; \\dots\\end{pmatrix}\\]such that \\(A A^{\\intercal} = A^\\intercal A = I\\)Then, for \\(Y = AX\\), we know \\(Y \\sim N(A \\mu, A \\sigma^2 I A^\\intercal) = N(A \\mu, \\sigma^2 I)\\) \\(Y_1 = \\sqrt{n}\\cdot\\overline{X}\\). Also\\[X = A^\\intercal Y\\]\\[\\begin{align}&amp; (n-1) \\overline{S} = \\sum_{k=1}^n X_k^2 - n(\\overline{X})^2\\\\=&amp; X^\\intercal X - n(\\overline{X})^2\\\\=&amp; Y^\\intercal A A ^\\intercal Y - Y_1^2\\\\=&amp; \\sum_{k=2}^n Y_k^2\\end{align}\\]Thus we know \\((n-1) \\overline{S}\\) are independent with \\(Y_1\\)Conditional Distribution\\[(X_1, X_2) \\sim N\\Bigg(\\begin{pmatrix}\\mu_1\\\\\\mu_2\\end{pmatrix},\\begin{pmatrix}\\Sigma_{11} &amp; \\Sigma_{12}\\\\\\Sigma_{21} &amp; \\Sigma_{22}\\end{pmatrix}\\Bigg), \\quadX_1 \\in \\mathbb{R}^m, X_2 \\in \\mathbb{R}^n\\]We want to find the distribution \\(f_{X_2 \\vert X_1} (x_2 \\vert x_1) = f_{X_1, X_2}(x_1, x_2) / f_{X_1}(x_1)\\), we need to deal with the following\\[\\exp \\Bigg( -\\frac{1}{2}\\begin{pmatrix}x_1 - \\mu_1\\\\x_2 - \\mu_2\\end{pmatrix}^\\intercal\\begin{pmatrix}\\Sigma_{11} &amp; \\Sigma_{12}\\\\\\Sigma_{21} &amp; \\Sigma_{22}\\end{pmatrix}^{-1}\\begin{pmatrix}x_1 - \\mu_1\\\\x_2 - \\mu_2\\end{pmatrix}- \\frac{1}{2} (x_1^\\intercal - \\mu_1^\\intercal) \\Sigma_{11}^{-1} (x_1 - \\mu_1)\\Bigg)\\]First we need to calculate the inverse\\[\\begin{pmatrix}\\Sigma_{11} &amp; \\Sigma_{12}\\\\\\Sigma_{21} &amp; \\Sigma_{22}\\end{pmatrix}\\begin{array}{c} \\begin{pmatrix} I &amp; 0 \\\\ -\\Sigma_{21} \\Sigma_{11}^{-1} &amp; I \\end{pmatrix} \\\\ \\xrightarrow[\\text{left}]{\\hspace{3cm}}\\end{array}\\begin{pmatrix}\\Sigma_{11} &amp; \\Sigma_{12}\\\\0 &amp; \\Sigma_{22} - \\Sigma_{21} \\Sigma_{11}^{-1} \\Sigma_{12}\\end{pmatrix}\\]\\[\\begin{pmatrix}\\Sigma_{11} &amp; \\Sigma_{12}\\\\0 &amp; \\Sigma_{22} - \\Sigma_{21} \\Sigma_{11}^{-1} \\Sigma_{12}\\end{pmatrix}\\begin{array}{c} \\begin{pmatrix} I &amp; -\\Sigma_{11}^{-1} \\Sigma_{12} \\\\ 0 &amp; I \\end{pmatrix} \\\\ \\xrightarrow[\\text{right}]{\\hspace{3cm}}\\end{array}\\begin{pmatrix}\\Sigma_{11} &amp; 0\\\\0 &amp; \\Sigma_{22} - \\Sigma_{21} \\Sigma_{11}^{-1} \\Sigma_{12}\\end{pmatrix}\\]Thus\\[\\begin{pmatrix} I &amp; 0 \\\\ -\\Sigma_{21} \\Sigma_{11}^{-1} &amp; I\\end{pmatrix}\\begin{pmatrix}\\Sigma_{11} &amp; \\Sigma_{12}\\\\\\Sigma_{21} &amp; \\Sigma_{22}\\end{pmatrix}\\begin{pmatrix} I &amp; -\\Sigma_{11}^{-1} \\Sigma_{12} \\\\ 0 &amp; I\\end{pmatrix}=\\begin{pmatrix}\\Sigma_{11} &amp; 0\\\\0 &amp; \\Sigma_{22} - \\Sigma_{21} \\Sigma_{11}^{-1} \\Sigma_{12}\\end{pmatrix}\\]we know\\[\\begin{pmatrix}I &amp; 0\\\\A &amp; I\\end{pmatrix}^{-1} =\\begin{pmatrix}I &amp; 0\\\\-A &amp; 0\\end{pmatrix}\\]\\[\\begin{pmatrix}I &amp; A\\\\0 &amp; I\\end{pmatrix}^{-1} =\\begin{pmatrix}I &amp; -A\\\\0 &amp; 0\\end{pmatrix}\\]thus\\[\\begin{pmatrix}\\Sigma_{11} &amp; \\Sigma_{12}\\\\\\Sigma_{21} &amp; \\Sigma_{22}\\end{pmatrix}=\\begin{pmatrix} I &amp; 0 \\\\ \\Sigma_{21} \\Sigma_{11}^{-1} &amp; I\\end{pmatrix}\\begin{pmatrix}\\Sigma_{11} &amp; 0\\\\0 &amp; \\Sigma_{22} - \\Sigma_{21} \\Sigma_{11}^{-1} \\Sigma_{12}\\end{pmatrix}\\begin{pmatrix} I &amp;\\Sigma_{11}^{-1} \\Sigma_{12} \\\\ 0 &amp; I\\end{pmatrix}\\]thus\\[\\begin{pmatrix}\\Sigma_{11} &amp; \\Sigma_{12}\\\\\\Sigma_{21} &amp; \\Sigma_{22}\\end{pmatrix}^{-1}=\\begin{pmatrix} I &amp;-\\Sigma_{11}^{-1} \\Sigma_{12} \\\\ 0 &amp; I\\end{pmatrix}\\begin{pmatrix}\\Sigma_{11}^{-1} &amp; 0\\\\0 &amp; (\\Sigma_{22} - \\Sigma_{21} \\Sigma_{11}^{-1} \\Sigma_{12})^{-1}\\end{pmatrix}\\begin{pmatrix} I &amp; 0 \\\\ -\\Sigma_{21} \\Sigma_{11}^{-1} &amp; I\\end{pmatrix}\\]we can continue\\[\\begin{align}&amp;\\begin{pmatrix}x_1 - \\mu_1\\\\x_2 - \\mu_2\\end{pmatrix}^\\intercal\\begin{pmatrix}\\Sigma_{11} &amp; \\Sigma_{12}\\\\\\Sigma_{21} &amp; \\Sigma_{22}\\end{pmatrix}^{-1}\\begin{pmatrix}x_1 - \\mu_1\\\\x_2 - \\mu_2\\end{pmatrix}\\\\=&amp;\\big( x_1^\\intercal - \\mu_1^\\intercal \\, , \\, x_2^\\intercal - \\mu_2^\\intercal - (x_1^\\intercal - \\mu_1^\\intercal)\\Sigma_{11}^{-1} \\Sigma_{12}\\big)\\begin{pmatrix}\\Sigma_{11}^{-1} &amp; 0\\\\0 &amp; (\\Sigma_{22} - \\Sigma_{21} \\Sigma_{11}^{-1} \\Sigma_{12})^{-1}\\end{pmatrix}\\begin{pmatrix}x_1 - \\mu_1\\\\x_2 - \\mu_2 - \\Sigma_{21}\\Sigma_{11}^{-1}(x_1 - \\mu_1)\\end{pmatrix}\\\\=&amp; (x_1^\\intercal - \\mu_1^\\intercal) \\Sigma_{11}^{-1} (x_1 - \\mu_1) + \\big(x_2 - \\mu_2 - \\Sigma_{21} \\Sigma_{11}^{-1}(x_1-\\mu_1) \\big)^\\intercal (\\Sigma_{22} - \\Sigma_{21} \\Sigma_{11}^{-1} \\Sigma_{12})^{-1} \\big(x_2 - \\mu_2 - \\Sigma_{21} \\Sigma_{11}^{-1} (x_1 - \\mu_1)\\big)\\end{align}\\]thus\\[\\exp \\Big( -\\dfrac{1}{2} \\big(x_2 - \\mu_2 - \\Sigma_{21} \\Sigma_{11}^{-1}(x_1-\\mu_1) \\big)^\\intercal (\\Sigma_{22} - \\Sigma_{21} \\Sigma_{11}^{-1} \\Sigma_{12})^{-1} \\big(x_2 - \\mu_2 - \\Sigma_{21} \\Sigma_{11}^{-1} (x_1 - \\mu_1)\\big) \\Big)\\]and\\[f_{X_2 \\vert X_1} (x_2 \\vert x_1) \\sim N(\\mu_2 + \\Sigma_{21}\\Sigma_{11}^{-1}(x_1 - \\mu_1), \\Sigma_{22} - \\Sigma_{21}\\Sigma_{11}^{-1}\\Sigma_{12})\\]Special CasesIf \\(X_1, X_2 \\in \\mathbb{R}\\)\\[E(X_2 \\vert X_1) = \\mu_2 + \\dfrac{\\sigma_{21}}{\\sigma_{11}}(x_1 - \\mu_1)\\]\\[\\mathrm{Var}(X_2 \\vert X_1) = \\sigma_{22} - \\dfrac{\\sigma_{21}^2}{\\sigma_{11}}\\]More ExamplesWith the conclusion we have, we can do\\[E(f(X_2) \\vert X_1)\\]for any function \\(f\\), since we know it is a normal distribution.We have conditional pdf, conditional expection, why we didn’t have conditional variable?For \\(Y \\sim N(0, \\sigma^2)\\), let’s calculate \\(E(Y^n)\\)\\[E(Y^n) =\\begin{cases}0, \\quad n = 2k-1\\\\(2k-1)!! \\sigma^{2k}, \\quad n = 2k\\end{cases}\\]\\[\\begin{align}&amp;\\int_{-\\infty}^{\\infty} y^{2k} \\exp\\big(-\\dfrac{y^2}{2\\sigma^2} \\big) dy\\\\=&amp; -\\sigma^2 \\int_{-\\infty}^{\\infty} y^{2k-1} d\\Big( \\exp\\big( - \\dfrac{y^2}{2\\sigma^2}\\big) \\Big)\\\\=&amp; -\\sigma^2 y^{2k-1} \\exp \\big( -\\dfrac{y^2}{2\\sigma^2} \\big) \\Big\\vert_{-\\infty}^{\\infty} + (2k-1)\\sigma^2 \\int_{-\\infty}^{\\infty} \\exp\\big(-\\dfrac{y^2}{2\\sigma^2} \\big) y^{2k-2} dy\\\\=&amp; (2k-1)\\sigma^2 \\int_{-\\infty}^{\\infty} \\exp\\big(-\\dfrac{y^2}{2\\sigma^2} \\big) y^{2k-2} dy\\end{align}\\]Thus\\[E(Y^{2k}) = (2k-1)\\sigma^2 E(Y^{2k-2})\\]Thus\\[E(Y^{2k}) = (2k-1)!! \\sigma^{2k}\\]where\\[(2k-1)!! = (2k-1) (2k-3) \\dots 1\\]\\[\\begin{align}E(Y^2) &amp;= \\sigma^2\\\\E(Y^4) &amp;= 3 \\sigma^4\\\\E(Y^6) &amp;= 15 \\sigma^6\\end{align}\\]For \\(Y \\sim N(\\mu, \\sigma^2)\\), how to calculate \\(E(\\cos(Y))\\)?\\[\\cos(Y) = \\dfrac{1}{2} \\big( \\exp(jY) + \\exp(-jY) \\big)\\]\\[\\begin{align}E(\\cos(Y)) &amp;= \\dfrac{1}{2} E\\big( \\exp(jY) + \\exp(-jY) \\big)\\\\&amp;= \\dfrac{1}{2} \\big(\\Phi_Y(1) + \\Phi_Y(-1) \\big)\\\\&amp;= \\dfrac{1}{2} \\big( \\exp(j \\mu - \\frac{1}{2}\\sigma^2) + \\exp(-j\\mu - \\frac{1}{2} \\sigma^2) \\big)\\\\&amp;= \\exp\\big(-\\frac{1}{2} \\sigma^2 \\Big) \\cos(\\mu)\\end{align}\\]BayesAssume \\(X \\sim N(\\mu, \\Sigma_X), \\quad Y = B X + Z\\), where \\(X, Z\\) are independent, \\(Z \\sim N(0, \\sigma^2 I)\\). We want to investigate \\(X \\vert Y\\). It can be understand as \\(Y\\) is the observed value, \\(X\\) is the state." }, { "title": "Cauchy Inequality", "url": "/posts/cauchy-inequality/", "categories": "math", "tags": "math", "date": "2023-05-03 05:00:00 +0000", "snippet": "Review inner product: an inner product \\(\\langle x, y \\rangle \\to \\mathbb{R}\\) if and only if it satisfies   \\(\\langle x, y \\rangle = \\langle y, x \\rangle\\)   \\(\\langle x, x \\rangle \\ge 0\\)   \\(\\langle x, x \\rangle = 0 \\quad \\implies \\quad x = 0\\). Confusion: do we really need this?   \\(\\langle x, \\alpha y + \\beta z \\rangle = \\alpha \\langle x, y \\rangle + \\beta \\langle x, z \\rangle\\)   \\(\\langle \\alpha x + \\beta y, z \\rangle = \\alpha \\langle x, z \\rangle + \\beta \\langle y, z \\rangle\\) Cauchy Inequality\\[\\vert \\langle x, y \\rangle \\vert \\le \\sqrt{\\langle x, x\\rangle \\langle y, y \\rangle}\\]Proof\\[g(\\lambda) = \\langle \\lambda x + y, \\lambda x + y \\rangle = \\lambda^2 \\langle x,x \\rangle + 2 \\lambda \\langle x, y \\rangle + \\langle y, y \\rangle \\ge 0\\]the above we use proerty 1, 2, 4, 5.if \\(\\langle x, x \\rangle = 0\\), then \\(\\langle x, y \\rangle\\) must be 0. Otherwise we can take \\(\\lambda = \\dfrac{-1 + \\langle y, y \\rangle}{2 \\langle x , y \\rangle}\\) such that \\(g(\\lambda) = -1\\).When \\(\\langle x, x \\rangle = 0\\), \\(0 \\le 0\\) of course holds.When \\(\\langle x, x \\rangle \\ne 0\\), \\(g(\\lambda)\\) is a quadratic equation with 1 or zero real root. Thus its discriminate \\(b^2 - 4 ac &lt; 0\\)\\[(2\\langle x, y \\rangle )^2 - 4 \\langle x, x \\rangle \\langle y, y \\rangle \\le 0\\]it gives\\[\\vert \\langle x, y \\rangle \\vert \\le \\sqrt{\\langle x, x\\rangle \\langle y, y \\rangle}\\]" }, { "title": "数学分析 001", "url": "/posts/mathematical-analysis-001/", "categories": "math", "tags": "math", "date": "2023-05-02 05:00:00 +0000", "snippet": "陈纪修老师的课程 P1第一章 集合与映射集合集合（集），具有某种特定性质，具体的或抽象的对象汇集的总体。集合一般用大写字母，例如\\(S,T,A,B,X,Y\\)表示。集合中的一个元素一般用小写字母标识，例如\\(s,t,a,b,x,y\\)。一些记号\\[x \\in S\\]\\[y \\notin S\\] 正整数集合 整数集合 有理数集合 实数集合 \\(\\mathbb{N}^+\\) \\(\\mathbb{Z}\\) \\(\\mathbb{Q}\\) \\(\\mathbb{R}\\) 集合的表示一般又两种方法： 枚举法 描述法枚举法  \\(\\{\\text{red}, \\text{green}, \\text{blue}\\}\\)  \\(\\mathbb{N}^+ = \\{1, 2, \\dots, n, \\dots\\}\\)  \\(\\mathbb{Z} = \\{0, \\pm 1, \\pm 2, \\dots, \\pm n, \\dots\\}\\)描述法  \\(S = \\{x \\vert x \\text{ satisfy property } p\\}\\)  \\(A = \\{x \\vert x^2 = 2\\} = \\{ \\pm \\sqrt{2}\\}\\)  \\(Q = \\{x \\vert x = \\dfrac{q}{p}, p \\in \\mathbb{N}^+, q \\in \\mathbb{Z}\\}\\)集合没有次序关系：\\(\\{a,b\\} = \\{b,a\\}\\)集合中的元素重复是没有意义的：\\(\\{a,b\\} = \\{a,a,b\\}\\)空集的概念：\\(\\emptyset = \\{\\} = \\{x \\vert x \\in \\mathbb{R} \\text{ and } x^2 = -1\\}\\) P2子集的概念：对于集合\\(S,T\\)，如果\\(S\\)的所有元素都存在于\\(T\\), 则\\(S \\subset T\\)数学逻辑上的定义：\\(S \\subset T\\) 定义为 \\(x \\in S \\implies x \\in T\\)\\(p \\implies q\\) 是记号 \\((p \\land q) \\lor (\\lnot p)\\)  \\(\\mathbb{N}^+ \\subset \\mathbb{Z} \\subset \\mathbb{Q} \\subset \\mathbb{R}\\)如果\\(S\\)中至少有一个元素不属于\\(T\\),则\\(S\\)不是\\(T\\)的子集，\\(S \\not\\subset T\\)真子集：\\(S \\subsetneq T\\)例1.1.1： \\(T=\\{a,b,c\\}\\), 求\\(T\\)的子集\\[\\emptyset, \\{a\\}, \\{b\\}, \\{c\\}, \\{a,b\\}, \\{a,c\\}, \\{b,c\\}, \\{a,b,c\\}\\]集合相等：\\(S = T \\iff S \\subset T \\text{ and } T \\subset S\\)实数的集合  \\((a,b) = \\{x \\vert x \\in \\mathbb{R} \\text{ and } a &lt; x &lt; b\\}\\)  \\((a, \\infty) = \\{x \\vert x \\in \\mathbb{R} \\text{ and } a &lt; x &lt; \\infty\\}\\)  \\([a,b] = \\{x \\vert x \\in \\mathbb{R}, \\text{ and } a \\le x \\le b\\}\\)集合的运算 并 交 差 补 \\(\\cup\\) \\(\\cap\\) \\(\\backslash\\) \\(S_{X}^{C}\\) 对于补运算，我们需要知道，我们在哪个总集合里讨论问题  \\(S \\cup T = \\{x \\vert x \\in S \\text{ or } x \\in T\\}\\)  \\(S \\cap T = \\{x \\vert x \\in S \\text{ and } x \\in T\\}\\)  \\(S \\backslash T = \\{x \\vert x \\in S \\text{ and } x \\not\\in T\\}\\)  \\(S_{X}^{C} = S^{C} = \\{x \\vert x \\in X \\text{ and } x \\not\\in S\\}\\)例：\\(S = \\{a,b,c\\}, T = \\{b,c,d,e\\}\\)  \\(S \\cup T = \\{a,b,c,d,e\\}\\)  \\(S \\cap T = \\{b,c\\}\\)  \\(S \\backslash T = \\{a\\}\\)交换律：\\[S \\cup T = T \\cup S\\]\\[S \\cap T = T \\cap S\\]结合律：\\[A \\cup (B \\cup D) = (A \\cup B) \\cup D\\]\\[A \\cap (B \\cap D) = (A \\cap B) \\cap D\\]分配律：\\[A \\cap (B \\cup D) = (A \\cap B) \\cup (A \\cap D)\\]\\[A \\cup (B \\cap D) = (A \\cup B) \\cap (A \\cup D)\\]以最后一条为例，进行证明，分两步，第一步证明\\(A \\cup (B \\cap D) \\subset (A \\cup B) \\cap (A \\cup D)\\)若\\(x \\in A \\cup (B \\cap D)\\)，则或者\\(x \\in A\\), 或者 \\(x \\in B \\text{ 且 } x \\in D\\)，则\\(x \\in A \\cup B\\) 且 \\(x \\in A \\cup D\\), 也就是\\(x \\in (A \\cup B) \\cap (A \\cup D)\\)，所以\\(A \\cup (B \\cap D) \\subset (A \\cup B) \\cap (A \\cup D)\\)第二步证明\\(A \\cup (B \\cap D) \\supset (A \\cup B) \\cap (A \\cup D)\\)若\\(x \\in (A \\cup B) \\cap (A \\cup D)\\), 则\\(x \\in A \\cup B\\)且\\(x \\in A \\cup D\\)，那么或者\\(x \\in A\\)，或者\\(x \\in B \\cap D\\)，也就是\\(x \\in A \\cup (B \\cap D)\\)，所以\\(A \\cup (B \\cap D) \\supset (A \\cup B) \\cap (A \\cup D)\\)对偶律(De Morgen)：  \\((A \\cup B)^C = A^C \\cap B^C\\)  \\((A \\cap B)^C = A^C \\cup B^C\\) P3有限集与无限集  \\(S\\) 是由\\(n\\)个元素组成（\\(n\\)是非负正整数），则\\(S\\)是有限集。空集是有限集。不是有限集的集合是无限集。可列集：如果无限集中的元素可按某种规则排成一列，则该集成为可列集。困惑：一列列出来有重复会有问题么？感觉没有问题。可列集：\\(\\mathbb{N}^+, \\{x \\vert \\sin(x) = 0\\}\\)任何一个无限集包含可列子集。但是无限集不一定是可列集。之后会证明实数集不是可列集。定理1.1.1：可列个可列集的并也是可列集\\[\\begin{align}\\bigcup\\limits_{n=1}^{\\infty} A_n &amp;= A_1 \\cup A_2 \\cup \\dots A_n \\cup \\dots\\\\&amp;= \\{x \\vert \\text{ 存在 } n \\in \\mathbb{N}^+, \\text{ 使得 } x \\in A_n\\}\\end{align}\\]先把各个集合列出来\\[\\begin{align}A_1&amp;: x_{11}, x_{12}, x_{13}, \\dots\\\\A_2&amp;: x_{21}, x_{22}, x_{23}, \\dots\\\\A_3&amp;: x_{31}, x_{32}, x_{33}, \\dots\\end{align}\\]然后对角线法排出大集合定理1.1.2: 有理数集合是可列集只需要证明\\((0,1]\\)上的有理数是可列集，然后可列个可列集是可列集。笛卡尔乘积集合P3, 27分钟" }, { "title": "Stochastic Process 2023 07", "url": "/posts/stochastic-process-2023-07/", "categories": "math", "tags": "math", "date": "2023-05-01 05:00:00 +0000", "snippet": "From Hao Zhang’s 2023 Lecture 07Gaussian ProcessesDefinition   \\(n=1, \\quad X \\sim N(\\mu, \\sigma^2)\\)\\[f_X(x) = \\dfrac{1}{\\sqrt{2\\pi} \\sigma} \\exp\\Big( -\\dfrac{(x-\\mu)^2}{2\\sigma^2} \\Big)\\]   \\(n=2, \\quad X \\sim N(\\mu_1, \\mu_2, \\sigma_1, \\sigma_2, \\rho)\\)\\[\\rho = \\dfrac{E((X_1-\\mu_1)(X_2 - \\mu_2))}{\\sigma_1 \\sigma_2}\\]\\[f_{X_1, X_2}(x_1, x_2) = \\dfrac{1}{2\\pi \\sigma_1 \\sigma_2 \\sqrt{1-\\rho^2}} \\exp \\bigg( -\\dfrac{1}{2(1-\\rho^2)} \\Big(\\big(\\dfrac{x_1 -\\mu_1}{\\sigma_1}\\big)^2 + \\big(\\dfrac{x_2 -\\mu_2}{\\sigma_2}\\big)^2 - 2\\rho \\big( \\dfrac{x_1 - \\mu_1}{\\sigma_1} \\big) \\big( \\dfrac{x_2 - \\mu_2}{\\sigma_2} \\big) \\Big) \\bigg)\\]If \\(X(t)\\) is a Gaussian Processes, then \\(\\forall n, \\forall t_1, \\dots, t_n\\), the \\(X = (X(t_1), \\dots, X(t_n))^{\\mathrm{T}}\\) follows Gaussian distribution. \\(X \\sim N(\\mu, \\Sigma)\\), where \\(\\mu = E(X), \\Sigma = E(X-\\mu)(X-\\mu)^{\\mathrm{T}}\\). It has pdf\\[f(x) = \\dfrac{1}{(2\\pi)^{n/2} (\\mathrm{det} \\Sigma)^{1/2}} \\exp \\big( -\\frac{1}{2} (x-\\mu)^{\\mathrm{T}} \\Sigma^{-1} (x-\\mu) \\big)\\]Let’s first do an exercise to be familiar with the notation. Let’s verify its integration is 1.\\[\\begin{align}\\int \\exp \\big( -\\frac{1}{2} (x-\\mu)^{\\mathrm{T}} \\Sigma^{-1} (x-\\mu) \\big) dx\\end{align}\\]we need to diagonalize \\(\\Sigma^{-1}\\). Since \\(\\Sigma\\) is P.D., we know \\(\\Sigma = U^{\\mathrm{T}} \\Lambda U\\), where \\(\\Lambda = \\mathrm{diag}(\\lambda_1, \\dots, \\lambda_n), U^{\\mathrm{T}}U = U U^{\\mathrm{T}} = I\\). And\\[\\Sigma^{-1} = U^{\\mathrm{T}} \\Lambda^{-1} U = U^{\\mathrm{T}} \\Lambda^{-\\frac{1}{2}} \\Lambda^{-\\frac{1}{2}}U = B^{\\mathrm{T}} B\\]\\[\\begin{align}&amp;\\int \\exp \\big( -\\frac{1}{2} (x-\\mu)^{\\mathrm{T}} \\Sigma^{-1} (x-\\mu) \\big) dx\\\\=&amp; \\int \\exp \\big( -\\frac{1}{2} (x-\\mu)^{\\mathrm{T}} B^{\\mathrm{T}} B (x-\\mu) \\big) dx\\end{align}\\]Let \\(y = B x\\). We need to calculate\\[\\begin{align}dx &amp;= \\bigg\\vert \\mathrm{det} \\dfrac{\\partial x}{\\partial y} \\bigg\\vert dy\\\\&amp;= \\bigg\\vert \\mathrm{det} \\dfrac{\\partial y}{\\partial x} \\bigg\\vert^{-1} dy\\\\&amp;= \\bigg\\vert \\mathrm{det} B \\bigg\\vert^{-1} dy\\\\&amp;= \\bigg\\vert \\mathrm{det} \\Lambda^{-\\frac{1}{2}} \\mathrm{det} U \\bigg\\vert^{-1} dy\\\\&amp;= \\bigg\\vert \\mathrm{det} \\Lambda \\bigg\\vert^{1/2} dy\\\\&amp;= ( \\mathrm{det} \\Sigma )^{1/2} dy\\end{align}\\]\\[\\begin{align}&amp; \\int \\exp \\big( -\\frac{1}{2} (x-\\mu)^{\\mathrm{T}} B^{\\mathrm{T}} B (x-\\mu) \\big) dx\\\\=&amp; ( \\mathrm{det} \\Sigma )^{1/2}\\int \\exp \\big( -\\frac{1}{2} y^{\\mathrm{T}} y \\big) dy\\\\=&amp; ( \\mathrm{det} \\Sigma )^{1/2}\\bigg(\\int_{-\\infty}^{\\infty} \\exp \\big( -\\frac{\\tau^2}{2} \\big) d\\tau\\bigg)^{n}\\end{align}\\]we know\\[\\int_{-\\infty}^{\\infty} \\exp \\big( -\\frac{\\tau^2}{2} \\big) d\\tau = \\sqrt{2\\pi}\\]but let’s prove it just for fun. Interestingly its indefinite integral doesn’t exist, we can’t find its primitive function then use the bounds.\\[\\begin{align}&amp;\\int_{-\\infty}^{\\infty} \\exp \\big( -\\frac{x^2}{2} \\big) dx \\int_{-\\infty}^{\\infty} \\exp \\big( -\\frac{y^2}{2} \\big) dy\\\\=&amp;\\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} \\exp \\big( -\\frac{x^2+y^2}{2} \\big) dx dy\\end{align}\\]we change to polar axis\\[\\begin{align}&amp;\\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} \\exp \\big( -\\frac{x^2+y^2}{2} \\big) dx dy\\\\=&amp;\\int_{0}^{\\infty} \\int_{2\\pi}^{0} \\exp \\big( -\\frac{r^2}{2} \\big)r d\\theta dr\\\\=&amp; 2\\pi \\int_{0}^{\\infty} \\exp \\big( -\\frac{r^2}{2} \\big)r dr\\\\=&amp; 2\\pi \\int_{0}^{\\infty} \\exp ( -u ) d u\\\\=&amp; 2\\pi \\cdot \\big(-\\exp(-u)\\big) \\big\\vert_{0}^{\\infty}\\\\=&amp; 2\\pi\\end{align}\\]thus\\[f(x) = \\dfrac{1}{(2\\pi)^{n/2} (\\mathrm{det} \\Sigma)^{1/2}} \\exp \\big( -\\frac{1}{2} (x-\\mu)^{\\mathrm{T}} \\Sigma^{-1} (x-\\mu) \\big)\\]\\[\\begin{align}&amp;\\int \\exp \\big( -\\frac{1}{2} (x-\\mu)^{\\mathrm{T}} \\Sigma^{-1} (x-\\mu) \\big) dx\\\\=&amp; \\int \\exp \\big( -\\frac{1}{2} (x-\\mu)^{\\mathrm{T}} B^{\\mathrm{T}} B (x-\\mu) \\big) dx\\\\=&amp; ( \\mathrm{det} \\Sigma )^{1/2}\\bigg(\\int_{-\\infty}^{\\infty} \\exp \\big( -\\frac{\\tau^2}{2} \\big) d\\tau\\bigg)^{n}\\\\=&amp; (2\\pi)^{n/2} (\\mathrm{det} \\Sigma)^{1/2}\\end{align}\\]Characteristic Function\\[\\Phi(\\omega_1, \\dots, \\omega_n) = E(\\exp(j \\omega^\\intercal X)) = \\int_{-\\infty}^{\\infty} \\dots \\int_{-\\infty}^{\\infty} \\exp(j (\\omega_1 x_1 + \\dots \\omega_n x_n)) f(x_1, \\dots, x_n) d x_1 \\dots d x_n\\]\\[f(x_1, \\dots x_n) = \\dfrac{1}{(2\\pi)^n} \\int_{-\\infty}^{\\infty} \\dots \\int_{-\\infty}^{\\infty} \\Phi(\\omega_1, \\dots, \\omega_n) \\exp(-j(\\omega_1 x_1 + \\dots + \\omega_n x_n)) d \\omega_1 \\dots d \\omega_n\\]\\[\\Phi_X(\\omega) = E(\\exp(j \\omega^{\\intercal} X)) = \\exp\\big(j \\omega^\\intercal \\mu - \\frac{1}{2} \\omega^\\intercal \\Sigma \\omega\\big)\\]\\[\\begin{align}\\dfrac{1}{(2\\pi)^{n/2}(\\mathrm{det}\\Sigma)^{1/2}} \\int_{\\mathbb{R}^n} \\exp(j \\omega^\\intercal x) \\exp\\big( -\\frac{1}{2}(x-\\mu)^\\intercal \\Sigma^{-1} (x-\\mu) \\big) dx\\end{align}\\]a small trick to find how to complete the square, from\\[-\\frac{1}{2} (x-\\mu)^\\intercal \\Sigma^{-1} (x-\\mu) + j \\omega^\\intercal x\\]we try to look\\[\\begin{align}&amp;-\\dfrac{1}{2\\sigma^2} (x-\\mu)^2 + j\\omega x\\\\=&amp; -\\dfrac{1}{2\\sigma^2} (x - \\mu - j\\sigma^2 \\omega)^2 + j \\omega \\mu - \\frac{1}{2}\\sigma^2 \\omega^2\\end{align}\\]then\\[\\begin{align}&amp;-\\frac{1}{2} (x-\\mu)^\\intercal \\Sigma^{-1} (x-\\mu) + j \\omega^\\intercal x\\\\= &amp; - \\frac{1}{2} (x - \\mu - j \\Sigma \\omega)^{\\intercal} \\Sigma^{-1} (x - \\mu - j \\Sigma \\omega ) + j \\omega^\\intercal \\mu - \\frac{1}{2}\\omega^{\\intercal} \\Sigma \\omega\\end{align}\\]thus\\[\\begin{align}&amp;\\int_{\\mathbb{R}^n} \\exp(j \\omega^\\intercal x) \\exp\\big( -\\frac{1}{2}(x-\\mu)^\\intercal \\Sigma^{-1} (x-\\mu) \\big) dx\\\\=&amp; \\exp\\big(j\\omega^\\intercal \\mu - \\frac{1}{2} \\omega^\\intercal \\Sigma \\omega\\big) \\int_{\\mathbb{R}^n} \\exp\\big(-\\frac{1}{2} (x-\\mu - j \\Sigma \\omega)^\\intercal \\Sigma^{-1} (x-\\mu - j \\Sigma \\omega)\\big) dx\\end{align}\\]The intergration inside just change the mean, thus\\[\\Phi_X(\\omega) = E(\\exp(j \\omega^{\\intercal} X)) = \\exp\\big(j \\omega^\\intercal \\mu - \\frac{1}{2} \\omega^\\intercal \\Sigma \\omega\\big)\\]Linear Property  \\(X \\in \\mathbb{R}^n, X \\sim N(\\mu, \\Sigma). A \\in \\mathbb{R}^{m \\times n}, Y = A X \\in \\mathbb{R}^m\\), then\\[Y \\sim N(A \\mu, A \\Sigma A^\\intercal)\\]\\[\\begin{align}\\Phi_{Y}(\\omega) &amp;= E(\\exp(j \\omega^\\intercal Y))\\\\&amp;= E(\\exp(j \\omega^\\intercal A X))\\\\&amp;= E(\\exp(j (A^\\intercal \\omega)^\\intercal X))\\\\&amp;= \\exp\\big( j (A^\\intercal \\omega)^\\intercal \\mu - \\frac{1}{2} (A^\\intercal \\omega)^\\intercal \\Sigma (A^{\\intercal} \\omega) \\big)\\\\&amp;= \\exp\\big( j \\omega^\\intercal A \\mu - \\frac{1}{2} \\omega^\\intercal A \\Sigma A^\\intercal \\omega \\big)\\end{align}\\]Then it is obvious, if joint-pdf is Gaussian, then all the \\(n\\) individual random variables are boundary Gaussian.But even if all the \\(n\\) individual random variables are boundary Gaussian, it may not be joint Gaussian.Actually, for \\(X \\in \\mathbb{R}^n\\), we will need \\(\\forall \\alpha \\in \\mathbb{R}^n, \\alpha^\\intercal X\\) is Gaussian to make sure \\(X\\) is joint Gaussian.\\[\\begin{align}\\Phi_X(\\omega) &amp;= E(\\exp(j \\omega^\\intercal X)) = \\Phi_{\\omega^\\intercal X}(1)\\\\&amp;= \\exp\\big( j \\mu_{\\omega^\\intercal X} - \\frac{1}{2} \\sigma_{\\omega^\\intercal X}^2 \\big)\\end{align}\\]we know\\[\\mu_{\\omega^\\intercal X} = E(\\omega^\\intercal X) = \\omega^{\\intercal} \\mu_X\\]\\[\\begin{align}&amp;\\sigma_{\\omega^\\intercal X}^2 = E(\\omega^\\intercal X - \\omega^\\intercal \\mu_X)^2\\\\=&amp; E(\\omega^\\intercal (X - \\mu_X) (X - \\mu_X)^\\intercal \\omega)\\\\=&amp; \\omega^\\intercal E((X - \\mu_X)(X - \\mu_X)^\\intercal) \\omega\\\\=&amp; \\omega^\\intercal \\Sigma_X \\omega\\end{align}\\]thus\\[\\Phi_X(\\omega) = \\exp\\big(j \\omega^\\intercal \\mu_X - \\frac{1}{2} \\omega^\\intercal \\Sigma_X \\omega\\big)\\]Another good thing about Gaussian: for \\(X = (X_1, \\dots, X_n)^\\intercal \\sim N\\), if \\(\\forall i, j, i \\ne j, E(X_i X_j) = E(X_i) E(X_j)\\) (meaning they are uncorrelated), then we have \\(\\forall i, j, i \\ne j\\), \\(X_i, X_j\\) are independent.This is due to for \\(i\\ne j, \\Sigma_{ij} = E(X_i - EX_i)(X_j - E X_j) = E(X_i X_j) - E(X_i)E(X_j) = 0\\).Thus \\(\\Sigma\\) is diagonal matrix, from its distribution we know they are independent (because joint-pdf equals the product of boundary pdfs).To generate any Gaussian, we only need to go from \\(X \\sim N(0, I)\\), then \\(\\tilde{X} = \\Sigma^{1/2} (X + \\Sigma^{-1/2} \\mu) \\sim N(\\mu, \\Sigma)\\)MidJourney DiffusionA small example, \\(X_k = \\sqrt{1-\\alpha_k} \\cdot X_{k-1} + \\sqrt{\\alpha_k} \\cdot \\epsilon_k\\), where \\(\\epsilon_k\\) i.i.d., \\(N(0, I)\\).Reparametric Trick\\[\\beta_k = 1 - \\alpha_k\\]\\[\\begin{align}X_k &amp;= \\sqrt{\\beta_k} \\cdot X_{k-1} + \\sqrt{1-\\beta_k} \\cdot \\epsilon_k\\\\&amp;= \\sqrt{\\beta_k} (\\sqrt{\\beta_{k-1}} \\cdot X_{k-1} + \\sqrt{1 - \\beta_{k-1}} \\cdot \\epsilon_{k-1}) + \\sqrt{1-\\beta_k} \\cdot \\epsilon_k\\\\&amp;= \\sqrt{\\beta_k \\beta_{k-1}} \\cdot X_{k-2} + \\sqrt{\\beta_k} \\sqrt{1 - \\beta_{k-1}} \\epsilon_{k-1} + \\sqrt{1- \\beta_k} \\epsilon_k\\\\&amp;= \\sqrt{\\beta_k \\beta_{k-1}} \\cdot X_{k-1} + N(0, \\sqrt{1- \\beta_{k} \\beta_{k-1}} I)\\end{align}\\]\\[X_{k} \\sim N(0, \\sqrt{1-\\beta_k \\beta_{k-1} \\dots \\beta_{1}} I)\\]" }, { "title": "Stochastic Process 03", "url": "/posts/stochastic-process-03/", "categories": "math", "tags": "math", "date": "2023-04-16 05:00:00 +0000", "snippet": "From Hao Zhang’s Lecture 03Spectral AnalysisLet’s review spectral analysis of deterministic signal.If \\(x(t)\\) is periodic signal with period \\(T\\), we have Fourier series.\\[x(t) = \\sum_{k=-\\infty}^{\\infty} \\alpha_k e^{j \\omega_k t}\\]where\\[\\omega_k = \\dfrac{2k\\pi}{T}\\]and\\[\\alpha_{k} = \\dfrac{1}{T} \\int_{-\\frac{T}{2}}^{\\frac{T}{2}} x(t) e^{-j \\omega_k t} dt\\]It can be written as\\[\\begin{align}x(t) &amp;= \\dfrac{1}{T} \\sum_{k=-\\infty}^{\\infty} \\bigg( \\int_{-\\frac{T}{2}}^{\\frac{T}{2}} x(s) \\exp(-j \\dfrac{2 k \\pi}{T} s) ds \\bigg) \\exp(j \\dfrac{2k \\pi}{T} t)\\end{align}\\]Integral is the limit of summation in the sense of\\[\\int f(x)dx = \\sum_{k} f(x_k) (x_k - x_{k-1}) = \\sum_{k} f(x_k) \\Delta x_k, \\quad \\text{ when } \\Delta x_k \\to 0\\]thus\\[\\begin{align}x(t) &amp;= \\dfrac{1}{2\\pi} \\sum_{k=-\\infty}^{\\infty} \\bigg( \\int_{-\\frac{T}{2}}^{\\frac{T}{2}} x(s) \\exp(-j \\dfrac{2 k \\pi}{T} s) ds \\bigg) \\exp(j \\dfrac{2k \\pi}{T} t) \\cdot \\dfrac{2\\pi }{T}\\\\&amp;= \\dfrac{1}{2\\pi} \\int_{-\\infty}^{\\infty} \\hat{X}(\\omega) \\exp(j \\omega t) d\\omega\\end{align}\\]where\\[\\hat{X}(\\omega) = \\int_{-\\infty}^{\\infty} x(t) \\exp(-j \\omega t) dt\\]and the fourier transfer exists if \\(x(t)\\) is absolute integrable.\\[\\int_{-\\infty}^{\\infty} \\vert x(t) \\vert dt &lt; \\infty\\]this is one fundamental difficulty for stationary random process.Power Spectral DensityFor a W.S.S. \\(X(t)\\), we can try to do this\\[\\begin{align}&amp;\\dfrac{1}{T} E \\bigg\\vert \\int_{-\\frac{T}{2}}^{\\frac{T}{2}} X(t) \\exp(-j \\omega t) dt \\bigg\\vert^2\\\\=&amp; \\dfrac{1}{T} E \\bigg(\\int_{-\\frac{T}{2}}^{\\frac{T}{2}} X(t) \\exp(-j\\omega t) dt \\bigg) \\cdot \\bigg( \\int_{-\\frac{T}{2}}^{\\frac{T}{2}} \\overline{X(s)} \\exp(j \\omega s) ds \\bigg)\\\\=&amp; \\dfrac{1}{T} \\int_{-\\frac{T}{2}}^{\\frac{T}{2}} \\int_{-\\frac{T}{2}}^{\\frac{T}{2}} E(X(t)\\overline{X(s)}) \\exp(-j\\omega (t-s)) \\mathrm{d}t \\mathrm{d}s\\\\= &amp; \\dfrac{1}{T} \\int_{-\\frac{T}{2}}^{\\frac{T}{2}} \\int_{-\\frac{T}{2}}^{\\frac{T}{2}} R(t-s) \\exp(-j\\omega (t-s)) \\mathrm{d}t \\mathrm{d}s\\end{align}\\]substitude variable, \\(u=t-s, \\quad v = t+s\\), we need to take variable carefully in the range, the function, and the absolute value of the Jacobian determinant.\\[\\begin{align}\\mathrm{d}t\\mathrm{d}s = \\bigg\\vert \\mathrm{det} \\dfrac{\\partial(t,s)}{\\partial (u,v)} \\bigg\\vert \\mathrm{d}u \\mathrm{d} v\\end{align}\\]remember that for square matrix \\(A, B\\), \\(\\vert A B \\vert = \\vert A \\vert \\cdot \\vert B \\vert\\), and \\(\\bigg( \\dfrac{\\partial(t,s)}{\\partial(u,v)} \\bigg)^{-1} = \\dfrac{\\partial (u,v)}{\\partial (t,s)}\\)thus\\(\\mathrm{d} t \\mathrm{d} s = \\bigg\\vert \\dfrac{\\partial (u,v)}{\\partial (t,s)} \\bigg\\vert^{-1} \\mathrm{d} u \\mathrm{d}v\\)and\\[\\dfrac{\\partial (u,v)}{\\partial (t,s)} =\\begin{bmatrix}\\dfrac{\\partial u}{\\partial t} &amp; \\dfrac{\\partial u}{\\partial s}\\\\\\dfrac{\\partial v}{\\partial t} &amp; \\dfrac{\\partial v}{\\partial s}\\end{bmatrix} =\\begin{bmatrix}1 &amp; -1\\\\1 &amp; 1\\end{bmatrix}\\]thus\\[\\mathrm{d} t \\mathrm{d} s = \\bigg\\vert \\dfrac{\\partial (u,v)}{\\partial (t,s)} \\bigg\\vert^{-1} \\mathrm{d} u \\mathrm{d}v = \\dfrac{1}{2} \\mathrm{d} u \\mathrm{d} v\\]to deal with the range, we need to draw the diagram. As long as the original range is the conventional direction, we can use the conventional direction for the new integralthus\\[\\begin{align}&amp;\\dfrac{1}{2T} \\bigg( \\int_{-T}^{0} \\int_{-T-u}^{T+u} + \\int_{0}^{T} \\int_{-T+u}^{T-u} \\bigg) R(u)\\exp(-j\\omega u) \\mathrm{d} v \\mathrm{d} u\\\\=&amp;\\dfrac{1}{2T} \\int_{-T}^{T} \\int_{-T+\\vert u \\vert}^{T- \\vert u\\vert} R(u)\\exp(-j\\omega u) \\mathrm{d} v \\mathrm{d} u\\\\=&amp;\\dfrac{1}{2T} \\int_{-T}^{T} (2T - 2 \\vert u \\vert) R(u)\\exp(-j\\omega u) \\mathrm{d} u\\\\=&amp; \\int_{-T}^{T} \\Big(1 - \\dfrac{ \\vert u \\vert}{T}\\Big) R(u)\\exp(-j\\omega u) \\mathrm{d} u\\end{align}\\]let \\(T \\to \\infty\\)\\[\\dfrac{1}{T} E \\bigg\\vert \\int_{-\\frac{T}{2}}^{\\frac{T}{2}} X(t) \\exp(-j \\omega t) dt \\bigg\\vert^2 = \\int_{-\\infty}^{\\infty} R(u) \\exp(-j\\omega u) du = S(\\omega)\\]This is called Wiener-Khinchine theorem.It is interesting to note here, if assume \\(R(u)\\) is real function, since \\(R(u)\\) is positive definite function (thus it is also symmetry), we know its Fourier transfer is real and symmetry, and positive for all \\(\\omega\\), meets the idea of power spectral density. Thus\\[\\begin{align}S(\\omega) &amp;= \\int_{-\\infty}^{\\infty} R(t) \\cos(\\omega t) dt\\\\R(t) &amp;= \\dfrac{1}{2\\pi} \\int_{-\\infty}^{\\infty} S(\\omega) \\cos(\\omega t) d\\omega\\end{align}\\]LTI ResponsesAssumes a W.S.S. process \\(X(t)\\) with PSD \\(S(\\omega)\\) passing through a LTI system with impulse response \\(h(t)\\), we know\\[Y(t) = \\int_{-\\infty}^{\\infty} h(t-\\tau) X(\\tau) d\\tau\\]then\\[\\begin{align}&amp;R_{Y}(t,s) = E(Y(t) \\overline{Y(s)}) = E \\bigg(\\int_{-\\infty}^{\\infty} h(t-\\tau) X(\\tau) d\\tau \\bigg) \\overline{\\bigg(\\int_{-\\infty}^{\\infty} h(s-r)X(r)dr\\bigg)}\\\\=&amp; \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} R_{X}(\\tau-r) h(t-\\tau) \\overline{h(s-r)} d\\tau dr\\end{align}\\]to see if the integral is a convolutoin, the summation of all the variable in the function, has to cancel all the integral variable, and it gives the convolutoin at the value of the summation.We need to define\\[\\tilde{h}(t) = \\overline{h(-t)}\\]then\\[\\begin{align}R_{Y}(t,s) &amp;= \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} R_{X}(\\tau-r) h(t-\\tau) \\overline{h(s-r)} d\\tau dr\\\\&amp;= \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} R_{X}(\\tau-r) h(t-\\tau) \\tilde{h}(r-s) d\\tau dr\\\\&amp;= R_{X} * h * \\tilde{h} (t-s)\\end{align}\\]thus it is clear \\(Y(t)\\) is W.S.S., also\\[S_Y(\\omega) = S_X(\\omega)\\cdot H(\\omega) \\cdot \\tilde{H}(\\omega)\\]and\\[\\begin{align}\\tilde{H}(\\omega) &amp;= \\int_{-\\infty}^{\\infty} \\tilde{h}(t) \\exp(-j\\omega t) dt\\\\&amp;= \\int_{-\\infty}^{\\infty} \\overline{h(-t)} \\exp(-j\\omega t) dt\\\\&amp;= \\overline{\\int_{-\\infty}^{\\infty} h(-t) \\exp(j\\omega t) dt}\\\\&amp;= \\overline{\\int_{-\\infty}^{\\infty} h(\\tau) \\exp(-j\\omega \\tau) d\\tau}\\\\&amp;= \\overline{H(\\omega)}\\end{align}\\]thus\\[S_Y(\\omega) = S_X(\\omega) \\cdot H(\\omega) \\cdot \\overline{H(\\omega)} = S_X(\\omega) \\vert H(\\omega) \\vert^2\\]Common PSDs And Their AutocorrelationsPlease note that the Fourier transform is not the same (or similar) as Laplace transform table, since the table we usually see is for single sided functions.\\[\\begin{align}S(\\omega) &amp;= \\int_{-\\infty}^{\\infty} R(\\tau) \\exp(-j\\omega\\tau)d\\tau\\\\R(\\tau) &amp;= \\dfrac{1}{2\\pi}\\int_{-\\infty}^{\\infty} S(\\omega) \\exp(j\\omega\\tau) d\\omega\\end{align}\\]or using frequency (Hz) instead of angular frequency\\[\\begin{align}S(f) &amp;= \\int_{-\\infty}^{\\infty}R(\\tau)\\exp(-j2\\pi f \\tau) d\\tau\\\\R(\\tau) &amp;= \\int_{-\\infty}^{\\infty} S(f) \\exp(j2\\pi f \\tau) df\\end{align}\\]White NoiseFor a white noise with two-sided PSD\\[S(f) = N_0\\]The autocorrelation is\\[R(\\tau) = N_0 \\delta(\\tau)\\]We can verify it by\\[\\int_{-\\infty}^{\\infty} N_0 \\delta(\\tau) \\exp(-j2\\pi f\\tau) d\\tau = N_0\\]Note that the directly apply of the inverse Fourier transform may seems difficult.Lowpass NoiseFor a two-sided PSD\\[S(f) = \\dfrac{N_0}{1+\\big(\\dfrac{f}{f_b}\\big)^2}\\]or\\[S(\\omega) = \\dfrac{N_0 \\cdot \\omega_b^2}{\\omega_b^2+\\omega^2}\\]using the formula\\[\\int_{-\\infty}^{\\infty} \\dfrac{1}{a^2+\\omega^2} \\cdot \\exp(j\\omega t)d \\omega = \\dfrac{\\pi}{a} \\cdot \\exp(-a\\vert t \\vert), \\quad a &gt; 0\\]\\[\\begin{align}R(\\tau) &amp;= \\dfrac{1}{2\\pi} \\int_{-\\infty}^{\\infty} \\dfrac{N_0 \\cdot \\omega_b^2}{\\omega_b^2 + \\omega^2} \\cdot \\exp(j\\omega \\tau) d\\omega\\\\&amp;= \\dfrac{1}{2\\pi} \\cdot N_0 \\cdot \\omega_b^2 \\cdot \\dfrac{\\pi}{\\omega_b} \\cdot \\exp(-\\omega_b \\vert \\tau \\vert)\\\\&amp;= \\pi f_b \\cdot N_0 \\cdot \\exp(-2\\pi f_b \\vert \\tau \\vert)\\end{align}\\]We can do a quick sanity check, since the effective power bandwidth is \\(\\pi f_b/2\\), we should have\\[2N_0 \\cdot \\dfrac{\\pi}{2} \\cdot f_b = \\pi f_b \\cdot N_0\\]LTV ResponsesWithout the assumption of W.S.S. process, we need to derive the PSD again by\\[\\begin{align}&amp; \\dfrac{1}{T} E \\left\\vert \\int_{-T/2}^{T/2} X(t)\\exp(-j\\omega t) dt \\right\\vert^2\\\\= &amp; \\dfrac{1}{T} E\\left(\\int_{-T/2}^{T/2} X(t)\\exp(-j\\omega t) dt\\right) \\left( \\int_{-T/2}^{T/2} \\overline{X(s)}\\exp(j\\omega s)\\right)\\\\= &amp; \\dfrac{1}{T} \\int_{-T/2}^{T/2} \\int_{-T/2}^{T/2} E(X(t)\\overline{X(s)}) \\exp(-j\\omega(t-s)) dt ds\\\\= &amp; \\dfrac{1}{T} \\int_{-T/2}^{T/2} \\int_{-T/2}^{T/2} R(t,s) \\exp(-j\\omega(t-s)) dt ds\\end{align}\\]For a LTV system\\[Y(t) = \\int_{-\\infty}^{\\infty} X(\\tau) h(t;\\tau) d\\tau\\]where \\(h(t;\\tau)\\) is the impulse response at time \\(t\\) when the impulse is at time \\(\\tau\\). The reason is\\[X(t) = \\int_{-\\infty}^{\\infty} X(\\tau) \\delta(t-\\tau) d\\tau\\]and the response for \\(\\delta(t-\\tau)\\) is \\(h(t;\\tau)\\). Thus by superposition, the response for \\(\\int_{-\\infty}^{\\infty} X(\\tau) \\delta(t-\\tau)\\) is\\[Y(t) = \\int_{-\\infty}^{\\infty} X(\\tau) h(t;\\tau) d\\tau\\]\\[R_Y(t,s) = \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} R_X(\\tau,r) h(t;\\tau) \\overline{h(s;r)} d\\tau dr\\]" }, { "title": "Stochastic Process 02", "url": "/posts/stochastic-process-02/", "categories": "math", "tags": "math", "date": "2023-04-09 05:00:00 +0000", "snippet": "From Hao Zhang’s Lecture 02Correlation FunctionsCorrelation functions are special functions, they have many interesting and good properties.\\[R_{X}(t,s) = E(X(t)X(s))\\]as we know correlation is an inner product, we can have, where we denote \\(R_X\\) as \\(R\\)   \\(R(t,s) = R(s,t)\\)   \\(\\vert R(t,s) \\vert \\le \\sqrt{R(t,t)R(s,s)}\\), from CauchyIf it is W.S.S., \\(R(t,s) = R(t-s)\\)   \\(R(\\tau) = R(-\\tau)\\)   \\(\\vert R(\\tau) \\vert \\le R(0)\\)   \\(R(\\tau)\\) is positive definite.Positive Definite Function\\[f(x) \\text{ is P.D.} \\quad \\iff \\quad \\forall x_1, x_2, \\dots, x_n, \\text{ the matrix } (f(x_i-x_j))_{ij} \\text{ is P.D. }\\]where it means put it in \\(i\\) row and \\(j\\) column.Remember the P.D. matrix is defined as, for a \\(n \\times n\\) symmetry matrix, if \\(\\forall \\alpha \\in \\mathbb{R}^n, \\alpha^{T}A\\alpha \\ge 0\\), then matrix \\(A\\) is P.D.For P.D. matrix, all the leading principal minors are \\(\\ge 0\\).Some properties of P.D. functionIf function \\(f(x)\\) is P.D.   \\(f(0) \\ge 0\\).we take \\(n = 1\\), then the matrix \\(f(0)\\) is P.D.   \\(f(x)\\) is an even function.   \\(f(0) \\ge \\vert f(\\tau) \\vert, \\forall \\tau \\in \\mathbb{R}\\).we take \\(n = 2\\), where \\(x_1 = 0, x_2 = \\tau\\)\\[\\begin{bmatrix}f(x_1 - x_1) &amp; f(x_1 - x_2) \\\\f(x_2 - x_1) &amp; f(x_2 - x_2) \\\\\\end{bmatrix}=\\begin{bmatrix}f(0) &amp; f(-\\tau)\\\\f(\\tau) &amp; f(0)\\end{bmatrix}\\]it is P.D., thus it is symmetry, \\(f(\\tau) = f(-\\tau)\\). Also its determinant is positive, thus \\(f(0) \\ge \\vert f(\\tau) \\vert\\).Prove \\(R(\\tau)\\) is P.D.Let\\[X = (X(\\tau_1, \\dots, \\tau_n))^{\\mathrm{T}}\\]then\\[(R(\\tau_i - \\tau_j))_{ij} = E(X X^{\\mathrm{T}})\\]\\[\\begin{align}\\alpha^{\\mathrm{T}} E(X X^{\\mathrm{T}}) \\alpha = E(\\alpha^{\\mathrm{T}} X X^{\\mathrm{T}} \\alpha) = E((\\alpha^{\\mathrm{T}}X) (\\alpha^{\\mathrm{T}} X)^{\\mathrm{T}}) = E((\\alpha^{\\mathrm{T}} X)^2) \\ge 0\\end{align}\\]More \\(R(\\tau)\\) Properties: Local to GlobalLet \\(R(\\tau)\\) be the auto-correlation of a W.S.S. process.   if \\(\\exists T &gt; 0, R(T) = R(0)\\), then \\(R(\\tau) = R(\\tau + T) \\quad \\forall \\tau\\)We will go through\\[R(0) = R(T) \\quad \\implies \\quad E|X(\\tau + T) - X(\\tau)|^2 = 0 \\quad \\implies \\quad R(\\tau) = R(\\tau + T)\\]first\\[\\begin{align}&amp;E|X(\\tau + T) - X(\\tau)|^2 \\\\=&amp; E(X^2(\\tau+T)) + E(X^2(\\tau)) - 2 E(X(t+T)X(\\tau)) \\\\=&amp; R(0) + R(0) - 2R(T) = 0\\end{align}\\]then\\[\\begin{align}&amp;\\vert R(\\tau + T) - R(\\tau) \\vert \\\\=&amp; \\vert E(X(0)X(\\tau+T)) - E(X(0)X(\\tau)) \\vert\\\\=&amp; \\vert E[X(0)(X(\\tau+T) - X(\\tau))] \\vert\\end{align}\\]use Cauchy\\[\\begin{align}&amp; \\vert E[X(0)(X(\\tau+T) - X(\\tau))] \\vert\\\\\\le &amp; \\sqrt{E\\vert X(0)\\vert^2 \\cdot E \\vert X(\\tau+T) - X(\\tau) \\vert^2} = 0\\end{align}\\]   if \\(R(\\tau)\\) is continuous at 0, then it is continuous at all \\(\\tau \\in \\mathbb{R}\\).Similar, we prove in the middle\\[E|X(\\tau + \\Delta) - X(\\tau)|^2 \\to 0 \\text{ when } \\Delta \\to 0\\]Proof in the video.P.D. Function CriterionFrom Bochner’s result. A function \\(f(x)\\) is P.D., if and only if its Fourier transform is always \\(\\ge 0\\).\\[\\int_{-\\infty}^{\\infty} f(x) e^{j\\omega x} dx \\ge 0\\]Some where in the video, any W.S.S. auto-correlation function is P.D.. For any P.D. function, you can find an W.S.S. process such that is auto-correlation is that P.D. function. P.D. is the characteristic property of W.S.S. auto-correlation function. Prove \\(\\int_{-\\infty}^{\\infty} f(x) e^{j\\omega x} dx \\ge 0 \\quad \\implies \\quad f(x)\\) is P.D." }, { "title": "Stochastic Process 01", "url": "/posts/stochastic-process-01/", "categories": "math", "tags": "math", "date": "2023-04-07 05:00:00 +0000", "snippet": "From Hao Zhang’s Lecture 01IntroductionIn stochastic process we study when there are multiple (possibily infinite) random variables.For example, two random variables \\(X, Y\\). For random variables, if we know their joint-pdf, we know everything (or it is impossible to know more).\\[\\begin{align}f_{X,Y}(x,y) &amp;= \\dfrac{\\partial^2}{\\partial x \\partial y} F_{X,Y}(x,y)\\\\F_{X,Y}(x,y) &amp;= P(X \\le x, Y \\le Y)\\end{align}\\]CorrelationBefore we define correlation, let’s do some examples. Example 01Assume two random variables \\(X,Y\\), the joint-pdf is spindle-shape, we can draw a line pass the origin, such that the joint-pdf fits the line best. The definition of the best fit is\\[\\alpha_{opt} = \\min E ((Y-\\alpha X)^2)\\]we can do the calculation\\[\\begin{align}E((Y-\\alpha X)^2) &amp;= E(Y^2) + \\alpha^2 E(X^2) - 2 \\alpha E (XY)\\end{align}\\]take the derivate w.r.t \\(\\alpha\\)\\[2\\alpha E(X^2) - 2 E(XY) = 0\\]thus\\[\\alpha_{opt} = \\dfrac{E(XY)}{E(X^2)}\\]A quick note here, in this course we typically assume random variables on \\(\\mathbb{R}\\). If to deal with random variable on \\(\\mathbb{C}\\), it will be something similar to \\(\\dfrac{E(X \\overline{Y})}{E(X^2)}\\). Example 01 FinishReview inner product: an inner product \\(\\langle x, y \\rangle \\to \\mathbb{R}\\) if and only if it satisfies   \\(\\langle x, y \\rangle = \\langle y, x \\rangle\\)   \\(\\langle x, x \\rangle \\ge 0\\)   \\(\\langle x, x \\rangle = 0 \\quad \\implies \\quad x = 0\\)   \\(\\langle x, \\alpha y + \\beta z \\rangle = \\alpha \\langle x, y \\rangle + \\beta \\langle x, z \\rangle\\)   \\(\\langle \\alpha x + \\beta y, z \\rangle = \\alpha \\langle x, z \\rangle + \\beta \\langle y, z \\rangle\\) We can calculate the angle between two vectors\\[\\cos \\theta = \\dfrac{\\langle x, y \\rangle}{\\sqrt{\\langle x, x \\rangle \\langle y, y \\rangle}}\\]Is it always well defined? Inner product has Cauchy-Schwarz\\[\\vert \\langle x, y \\rangle \\vert \\le \\sqrt{\\langle x, x\\rangle \\langle y, y \\rangle}\\]Let’s quickly prove Cauchy-Schwarz. We can define a function\\[g(\\lambda) = \\langle \\lambda x + y, \\lambda x + y \\rangle = \\lambda^2 \\langle x,x \\rangle + 2 \\lambda \\langle x, y \\rangle + \\langle y, y \\rangle \\ge 0\\]it is a quadratic equation with 1 or zero real root. Thus its discriminate \\(b^2 - 4 ac &lt; 0\\)\\[(2\\langle x, y \\rangle )^2 - 4 \\langle x, x \\rangle \\langle y, y \\rangle \\le 0\\]it gives\\[\\vert \\langle x, y \\rangle \\vert \\le \\sqrt{\\langle x, x\\rangle \\langle y, y \\rangle}\\]We can verify \\(E(XY)\\) is an inner product. There may have some concern with rule 3.We can somehow view random variable as vector, and the angle is calculated as\\[\\cos \\theta = \\dfrac{E(XY)}{\\sqrt{E(X^2)E(Y^2)}}\\]If \\(\\theta = 0\\), means the two random variable are complete linear correlated. If \\(\\theta = \\pi/2\\), the two random variable are not linear correlated. If \\(\\theta = \\pi\\), they are complete inverse linear correlated.The formal definition of correlation coefficient shows the linear relationship between two random variables.\\[\\rho_{X,Y} = \\dfrac{E((X-\\mu_X)(Y-\\mu_Y))}{\\sigma_{X} \\sigma_{Y}}\\]in this course most random variabel is zero mean, then\\[\\rho_{X,Y} = \\dfrac{E(XY)}{\\sqrt{E(X^2)E(Y^2)}} = \\cos \\theta\\]we can do example 01 again using the view of vector\\[\\begin{align}\\alpha_{opt} &amp;= \\dfrac{\\vert \\vert Y \\vert \\vert \\cdot \\cos \\theta}{\\vert \\vert X \\vert \\vert}\\\\&amp;= \\dfrac{\\sqrt{E(Y^2)} \\cdot \\dfrac{E(XY)}{\\sqrt{E(X^2)E(Y^2)}}}{\\sqrt{E(X^2)}}\\\\&amp;= \\dfrac{E(XY)}{E(X^2)}\\end{align}\\]Stochastic ProcessStochastic process is many (possibly infinite) random variables with index.For example, we denote \\(X(t)\\) as a stochastic process, meaning \\(\\forall t_0 \\in R\\), \\(X(t_0)\\) is a random variable.Auto-correlation function: \\(R_X(t,s) = E(X(t)X(s))\\).Auto-covariance function: \\(K_X(t,s) = E((X(t)-\\mu_t) (X(s) - \\mu_s))\\)Stationary: some statistic property doesn’t change with time.There are many different kind of stationary. One popular one is wide-sense stationary   \\(E(X(t)) = m\\)   \\(R_X(t,s) = R_X (t+d, s+d)\\) Example 02The stochasitc process \\(X(t) = A(t) \\cos( \\omega_0 t + \\theta)\\), and \\(A, \\theta\\) are independent. And \\(\\theta \\sim U(0, 2\\pi)\\).\\[\\begin{align}E(X(t)) &amp;= E(A(t)) E(\\cos(\\omega_0 t + \\theta)) = E(A(t)) \\cdot 0 = 0\\end{align}\\]\\[\\begin{align}R_X(t,s) &amp;= E(X(t)X(s)) \\\\&amp;= E(A(t)\\cos(\\omega_0 t + \\theta) A(s) \\cos(\\omega_0 t + \\theta))\\\\&amp;= E(A(t)A(s)) \\cdot E(\\cos(\\omega_0 t + \\theta)\\cos(\\omega_0 s + \\theta))\\\\&amp;= \\dfrac{1}{2} \\cdot E(A(t)A(s)) \\cdot (E(\\cos(\\omega_0 (t+s) + 2 \\theta)) + E(\\cos(\\omega_0 (t-s))))\\\\&amp;= \\dfrac{1}{2} \\cdot E(A(t)A(s)) \\cdot \\cos(\\omega_0 (t-s))\\end{align}\\]So it is W.S.S. if \\(A(t)\\) is W.S.S." }, { "title": "Noise Bandwidth", "url": "/posts/noise-bandwidth/", "categories": "math", "tags": "math", "date": "2023-03-20 05:00:00 +0000", "snippet": "From TCAS-2017: Design Methodology for Phase-Locked Loops Using Binary (Bang-Bang) Phase Detectors\\[\\begin{align}H_{LP}(s) &amp;= H_0 \\dfrac{1}{1 + \\dfrac{s}{\\omega_0 Q} + \\dfrac{s^2}{\\omega_0^2}}\\\\\\mathrm{NBW}_{LP} &amp;= \\dfrac{\\omega_0 Q}{4} (\\mathrm{Hz})\\end{align}\\]\\[\\begin{align}H_{BP}(s) &amp;= H_{max} \\dfrac{\\dfrac{s}{\\omega_0 Q}}{1 + \\dfrac{s}{\\omega_0 Q} + \\dfrac{s^2}{\\omega_0^2}}\\\\\\mathrm{NBW}_{BP} &amp;= \\dfrac{\\omega_0}{4Q} (\\mathrm{Hz})\\end{align}\\]ProofUsing the result from book Theory of Servomechanisms" }, { "title": "Discrete Time Signals", "url": "/posts/discrete-time-signals/", "categories": "math", "tags": "math", "date": "2023-03-07 05:00:00 +0000", "snippet": "\\[H(s) = \\mathrm{tf(a,b)} = \\dfrac{a_0 s^m + a_1 s^{m-1} + \\dots + a_m}{b_0 s^n + b_1 s^{n-1} + \\dots + b_n}\\]\\[H(z) = \\mathrm{tf(a,b,T_s)} = \\dfrac{a_0 z^m + a_1 z^{m-1} + \\dots + a_m}{b_0 z^n + b_1 z^{n-1} + \\dots + z_n}\\]All the signals has sampling rate \\(F_s\\)The white noise with zero mean and \\(\\sigma^2\\) variance, it has PSD\\[S(f) = \\dfrac{\\sigma^2}{F_s / 2}\\]Discrete-Time Integration\\[\\dfrac{1}{s} \\approx T_s \\cdot \\dfrac{z^{-1}}{1-z^{-1}} = \\dfrac{T_s}{z - 1}\\]\\[\\dfrac{1}{1-z^{-1}} \\approx \\dfrac{F_s}{s}\\]\\[\\bigg\\vert \\dfrac{1}{1-e^{-j2\\pi f / F_s}} \\bigg\\vert^2 \\approx \\bigg\\vert \\dfrac{1}{2\\pi f / F_s} \\bigg\\vert^2\\]\\[Y_k = Y_{k-1} + X_k, \\quad \\forall k \\ge 1\\]and\\[Y_0 = X_0\\]\\[\\begin{align}Y_{k-1} &amp;= Y_{k-2} + X_{k-1}\\\\A_k &amp;= Y_{k-1}\\\\A_{k-1} &amp;= Y_{k-2}\\end{align}\\]thus\\[A_{k} = A_{k-1} + X_{k-1}, \\quad \\forall k \\ge 1\\]and\\[A_0 = 0\\]" }, { "title": "Paper A Charge-Sharing Locking Technique With a General Phase Noise Theory of Injection Locking", "url": "/posts/paper-a-charge-sharing-locking-technique-with-a-general-phase-noise-theory/", "categories": "analog-circuit", "tags": "analog, pll", "date": "2023-01-10 05:00:00 +0000", "snippet": "From Robert Bogdan Staszewski Group’s PaperPhase Noise of Charges-Sharing LockingTimestamps of Oscillator and Reference\\[\\begin{align}t_{ref}[n] &amp;= n T_{ref} + \\Delta t_{ref}[n]\\\\t_{osc}[k] &amp;= k T_{osc} + \\Delta t_{osc}[k] = \\sum_{m=1}^{k}(T_{osc} + \\Delta T_{osc}[m])\\\\T_{ref} &amp;= N T_{osc} \\iff z_{ref}^{-1} = z_{osc}^{-N} \\iff z_{osc}^{-1} = z_{ref}^{-1/N}\\end{align}\\]To get the spectrum from \\(z\\)-transform, we can substitude\\[\\begin{align}z_{ref} &amp;= e^{j2\\pi \\Delta f / f_{ref}}\\\\z_{osc} &amp;= e^{j2\\pi \\Delta f / f_{osc}}\\end{align}\\]Downsampling of Oscillator TimestampsTo interface a high sampling-rate (i.e., \\(f_{osc}\\)) domain with a low sampling-rate (i.e., \\(f_{ref}\\)) domain, a downsampling operation is required.1\\[\\begin{align}\\widehat{T}_{osc,down}(z_{ref}) &amp;= \\dfrac{1}{N} \\sum_{m=0}^{N-1} \\widehat{T}_{osc}(z_{ref}^{1/N} e^{-j \\mspace{2mu} 2\\pi \\mspace{2mu} m / N})\\\\&amp;= \\dfrac{1}{N} \\sum_{m=0}^{N-1} \\widehat{T}_{osc}(z_{osc} e^{-j \\mspace{2mu} 2\\pi \\mspace{2mu} m / N})\\end{align}\\]The PN of \\(t_{osc,down}[n]\\) can be calculated by taking the square of \\(\\vert \\widehat{T}_{osc,down} \\vert\\) and normalizing it into phase down as2\\[\\begin{align}\\mathcal{L}_{osc,down}(z_{ref}) &amp;= \\dfrac{1}{N^2} \\sum_{m=0}^{N-1} \\mathcal{L}_{osc} (z_{osc} \\mspace{1mu} e^{-j \\mspace{2mu} 2\\pi \\mspace{2mu} m f_{ref}/f_{osc}})\\\\\\mathcal{L}_{osc,down}(\\Delta f) &amp;= \\dfrac{1}{N^2} \\sum_{m=0}^{N-1} \\mathcal{L}_{osc}(\\Delta f - m f_{ref})\\end{align}\\]Oscillator PN \\(\\mathcal{L}_{osc} (z_{osc})\\) can be modeled as\\[\\begin{align}\\mathcal{L}_{osc} (z_{osc}) &amp;= \\dfrac{(2\\pi \\sigma_{\\Delta T,osc} / T_{osc})^2}{f_{osc}} \\cdot \\Big\\vert \\dfrac{1}{1-z_{osc}^{-1}} \\Big\\vert^2\\\\\\mathcal{L}_{osc} (\\Delta f) &amp;= \\dfrac{(2\\pi \\sigma_{\\Delta T,osc} / T_{osc})^2}{f_{osc}} \\cdot \\Big\\vert \\dfrac{1}{1-z_{osc}^{-1}} \\Big\\vert^2\\end{align}\\]where \\(\\sigma_{\\Delta T, osc}\\) is the rms value of \\(\\Delta T_{osc[k]}\\), the instantaneous period jitter of the oscillator.For example, \\(\\sigma_{\\Delta T,osc} = 1 \\mathrm{fs}, f_{osc} = 10 \\, \\mathrm{GHz}\\) gives \\(\\mathcal{L}_{osc}\\) being \\(-140 \\, \\mathrm{dBc/Hz}\\) at \\(\\Delta f = 10 \\, \\mathrm{MHz}\\).Upsampling and ZOH of Reference Timestamps\\[\\begin{align}\\widehat{T}_{ref,up-zoh}(z_{osc}) &amp;= \\dfrac{1-z_{ref}^{-1}}{1-z_{osc}^{-1}} \\cdot \\widehat{T}_{ref} (z_{ref})\\\\\\mathcal{L}_{ref,up-zoh}(\\Delta f) &amp;= \\Big\\vert \\dfrac{1-z_{osc}^{-N}}{1-z_{osc}^{-1}} \\Big\\vert^2 \\cdot \\mathcal{L}_{ref}(\\Delta f)\\end{align}\\]Neglecting the flicker PN, the reference’s instaneous absolute jitter, \\(\\Delta t_{ref}[n]\\), is normally distributed with zero-mean and standard deviation of \\(\\sigma_{\\Delta t,ref}\\) (i.e., its rms jitter and the rms value of \\(\\Delta t_{ref}[n]\\)).Thus, the reference’s PN could be modeled by the PSD of the Gaussian noise as\\[\\mathcal{L}_{ref}(\\Delta f) = \\dfrac{(2\\pi \\sigma_{\\Delta t,ref}/T_{ref})^2}{f_{ref}}\\]Frequency- and Phase-Controlled OscillatorsMultirate Timestamp Model of CSL (or ILO)At the reference detector side\\[\\beta(t_{ref}[n] - t_{out,down,uncorr}[n]) = \\Delta t_{corr}[n] = t_{corr}[n] - t_{corr}[n-1]\\]\\[\\beta (\\widehat{T}_{ref} (z_{ref}) - z_{ref}^{-1} \\, \\widehat{T}_{corr} (z_{ref}) - \\widehat{T}_{osc,down} (z_{ref})) = \\widehat{T}_{corr} (z_{ref}) - z_{ref}^{-1} \\, \\widehat{T}_{corr} (z_{ref})\\]\\[\\begin{equation*} \\widehat {T}_{\\text {corr}}(z_{\\text {ref}})= \\frac {\\beta \\left ({\\widehat {T}_{\\text {ref}}(z_{\\text {ref}})-\\widehat {T}_{\\text {osc,down}}(z_{\\text {ref}})}\\right)}{1-(1-\\beta)z_{\\text {ref}}^{-1}}. \\tag{28}\\end{equation*}\\]where\\[\\begin{align}\\widehat{T}_{osc,down}(z_{ref}) &amp;= \\dfrac{1}{N} \\sum_{m=0}^{N-1} \\widehat{T}_{osc}(z_{osc}\\, e^{-j \\mspace{2mu} 2\\pi \\mspace{2mu} m f_{ref}/f_{osc}})\\end{align}\\]At the oscillator output side\\[\\begin{equation*} t_{\\text {out}}[k] = t_{\\text {corr,up-zoh}}[k] + t_{\\text {osc}}[k] \\tag{29}\\end{equation*}\\]\\[\\begin{align}\\widehat{T}_{out} (z_{osc}) =&amp; \\dfrac {1-z_{ref}^{-1}} {1- z_{osc}^{-1}} \\cdot \\widehat{T}_{corr} (z_{ref}) + \\widehat{T}_{osc}(z_{osc})\\\\=&amp; \\dfrac {1-z_{ref}^{-1}} {1- z_{osc}^{-1}} \\cdot \\dfrac{\\beta}{1-(1-\\beta)z_{ref}^{-1}} \\bigg( \\widehat{T}_{ref}(z_{ref}) - \\widehat{T}_{osc,down}(z_{ref}) \\bigg) + \\widehat{T}_{osc}(z_{osc})\\\\ \\widehat{T}_{out} (z_{osc}) =&amp; \\dfrac {1-z_{ref}^{-1}} {1- z_{osc}^{-1}} \\cdot \\dfrac{\\beta}{1-(1-\\beta)z_{ref}^{-1}} \\cdot \\widehat{T}_{ref}(z_{ref})\\\\&amp; + \\bigg( 1 - \\dfrac{1}{N} \\dfrac {1-z_{ref}^{-1}} {1- z_{osc}^{-1}} \\cdot \\dfrac{\\beta}{1-(1-\\beta)z_{ref}^{-1}} \\bigg) \\cdot \\widehat{T}_{osc} (z_{osc})\\\\&amp; - \\dfrac{1}{N} \\dfrac {1-z_{ref}^{-1}} {1- z_{osc}^{-1}} \\cdot \\dfrac{\\beta}{1-(1-\\beta)z_{ref}^{-1}} \\sum_{m=1}^{N-1} \\widehat{T}_{osc}(z_{osc}\\, e^{-j \\mspace{2mu} 2\\pi \\mspace{2mu} m f_{ref}/f_{osc}})\\end{align}\\]the phase noise\\[\\begin{align*}&amp;\\hspace {-23pt}\\mathcal {L}_{\\text {out}}(z_{\\text {osc}}) = \\left |{\\frac {1}{N}\\frac {1-z_{\\text {ref}}^{-1}}{1-z_{\\text {osc}}^{-1}}\\frac {\\beta }{1-(1-\\beta)z_{\\text {ref}}^{-1}}}\\right |^{2}~N^{2} \\mathcal {L}_{\\text {ref}}(z_{\\text {ref}}) \\\\&amp;\\hspace {-20pt}+\\left |{1\\!-\\!\\frac {1}{N}\\frac {1\\!-\\!z_{\\text {ref}}^{-1}}{1-z_{\\text {osc}}^{-1}}\\frac {\\beta }{1\\!-\\!(1\\!-\\!\\beta)z_{\\text {ref}}^{-1}}}\\right |^{2}\\mathcal {L}_{\\text {osc}}(z_{\\text {osc}}) \\\\&amp;\\hspace {-20pt}+\\left |{\\frac {1}{N}\\frac {1\\!-\\!z_{\\text {ref}}^{-1}}{1\\!-\\!z_{\\text {osc}}^{-1}}\\frac {\\beta }{1\\!-\\!(1\\!-\\!\\beta)z_{\\text {ref}}^{-1}}}\\right |^{2}\\sum _{m=1}^{N-1}\\mathcal {L}_{\\text {osc}}(z_{\\text {osc}}e^{-j2\\pi mf_{\\text {ref}}/f_{\\text {osc}}}) \\tag{22}\\end{align*}\\]Assuming \\(\\mathcal{L}_{ref} = 0\\)Assuming \\(\\mathcal{L}_{osc} = 0\\)Total output jitter\\[\\sigma_{out} = T_{sample} \\sqrt{\\dfrac{1}{2\\pi^2} \\int_{0}^{f_{sample}/2} \\mathcal{L}_{out}(f) df}\\]Footnote The downsampling operation could model the edge removal or frequency division in divider-based Plls, a sub-sampling operation in divider-less PLLs (e.g., ADPLLs or SS-PLLs), and CSL/IL. Intrinsically, there should be no fundamental difference between the divider-based PLLs or divider-less PLLs. &#8617; The corss products of two \\(\\widehat{T}_{osc}\\) at different harmonics of \\(f_{ref}\\) could be practically neglected as compared to the self-squares. &#8617; " }, { "title": "PLL 02", "url": "/posts/pll-02/", "categories": "analog-circuit", "tags": "analog, pll", "date": "2023-01-09 05:00:00 +0000", "snippet": "Paper Multirate Timestamp Modeling for Ultralow-Jitter Frequency Synthesis: A Tutorial High-bandwidth PLL (\\(\\ge 30 \\% f_{ref}\\)). ADPLL with TDC and digitial loop filter. Injection locking or charge-sharing locking oscillator.(a) Conceptual multirate timestamp model suitable for a high-bandwidth PLL. (b) Implementation details of the ADPLL blocks (ADC-based TDC and digital loop filter) that convert \\(\\Delta t_{err}[n]\\) into \\(\\Delta T_{corr}[n]\\).Multirate timestamp model of CSL or IL oscillators.Paper A Comprehensive Phase Noise Analysis of Bang-Bang Digital PLLs Bang-bang DPLL.Bang-bang DPLL multirate time-domain model.Linearized time-variant bang-bang DPLL model.Paper A multiple-crystal interface PLL with VCO realignment to reduce phase noise Phase realignment (injection) PLL. MDLL It exhibits quite a good agreement when the frequency ratio \\(N\\) is large (e.g., \\(N &gt; 10\\)) and the realigning factor (i.e., injection strength) \\(\\beta\\) is small (e.g., \\(\\beta &lt; 0.2\\)), but it cannot predict the phase noise folding.Paper A Charge-Sharing Locking Technique With a General Phase Noise Theory of Injection Locking" }, { "title": "Laplace Transforms", "url": "/posts/laplace-transform/", "categories": "math", "tags": "fourier-analysis", "date": "2023-01-03 05:00:00 +0000", "snippet": " Heaviside step function\\[u(t) = \\begin{cases}1 &amp; \\text{ for } \\quad t &gt; 0,\\\\0 &amp; \\text{ for } \\quad t \\le 0\\end{cases}\\] \\(H(s)\\) \\(h(t)\\) \\(\\text{init cond}\\)1 \\(1/s\\) \\(u(t)\\) - \\(1/s^2\\) \\(r(t) = t \\cdot u(t)\\) - \\(\\dfrac{1}{s+a}\\) \\(e^{-at} \\cdot u(t)\\) \\(A\\cdot e^{-at}\\) \\(\\dfrac{1}{s}\\cdot\\dfrac{1}{s+a}\\) \\(\\dfrac{1}{a} [1-e^{-at}] u(t)\\) - \\(\\dfrac{s^2}{s^2+b^2}\\) - - \\(\\dfrac{s}{s^2+b^2}\\) \\(\\cos bt \\cdot u(t)\\) - \\(\\dfrac{bs}{(s+a)^2+b^2}\\) - - \\(\\dfrac{b}{(s+a)^2+b^2}\\) \\(e^{-at} \\sin bt \\cdot u(t)\\) - Responses With Initial ConditionFor transfer function \\(H(s)\\), input \\(X(s)\\) (usually step input \\(X(s) = 1/s\\)).The full response with initial condition \\(A\\) is\\[y(t) = F^{-1}(H(s)X(s)) + A \\cdot ic(t)\\] Initial condition function only depends on the denominator of the transfer function. Such function can be added to the response with appropriate coefficient, to obtain the correct initial condition. &#8617; " }, { "title": "Measure Theory 04", "url": "/posts/measure-theory-04/", "categories": "math", "tags": "math", "date": "2023-01-01 05:00:00 +0000", "snippet": "Caratheodory’s Extension TheoremWe already know given a \\(\\sigma\\)-additive \\(\\mu: \\mathscr{S} \\to [0,\\infty]\\), there is an unique extension to an algebra with \\(\\sigma\\)-additive \\(\\nu: \\mathscr{A}(\\mathscr{S}) \\to [0,\\infty]\\), with \\(\\nu(A) = \\mu(A) \\forall A \\in \\mathscr{A}(\\mathscr{S})\\), in fact, the set function is\\[\\nu(E) = \\sum_{j=1}^{n}\\mu(E_j)\\]where \\(E_j \\in \\mathscr{S}\\) and they are disjoint.Next we will prove, given a \\(\\sigma\\)-additive \\(\\nu: \\mathscr{A} \\to [0,\\infty]\\), if \\(\\Omega\\) is \\(\\nu\\) \\(\\sigma\\)-finite, then there is an unique extension to an \\(\\sigma\\)-algebra \\(\\pi: \\mathscr{F}(\\mathscr{A}) \\to [0,\\infty]\\), with \\(\\pi(A) = \\nu(A) \\forall A \\in \\mathscr{A}\\).We would first define a set funtion on the whole \\(\\mathscr{P}(\\Omega)\\), called \\(\\pi^{*}: \\mathscr{P}(\\Omega) \\to \\mathbb{R}_+ \\cup \\{\\infty\\}\\). we can show \\(\\pi^{*}\\) is an outer-measure, although it is not \\(\\sigma\\)-additive. then we will define a set \\(\\mathscr{M} \\subseteq \\mathscr{P}(\\Omega)\\), such that it satisfies \\(\\mathscr{M}\\) is an \\(\\sigma\\)-algebra and \\(\\mathscr{M} \\supseteq \\mathscr{A}\\) it satisfies \\(\\pi^* \\vert_{\\mathscr{M}}\\) is \\(\\sigma\\)-additive it satisfies \\(\\pi^* \\vert_{\\mathscr{A}(\\mathscr{S})} = \\nu\\) does \\(\\mathscr{M} = \\mathscr{F}(\\mathscr{A})\\)? No, we can see example that \\(\\mathscr{M}\\) is strictly bigger than \\(\\mathscr{F}(\\mathscr{A})\\) Outer MeasureA set function \\(\\mu: \\mathscr{C} \\to \\mathbb{R}_+ \\cup \\{\\infty\\}\\) is an outer measure if and only if(i) \\(\\mu(\\emptyset) = 0\\)(ii) \\(E \\subseteq F, E, F \\in \\mathscr{C} \\quad \\implies \\quad \\mu(E) \\le \\mu(F)\\)(iii) \\(\\mu\\) is countable monotone: if \\(\\{E_j\\}_{j=1}^{\\infty}\\) is a countable covering of \\(E\\), then \\(\\mu(E) \\le \\sum_{j=1}^{\\infty} \\mu(E_j)\\).Step 1We define a set function on the whole \\(\\mathscr{P}(\\Omega)\\), namely \\(\\pi^*: \\mathscr{P}(\\Omega) \\to [0,\\infty]\\)\\[\\pi^*(A) = \\inf_{\\{E_i\\}} \\sum_{i \\ge 1} \\nu(E_i)\\]where \\(\\{E_i\\}_{i \\ge 1}\\) is a countable covering of \\(A\\) in \\(\\mathscr{A}(\\mathscr{S})\\), i.e., \\(E_i \\in \\mathscr{A}(\\mathscr{S})\\) and \\(A \\subseteq \\cup_{i \\ge 1} E_i\\).And the function \\(\\pi^*\\) is defined as the inf value for all possible covering of \\(A\\).Proof: \\(\\pi^* : \\mathscr{P}(\\Omega) to [0,1]\\) is an outer measure.Step 2Define a set \\(\\mathscr{M}\\) (measurable subset). If \\(A \\in \\mathscr{M}\\), then \\(\\forall E \\subseteq \\Omega, \\pi^*(E) = \\pi^*(E \\cap A) + \\pi^*(E \\cap A^c)\\). we can prove it satisfies \\(\\mathscr{M}\\) is an \\(\\sigma\\)-algebra and \\(\\mathscr{M} \\supseteq \\mathscr{A}(\\mathscr{S})\\) Thus \\(\\mathscr{M} \\supseteq \\mathscr{F}(\\mathscr{S})\\). Step 3Consider \\(\\pi^* \\vert_{\\mathscr{M}}: \\mathscr{M} \\to [0,\\infty]\\). it satisfies \\(\\pi^* \\vert_{\\mathscr{M}}\\) is \\(\\sigma\\)-additive it satisfies \\(\\pi^* \\vert_{\\mathscr{A}(\\mathscr{S})} = \\nu\\) Step 4For any two \\(\\mu_1, \\mu_2: \\mathscr{F}(\\mathscr{A}) \\to [0,\\infty]\\), if \\(\\mu_1 \\vert_{\\mathscr{A}} = \\mu_2 \\vert_{\\mathscr{A}}\\), and if \\(\\Omega\\) is \\(\\sigma\\)-finite on \\(\\mu_1 \\vert_{\\mathscr{A}} (\\text{or } \\mu_2 \\vert_{\\mathscr{A}})\\), then\\[\\mu_1 = \\mu_2\\]\\(\\sigma\\)-FiniteIf \\(\\Omega\\) is \\(\\sigma\\)-finite on \\(\\mu: \\mathscr{C} \\to [0,\\infty]\\), it means\\[\\exists \\text{ countable } \\{E_j\\}_{j=1}^{\\infty}, E_j \\in \\mathscr{C}, E_j \\uparrow \\Omega, \\text{ and } \\mu(E_j) &lt; \\infty \\forall j &lt; \\infty\\]Monotone ClassDefine, \\(\\mathscr{G} \\subseteq \\mathscr{P}(\\Omega)\\) is a monotone class if and only of(i) \\(A_j \\in \\mathscr{G} \\forall j \\ge 1\\) and \\(A_j \\subseteq A_{j+1} \\quad \\implies \\quad \\cup_{j \\ge 1}A_j \\in \\mathscr{G}\\).(ii) \\(B_j \\in \\mathscr{G} \\forall j \\ge 1\\) and \\(B_j \\supseteq B_{j+1} \\quad \\implies \\quad \\cap_{j \\ge 1}B_j \\in \\mathscr{G}\\).Observations: \\(\\mathscr{G}_{\\alpha}\\) is monotone class, where \\(\\alpha \\in I\\) and \\(I\\) is any index set. Then \\(\\cap_{\\alpha \\in I} \\mathscr{G}_{\\alpha}\\) is a monotone class. So we can talk about the smallest monotone class \\(\\mathscr{M}(\\mathscr{C})\\) generated by a class \\(\\mathscr{C}\\) (contain \\(\\mathscr{C}\\)). LemmaGiven algebra \\(\\mathscr{A} \\subseteq \\mathscr{P}(\\Omega)\\), then\\[\\mathscr{M}(\\mathscr{A}) = \\mathscr{F}(\\mathscr{A})\\]SummaryGiven \\(\\nu: \\mathscr{A} \\to [0,\\infty]\\) and \\(\\Omega\\) is \\(\\nu\\) \\(\\sigma\\)-finite.Then there is an unique extantion \\(\\pi: \\mathscr{F}(\\mathscr{A}): [0,\\infty]\\) with \\(\\pi(A) = \\nu(A) \\forall A \\in \\mathscr{A}\\).We define\\[\\begin{align}\\pi^*&amp;: \\mathscr{P}(\\Omega) \\to [0,\\infty]\\\\\\pi^*(A) &amp;= \\inf_{\\{E_i\\}} \\sum_{i \\ge 1} \\nu(E_i)\\end{align}\\]where \\(\\{E_i\\}\\) is covering of \\(A\\).Then\\[\\pi = \\pi^* \\vert_{\\mathscr{F}(\\mathscr{A})}\\]" }, { "title": "Measure Theory 03", "url": "/posts/measure-theory-03/", "categories": "math", "tags": "math", "date": "2022-12-31 05:00:00 +0000", "snippet": "Set FunctionsAdditiveGiven \\(\\mathscr{C} \\subseteq \\mathscr{P}(\\Omega)\\) and \\(\\emptyset \\in \\mathscr{C}\\).The function \\(\\mu: \\mathscr{C} \\to \\mathbb{R}_+ \\cup \\{\\infty\\}\\) is additive, if and only if(i) \\(\\mu(\\emptyset) = 0\\)(ii) if disjoint \\(E_1, E_2, \\dots, E_n \\in \\mathscr{C}\\), and \\(E = \\sum_{j=1}^{n}E_j \\in \\mathscr{C}\\), then \\(\\mu(E) = \\sum_{j=1}^n \\mu(E_j)\\).We can natually define a set function on the semi-algebra like \\((a,b]\\).We want to extend the set function to algebra and \\(\\sigma\\)-algebra.\\(\\sigma\\)-AdditiveGiven \\(\\mathscr{C} \\subseteq \\mathscr{P}(\\Omega)\\) and \\(\\emptyset \\in \\mathscr{C}\\).The function \\(\\mu: \\mathscr{C} \\to \\mathbb{R}_+ \\cup \\{\\infty\\}\\) is \\(\\sigma\\)-additive, if and only if(i) \\(\\mu(\\emptyset) = 0\\)(ii) if disjoint \\(E_1, E_2, \\dots, E_j, \\dots \\in \\mathscr{C}\\), and \\(E = \\sum_{j\\ge 1}E_j \\in \\mathscr{C}\\), then \\(\\mu(E) = \\sum_{j\\ge 1} \\mu(E_j)\\).CoverSet \\(E\\) is covered by a collection \\(\\{E_k\\}_{k=1}^{n}\\) means\\[E \\subseteq \\cup_{k=1}^{n} E_k\\]Finite MonotoneA set function \\(\\mu: \\mathscr{C} \\to [0,\\infty]\\) is said to be finite monotone provided that, whenever a set \\(E \\in \\mathscr{C}\\) is covered by a finite collection \\(\\{E_k\\}_{k=1}^{n}\\) of sets in \\(\\mathscr{C}\\)\\[\\mu(E) \\le \\sum_{k=1}^{n}\\mu(E_k)\\]Countable Monotone (subadditivity)A set function \\(\\mu: \\mathscr{C} \\to [0,\\infty]\\) is said to be countable monotone provided that, whenever a set \\(E \\in \\mathscr{C}\\) is covered by a countable collection \\(\\{E_k\\}_{k=1}^{\\infty}\\) of sets in \\(\\mathscr{C}\\)\\[\\mu(E) \\le \\sum_{k=1}^{\\infty}\\mu(E_k)\\]Continuous MeasureGiven \\(\\mathscr{C} \\subseteq \\mathscr{P}(\\Omega)\\), and \\(\\mu: \\mathscr{C} \\to \\mathbb{R}_+ \\cup \\{\\infty\\}\\). for \\(E \\in \\mathscr{C}\\), \\(\\mu\\) continuous from below at \\(E\\), if and only if \\(\\forall (E_n)_{n \\ge 1}, E_n \\in \\mathscr{C}, E_n \\uparrow E\\), then \\(\\mu(E_n) \\uparrow \\mu(E)\\). for \\(E \\in \\mathscr{C}\\), \\(\\mu\\) continuous from above at \\(E\\), if and only if \\(\\forall (E_n)_{n \\ge 1}, E_n \\in \\mathscr{C}, E_n \\downarrow E\\) and \\(\\exists n_0, \\mu(E_{n_0}) &lt; \\infty\\), then \\(\\mu(E_n) \\downarrow \\mu(E)\\). the need of \\(\\mu(E_{n_0})\\), consider the example of \\(E_n = (n, \\infty)\\), then \\(E_n \\downarrow \\emptyset\\). Lemma: \\(\\mathscr{A} \\subseteq \\mathscr{P}(\\Omega)\\) is an algebra, then for an additive \\(\\mu: \\mathscr{A} \\to \\mathbb{R}_+ \\cup \\{\\infty\\}\\). \\(\\mu\\) is \\(\\sigma\\)-additive \\(\\quad \\implies \\quad\\) \\(\\mu\\) continuous at \\(E , \\forall E \\in \\mathscr{A}\\). \\(\\mu\\) is continuous from below \\(\\quad \\implies \\quad\\) \\(\\mu\\) is \\(\\sigma\\)-additive. \\(\\mu\\) is continuous from above at \\(\\emptyset\\) and \\(\\mu\\) is finite (\\(\\mu(\\Omega) &lt; \\infty\\)) \\(\\quad \\implies \\quad\\) \\(\\mu\\) is \\(\\sigma\\)-additive. Measure ExtantionThere exists only one unique extantion from \\(\\mathscr{S}\\) to \\(\\mathscr{A}(\\mathscr{S})\\).Theorem 1: \\(\\mathscr{S} \\subseteq \\mathscr{P}(\\Omega)\\) is a semi-algebra, \\(\\mu: \\mathscr{S} \\to \\mathbb{R}_+ \\cup \\{\\infty\\}\\) is additive. Then there exists \\(\\nu: \\mathscr{A}(\\mathscr{S}) \\to \\mathbb{R}_+ \\cup \\{\\infty\\}\\) is additive, and(a) \\(\\nu(A) = \\mu(A) \\forall A \\in \\mathscr{S}\\)(b) \\(\\nu_1(A) = \\nu_2(A) \\forall A \\in \\mathscr{S} \\quad \\implies \\nu_1(E) = \\nu_2(E) \\forall E \\in \\mathscr{A}(\\mathscr{\\mathscr{S}})\\)Theorem 2: \\(\\mathscr{S} \\subseteq \\mathscr{P}(\\Omega)\\) is a semi-algebra, \\(\\mu: \\mathscr{S} \\to \\mathbb{R}_+ \\cup \\{\\infty\\}\\) is \\(\\sigma\\)-additive. Then there exists \\(\\nu: \\mathscr{A}(\\mathscr{S}) \\to \\mathbb{R}_+ \\cup \\{\\infty\\}\\) is \\(\\sigma\\)-additive, and(a) \\(\\nu(A) = \\mu(A) \\forall A \\in \\mathscr{S}\\)(b) \\(\\nu_1(A) = \\nu_2(A) \\forall A \\in \\mathscr{S} \\quad \\implies \\nu_1(E) = \\nu_2(E) \\forall E \\in \\mathscr{A}(\\mathscr{\\mathscr{S}})\\)Using \\(\\mathscr{A}(\\mathscr{S}) = \\{ \\sum_{j=1}^n E_j, \\forall \\text{ disjoint } E_j \\in \\mathscr{S}, \\forall 1 \\le n &lt; \\infty\\}\\), the set function extention is in fact\\[\\nu(E) = \\sum_{j=1}^n \\mu(E_j)\\]" }, { "title": "Measure Theory 02", "url": "/posts/measure-theory-02/", "categories": "math", "tags": "math", "date": "2022-12-30 05:00:00 +0000", "snippet": "Classes of SubsetsSemi-AlgebrasDefinition: \\(\\mathscr{S} \\subseteq \\mathscr{P}(\\Omega)\\) is a semi-algebra, if and only if:(i) \\(\\Omega \\in \\mathscr{S}\\)(ii) \\(A, B \\in \\mathscr{S} \\implies A \\cap B \\in \\mathscr{S}\\)(iii) \\(\\forall A \\in \\mathscr{S}, \\exists E_1, \\dots, E_h \\in \\mathscr{S}\\), such that \\(A^c = \\sum_{j=1}^h E_j\\).Example: \\(\\Omega = \\mathbb{R}\\).Then semi-algebra \\(\\mathscr{S}\\) includes\\[\\begin{align}&amp;\\mathbb{R}, \\emptyset\\\\&amp; \\{(a,b], a &lt; b, a,b \\in \\mathbb{R}\\}\\\\&amp; \\{(-\\infty, b], b \\in \\mathbb{R}\\}\\\\&amp; \\{(a, \\infty), a \\in \\mathbb{R}\\}\\end{align}\\]Example: \\(\\Omega = (0,1)\\), the semi-algebra \\(\\mathscr{S}\\) includes\\[\\begin{align}&amp;(0,1), \\emptyset, \\{(a,b], 0 \\le a &lt; b &lt; 1\\}\\end{align}\\]AlgebrasDefinition: \\(\\mathscr{A} \\in \\mathscr{P}(\\Omega)\\) is an algebra if and only if(i) \\(\\Omega \\in \\mathscr{A}\\)(ii) \\(A,B \\in \\mathscr{A} \\implies A \\cap B \\in \\mathscr{A}\\)(iii) \\(A \\in \\mathscr{A} \\implies A^c \\in \\mathscr{A}\\)Observation: If \\(\\mathscr{A}\\) is an algebra, then \\(\\mathscr{A}\\) is semi-algebra. [assertion 01]. \\(A, B \\in \\mathscr{A} \\implies A \\cup B \\in \\mathscr{A}\\), since \\(A \\cup B = (A^c \\cap B^c)^c\\). If \\(\\mathscr{A}_{\\alpha} \\subseteq \\mathscr{P}(\\Omega)\\) is algebra for each \\(\\alpha \\in I\\), where \\(I\\) can be any index set (it can be uncountable).Then \\(\\cap_{\\alpha \\in I} \\mathscr{A}_{\\alpha}\\) is an algebra. [assertion 02] Algebra Generated By The Class \\(\\mathscr{C}\\)The algebra generabted by \\(\\mathscr{C} \\subseteq \\mathscr{P}(\\Omega)\\) is defined as \\(\\mathscr{A}(\\mathscr{C})\\), such that(i) \\(\\mathscr{C} \\subseteq \\mathscr{A}(\\mathscr{C})\\)(ii) For any algebra \\(\\mathscr{B}, \\mathscr{B} \\supseteq \\mathscr{C} \\implies \\mathscr{B} \\supseteq \\mathscr{A}(\\mathscr{C})\\), i.e., \\(\\mathscr{A}(\\mathscr{C})\\) is the smallest algebra containing \\(\\mathscr{C}\\).\\[\\cap_{\\alpha} \\mathscr{A}_{\\alpha}\\] TODO: question, this is similar to infimum, but can we be sure the infimum exists like in the real number? For example, we take all the interval such that there exists element \\(a &lt; 0\\) and \\(b &gt; 1\\). Then does \\(\\cap_{\\alpha} A_{\\alpha}\\) exists?   \\(a \\in A_{\\alpha} \\forall \\alpha \\quad \\implies \\quad a \\in \\cap_{\\alpha} A_{\\alpha}\\)   \\(\\exists \\alpha, a \\notin A_{\\alpha}, \\quad \\implies a \\notin \\cap_{\\alpha} A_{\\alpha}\\). Godel’s incompleteness theorems. Lemma: \\(\\mathscr{S} \\subseteq \\mathscr{P}(\\Omega)\\) is a semi-algebra. Then\\[A \\in \\mathscr{A}(\\mathscr{S}) \\quad \\iff \\quad \\exists \\text{ disjoint } E_j \\in \\mathscr{S}, 1 \\le j \\le n, \\text{ such that } A = \\sum_{j=1}^{n} E_j\\]or it is all the finite unions of disjoint sets.\\[\\mathscr{A}(\\mathscr{S}) = \\{ \\sum_{j=1}^n E_j, \\forall \\text{ disjoint } E_j \\in \\mathscr{S}, \\forall n = 1, 2, 3, \\dots\\}\\]\\(\\sigma\\)-AlgebrasDefinition: \\(\\mathscr{F} \\subseteq \\mathscr{P}(\\Omega)\\) is \\(\\sigma\\)-algebra if and only if(i) \\(\\Omega \\in \\mathscr{F}\\)(ii) \\(A_j \\in \\mathscr{F} \\implies \\cap_{j\\ge 1} A_j \\in \\mathscr{F}\\)(iii) \\(A \\in \\mathscr{F} \\implies A^c \\in \\mathscr{F}\\)Observation: If \\(\\mathscr{F}\\) is a \\(\\sigma\\)-algebra, then \\(\\mathscr{F}\\) is algebra, \\(\\mathscr{F}\\) is semi-algebra. \\(A_j \\in \\mathscr{F} \\implies \\cup_{j \\ge 1} A_j \\in \\mathscr{F}\\), since \\(\\cup_{j \\ge 1} A_j = (\\cap_{j \\ge 1} A_j^c)^c\\) If \\(\\mathscr{F}_{\\alpha} \\subseteq \\mathscr{P}(\\Omega)\\) is \\(\\sigma\\)-algebra for each \\(\\alpha \\in I\\), where \\(I\\) can be any index set (it can be uncountable).Then \\(\\cap_{\\alpha \\in I} \\mathscr{F}_{\\alpha}\\) is a \\(\\sigma\\)-algebra. \\(\\sigma\\)-Algebra Generated By The Class \\(\\mathscr{C}\\)\\(\\mu\\) FunctionAdditiveGiven \\(\\mathscr{C} \\subseteq \\mathscr{P}(\\Omega)\\) and \\(\\emptyset \\in \\mathscr{C}\\).The function \\(\\mu: \\mathscr{C} \\to \\mathbb{R}_+ \\cup \\{\\infty\\}\\) is additive, if and only if(i) \\(\\mu(\\emptyset) = 0\\)(ii) if disjoint \\(E_1, E_2, \\dots, E_n \\in \\mathscr{C}\\), and \\(E = \\sum_{j=1}^{n}E_j \\in \\mathscr{C}\\), then \\(\\mu(E) = \\sum_{j=1}^n \\mu(E_j)\\).\\(\\sigma\\)-AdditiveGiven \\(\\mathscr{C} \\subseteq \\mathscr{P}(\\Omega)\\) and \\(\\emptyset \\in \\mathscr{C}\\).The function \\(\\mu: \\mathscr{C} \\to \\mathbb{R}_+ \\cup \\{\\infty\\}\\) is \\(\\sigma\\)-additive, if and only if(i) \\(\\mu(\\emptyset) = 0\\)(ii) if disjoint \\(E_1, E_2, \\dots, E_j, \\dots \\in \\mathscr{C}\\), and \\(E = \\sum_{j\\ge 1}E_j \\in \\mathscr{C}\\), then \\(\\mu(E) = \\sum_{j\\ge 1} \\mu(E_j)\\).Example: Additive But Not \\(\\sigma\\)-AdditiveLet \\(\\Omega = (0,1), \\mathscr{C} = {(a,b], 0 \\le a &lt; b &lt; 1}\\)\\[\\mu((a,b]) = \\begin{cases}\\infty, &amp; a = 0 \\\\b-a, &amp; a &gt; 0\\end{cases}\\]Proofs Assertion 01: If \\(\\mathscr{A}\\) is an algebra, then \\(\\mathscr{A}\\) is semi-algebra.Proof: We can verify \\(\\mathscr{A}\\) is semi-algebra.(i) \\(\\Omega \\in \\mathscr{A}\\)(ii) \\(A, B \\in \\mathscr{A} \\implies A \\cap B \\in \\mathscr{A}\\)(iii) \\(\\forall A \\in \\mathscr{A}, A^c = A^c \\in \\mathscr{A}\\) Assertion 02: If \\(\\mathscr{A}_{\\alpha} \\subseteq \\mathscr{P}(\\Omega)\\) is algebra for each \\(\\alpha \\in I\\), where \\(I\\) is any general index set. Then \\(\\cap_{\\alpha \\in I} \\mathscr{A}_{\\alpha}\\) is an algebra.Proof: We can verify \\(\\cap_{\\alpha} \\mathscr{A}_{\\alpha}\\) is an algebra(i) \\(\\mathscr{A}_{\\alpha}\\) is an algebra \\(\\implies \\Omega \\in \\mathscr{A}_{\\alpha} \\implies \\Omega \\in \\cap_{\\alpha} \\mathscr{A}_{\\alpha}\\)(ii) for any \\(A, B \\in \\cap_{\\alpha} \\mathscr{A}_{\\alpha}\\), we know \\(A, B \\in \\mathscr{A}_{\\alpha} \\forall \\alpha\\), then \\(A \\cap B \\in \\mathscr{A}_{\\alpha} \\forall \\alpha\\), then \\(A \\cap B \\in \\cap_{\\alpha} \\mathscr{A}_{\\alpha}\\)(iii) similar to above \\(A \\in \\cap_{\\alpha} \\mathscr{A}_{\\alpha} \\implies A^c \\in \\cap_{\\alpha} \\mathscr{A}_{\\alpha}\\)" }, { "title": "Measure Theory 01", "url": "/posts/measure-theory-01/", "categories": "math", "tags": "math", "date": "2022-12-30 04:00:00 +0000", "snippet": "Introdution: A Non-Measurable SetCan we define a measure on \\(\\mathbb{R}\\), such that it works on all the subsets of \\(\\mathbb{R}\\)?(i) \\(\\lambda: \\mathfrak{P}(\\mathbb{R}) \\to \\mathbb{R}_+ \\cup \\{\\infty\\}\\)(ii) \\(\\lambda((a,b)) = \\lambda([a,b]) = b-a\\)(iii) \\(\\lambda(A+x) = \\lambda(\\{a_i + x, a_i \\in A\\}) = \\lambda(A)\\)(iv) \\(\\lambda(\\cup_{j\\ge 1} A_j) = \\sum_{j \\ge 1} \\lambda (A_j)\\)It is impossible to define such a measure on all the subsets. We define \\(x \\sim y \\quad \\iff \\quad y - x \\in \\mathbb{Q}\\). Define \\([x] = \\{ y \\in \\mathbb{R}, y - x \\in \\mathbb{Q}\\}\\). then \\(\\Lambda = \\mathbb{R} \\vert_{\\sim}\\). then using the axiom of choice, from each family in \\(\\Lambda\\), we choose one representitive to form \\(\\Omega\\), it is possible to make \\(\\Omega \\subseteq \\mathbb{R}\\) and \\(\\Omega \\subseteq (0,1)\\).For example, \\(\\Omega = \\{\\dfrac{1}{2}, \\dfrac{\\sqrt{2}}{2}, \\dfrac{\\sqrt{3}}{2}, \\cdots\\}\\). Then it is easy to show if \\(p, q \\in \\mathbb{Q}\\) and \\(p \\ne q\\), we have \\((\\Omega + p) \\cap (\\Omega + q) = \\emptyset\\) For set \\(\\sum_{q\\in \\mathbb{Q}, -1 &lt; q &lt; 1} (\\Omega + q)\\) first, \\(\\sum_{q \\in \\mathbb{Q}, -1&lt; q &lt;1} (\\Omega + q) \\subseteq (-1,2)\\), then \\(\\lambda(\\Omega) = 0\\). second, \\((0,1) \\subseteq \\sum_{q\\in \\mathbb{Q}, -1&lt;q&lt;1} (\\Omega + q)\\), then \\(\\lambda(\\Omega) \\ne 0\\). Set OperationsThe following set operations are valid, or shown as identity equation   \\(A \\cup B = (A^c \\cap B^c)^c\\)   \\(\\cap_{\\alpha \\in I} A_{\\alpha}\\), where \\(I\\) is any index set (e.g., it can be uncountable) " }, { "title": "Probability: Theory and Examples 01", "url": "/posts/probability-theory-and-examples-01/", "categories": "math", "tags": "math", "date": "2022-12-28 10:00:00 +0000", "snippet": "From book_probability_theory_and_examplesMeasure TheoryProbability SpacesHere and throughout the book, terms being defined are set in boldface.Here and in what follows, countable means finite or countably infinite.\\(\\sigma\\)-field(or \\(\\sigma\\)-algebra) \\(\\mathcal{F}\\) is a (nonempty) collection of subsets of \\(\\Omega\\) that satisfy(i) if \\(A \\in \\mathcal{F}\\) then \\(A^c \\in \\mathcal{F}\\), and(ii) if \\(A_i \\in \\mathcal{F}\\) is a countable sequence of sets then \\(\\cup_i A_i \\in \\mathcal{F}\\).Since \\(\\cap_i A_i = (\\cup_i A_i^c)^c\\), it follows that a \\(\\sigma\\)-field is closed under countable intersections.\\((\\Omega, \\mathcal{F})\\) is called a measurable space, i.e., it is a space on which we can put a measure.A measure is a nonnegative countably additive set function; that is, a function \\(\\mu: \\mathcal{F} \\to \\mathbf{R}\\) with(i) \\(\\mu(A) \\ge \\mu(\\emptyset)\\) for all \\(A \\in \\mathcal{F}\\), and(ii) if \\(A_i \\in \\mathcal{F}\\) is a countable sequence of disjoint sets, then\\[\\mu(\\cup_i A_i) = \\sum_{i}\\mu(A_i)\\]If \\(\\mu(\\Omega) = 1\\), we call \\(\\mu\\) a probability measure.In this book, probability measures are usually denoted by \\(P\\).Theorem 1.1.1.Let \\(\\mu\\) be a measure on \\((\\Omega,\\mathcal{F})\\)(i) monotonicity. If \\(A \\subset B\\) then \\(\\mu(A) \\le \\mu(B)\\).(ii) subadditivity. If \\(A \\subset \\cup_{m=1}^{\\infty} A_m\\) then \\(\\mu(A) \\le \\sum_{m=1}^{\\infty} \\mu(A_m)\\).(iii) continuity from below. If \\(A_i \\uparrow A\\) (i.e., \\(A_1 \\subset A_2 \\subset \\dots\\) and \\(\\cup_i A_i = A\\)) then \\(\\mu(A_i) \\uparrow \\mu(A)\\).(iv) continuity from above. If \\(A_i \\downarrow A\\) (i.e., \\(A_1 \\supset A_2 \\supset \\dots\\) and \\(\\cap_i A_i = A\\)), with \\(\\mu(A_1) &lt; \\infty\\) then \\(\\mu(A_i) \\downarrow \\mu(A)\\).Proof.(i) Let \\(B - A = B \\cap A^c\\) be the difference of the two sets.Using \\(+\\) to denote disjoint union, \\(B = A + (B-A)\\) so\\[\\mu(B) = \\mu(A) + \\mu(B-A) \\ge \\mu(A)\\]Lemma.Let \\(I \\ne \\emptyset\\) is an arbitrary index set (i.e., possibly uncountable).If \\(\\mathcal{F}_i, i \\in I\\) are \\(\\sigma\\)-fieldm then \\(\\cap_{i\\in I} \\mathcal{F}_i\\) is \\(\\sigma\\)-field.Proof.(i) If \\(A \\in \\cap_{i \\in I} \\mathcal{F}_i\\), then \\(A \\in \\mathcal{F}_i, \\forall i\\).It follows \\(A^c \\in \\mathcal{F}_i, \\forall i\\), and \\(A^c \\in \\cap_{i\\in I} \\mathcal{F}_i\\).(ii) Similarily if \\(A_j \\in \\cap_{i\\in I}\\mathcal{F}\\) is a countable sequence of sets, then \\(\\cup_{j} A_j \\in \\cap_{i \\in I} \\mathcal{F}\\).Frorm this it follows that if we are given a set \\(\\Omega\\) and a collection \\(\\mathcal{A}\\) of subsets of \\(\\Omega\\), then there is a smallest \\(\\sigma\\)-field containing \\(\\mathcal{A}\\).We will call this the \\(\\sigma\\)-field generated by \\(\\mathcal{A}\\) and denote it by \\(\\sigma(A)\\).Let \\(\\mathcf{R}^d\\) btthe set of vectors \\((x_1,\\dots,x_d)\\) of real numbers" }, { "title": "Topics in The Theory of Random Noise 01", "url": "/posts/topics-in-the-theory-of-random-noise-01/", "categories": "math", "tags": "math", "date": "2022-12-20 10:00:00 +0000", "snippet": "From book_topics_in_the_theory_of_random_noiseChapter 1: Random Functions and Their Statistical Characteristics1. Random VariablesA random variable $\\xi$ should have definition mean of any function \\(\\langle f(\\xi)\\rangle_{\\xi}\\)\\[\\langle\\xi\\rangle_{\\xi} = \\lim_{n\\to\\infty} \\dfrac{\\xi_1 + \\xi_2 + \\dots + \\xi_n}{n} = \\lambda()\\]\\[\\langle\\xi^2\\rangle_{\\xi} = \\lim_{n\\to\\infty} \\dfrac{\\xi_1^2 + \\xi_2^2 + \\dots + \\xi_n^2}{n} = \\lambda()\\]\\[\\langle f(\\xi)\\rangle_{\\xi} = \\lim_{n\\to\\infty} \\dfrac{f(\\xi_1) + f(\\xi_2) + \\dots + f(\\xi_n)}{n} = \\lambda()\\]\\[\\langle f(x,\\xi)\\rangle_{\\xi} = \\lim_{n\\to\\infty} \\dfrac{f(x,\\xi_1) + f(x,\\xi_2) + \\dots + f(x,\\xi_n)}{n} = \\lambda(x)\\] Derivitive: \\(\\dfrac{d}{dx}\\langle f(x,\\xi)\\rangle_{\\xi} = \\langle \\dfrac{\\partial}{\\partial x} f(x,\\xi)\\rangle_{\\xi}\\)\\[\\begin{align}&amp; \\dfrac{d}{dx} \\langle f(x,\\xi)\\rangle_{\\xi} = \\lim_{\\Delta x \\to 0}\\dfrac{\\langle f(x + \\Delta x,\\xi)\\rangle_{\\xi} - \\langle f(x,\\xi)\\rangle_{\\xi}}{\\Delta x}\\\\=&amp; \\lim_{\\Delta x \\to 0} \\dfrac{\\lim_{n\\to\\infty} \\dfrac{f(x+\\Delta x,\\xi_1) + \\dots + f(x+\\Delta x, \\xi_n)}{n} - \\lim_{n\\to\\infty} \\dfrac{f(x,\\xi_1)+\\dots+f(x,\\xi_n)}{n}}{\\Delta x}\\\\= &amp; \\lim_{\\Delta x \\to 0} \\Big[ \\lim_{n\\to\\infty}\\dfrac{f(x+\\Delta x,\\xi_1)+\\dots + f(x+\\Delta x, \\xi_n)}{n\\Delta x} - \\lim_{n\\to\\infty} \\dfrac{f(x,\\xi_1)+\\dots + f(x,\\xi_n)}{n\\Delta x} \\Big]\\\\= &amp; \\lim_{\\Delta x \\to 0} \\lim_{n\\to\\infty} \\dfrac{(f(x+\\Delta x,\\xi_1)-f(x,\\xi_1)) + \\dots + (f(x+\\Delta x,\\xi_n)-f(x,\\xi_n))}{n\\Delta x}\\\\\\doteq &amp; \\lim_{n\\to\\infty}\\lim_{\\Delta x\\to 0} \\dfrac{(f(x+\\Delta x,\\xi_1)-f(x,\\xi_1)) + \\dots + (f(x+\\Delta x,\\xi_n)-f(x,\\xi_n))}{n\\Delta x}\\\\= &amp; \\lim_{n\\to\\infty} \\dfrac{\\dfrac{\\partial}{\\partial x}f(x,\\xi_1)+\\dots \\dfrac{\\partial}{\\partial x}f(x,\\xi_n)}{n}\\\\= &amp; \\langle \\dfrac{\\partial}{\\partial x}f(x,\\xi)\\rangle_{\\xi}\\end{align}\\] Integration: \\(\\int_{a}^{b}\\langle f(x,\\xi)\\rangle_{\\xi}dx = \\langle\\int_{a}^{b} f(x,\\xi)dx\\rangle_{\\xi}\\) Assume we have a random variable \\(\\eta\\), but it is actually fully determined by another random variable \\(\\xi\\), \\(\\eta = g(\\xi)\\). \\[\\begin{align}&amp;\\langle f(\\eta)\\rangle_{\\eta} = \\lim_{n\\to\\infty} \\dfrac{f(\\eta_1)+\\dots+f(\\eta_n)}{n} = \\lim_{n\\to\\infty} \\dfrac{f(g(\\xi_1))+\\dots+f(g(\\xi_n))}{n}\\\\=&amp; \\langle f(g(\\xi))\\rangle_{\\xi}\\end{align}\\]so, if \\(\\eta = g(\\xi)\\), then \\(\\langle f(\\eta)\\rangle_{\\eta} = \\langle f(g(\\xi))_{\\xi}\\), or \\(\\langle f(g(\\xi))\\rangle_{g(\\xi)} = \\langle f(g(\\xi))\\rangle_{\\xi}\\)if we define function\\[\\theta(z) = \\begin{cases}1 &amp; \\text{ for } \\quad z &gt; 0,\\\\0 &amp; \\text{ for } \\quad z \\le 0\\end{cases}\\]Then we have distribution function\\[P\\{\\xi &lt; x\\} = F_{\\xi}(x) = \\langle\\theta(x-\\xi)\\rangle_\\xi\\]and the probability density function\\[\\begin{align}w_\\xi(x) &amp;= \\dfrac{d}{dx} F_\\xi(x) = \\dfrac{d}{dx}\\langle\\theta(x-\\xi)\\rangle_\\xi = \\langle\\dfrac{\\partial}{\\partial x}\\theta(x-\\xi)\\rangle_\\xi = \\langle\\delta(x-\\xi)\\rangle_\\xi\\end{align}\\]this can be used to calculate cany \\(\\langle f(\\xi)\\rangle\\)\\[\\begin{align}\\langle f(\\xi)\\rangle_{\\xi} &amp;= \\langle \\int_{-\\infty}^{\\infty} f(x)\\delta(x-\\xi)dx \\rangle_{\\xi} = \\int_{-\\infty}^{\\infty}\\langle f(x)\\delta(x-\\xi)\\rangle_{\\xi} dx\\\\&amp;= \\int_{-\\infty}^{\\infty} f(x)\\langle\\delta(x-\\xi)\\rangle_{\\xi} dx = \\int_{-\\infty}^{\\infty} f(x)w_{\\xi}(x)dx\\end{align}\\]Next, we show how the probability density behaves under transformations of the original random variable \\(\\xi\\).Let the new randon variable \\(\\eta\\) be defined by\\[\\eta = g(\\xi)\\]then\\[\\begin{align}w_{\\eta}(x) &amp;= \\langle\\delta(x-\\eta)\\rangle_{\\eta} = \\langle\\delta(x-g(\\xi))\\rangle_{\\xi}\\\\&amp;= \\int_{-\\infty}^{\\infty} \\delta(x-g(\\xi)) w_{\\xi}(\\xi)d\\xi\\end{align}\\]If \\(g(\\xi)\\) is monotone, it has a unique inverse function \\(\\xi = g^{-1}(\\eta)\\)\\[\\begin{align}w_{\\eta}(x) &amp;= \\int_{-\\infty}^{\\infty} \\delta(x-g(g^{-1}(\\eta)))w_{\\xi}(g^{-1}(\\eta))d(g^{-1}(\\eta))\\\\&amp;= \\int_{-\\infty}^{\\infty} \\delta(x-\\eta)w_{\\xi}(g^{-1}(\\eta)) \\dfrac{dg^{-1}(\\eta)}{d\\eta}d\\eta\\\\&amp;= w_{\\xi}(g^{-1}(x))\\dfrac{d g^{-1}(\\eta)}{d\\eta}\\bigg|_{\\eta=x}\\\\&amp;= w_{\\xi}(\\xi) \\cdot (\\dfrac{dg}{d\\xi})^{-1}\\bigg|_{\\xi=g^{-1}(x)}\\end{align}\\]If there are multiple \\(g(\\xi_i) = x\\)\\[w_{\\eta}(x) = \\sum_{i}\\Bigg[ w_{\\xi}(\\xi_i)\\cdot (\\dfrac{dg}{d\\xi})\\bigg|_{\\xi=\\xi_i} \\Bigg]\\]Define characteristic function\\[\\Theta_{\\xi}(u) = \\langle e^{iu\\xi}\\rangle_{\\xi} = \\int e^{iu\\xi}w(\\xi)d\\xi\\]this is similar to Fourier transform with only a sign different\\[w(\\xi) = \\dfrac{1}{2\\pi}\\int \\Theta_{\\xi}(u)e^{-iu\\xi}du\\]The moments \\(m_n = \\langle \\xi^n\\rangle_{\\xi}\\) can be calculated as\\[\\begin{align}&amp;\\dfrac{1}{i^n}\\dfrac{d^n \\Theta_{\\xi}(u)}{du^n}\\bigg|_{u=0} = \\dfrac{1}{i^n} \\langle \\dfrac{d^n}{du^n} e^{iu\\xi} \\rangle_{\\xi}\\bigg|_{u=0}\\\\=&amp; \\dfrac{1}{i^n}\\langle i^n \\xi^n e^{iu\\xi}\\rangle_{\\xi}\\bigg|_{u=0} = \\langle \\xi^n\\rangle_{\\xi}\\end{align}\\]we can write \\(\\Theta_{\\xi}(u)\\) as a Maclaurin series:\\[\\Theta_\\xi(u) = 1 + \\sum_{n=1}^{\\infty} \\dfrac{(iu)^n}{n!}m_n\\]For a variaty of reasons, it is more convenient to characterize a random variable not by its moments, but by its cumulants (or semi-invariants) \\(k_n\\), defined by the relation\\[\\Theta_{\\xi}(u) = \\exp \\bigg\\{ \\sum_{n=0}^{\\infty} \\dfrac{(iu)^n}{n!}k_n \\bigg\\}\\]using \\(e^x = 1 + x + \\dfrac{x^2}{2!} + \\dfrac{x^3}{3!} + \\dots\\), comparing the two equations, we have, for example\\[\\Theta_{\\xi}(u) = 1 + \\dfrac{iu}{1!}k_1 + \\dfrac{(iu)^2}{2!}k_2 + \\dfrac{(iu)^2}{2}k_1^2\\]we can obtain the following formulas relating the moments and the cumulants:\\[\\begin{align}k_1 &amp;= m_1\\\\k_2 &amp;= m_2 - m_1^2\\\\k_3 &amp;= m_3 - 3m_1 m_2 + 2m_1^2\\\\k_4 &amp;= m_4 - 3m_2^2 - 4m_1 m_3 + 12 m_1^2 m_2 - 6m_1^4\\end{align}\\]it shows\\[k_2 = \\langle (\\xi-\\langle\\xi\\rangle)^2\\rangle, \\quad k_3 = \\langle (\\xi-\\langle\\xi\\rangle)^3\\rangle\\]The quantity \\(k_2 = \\langle\\xi^2\\rangle - \\langle\\xi\\rangle^2\\) is called the variance (or dispersion) of \\(\\xi\\), and is denoted by\\[\\mathbf{D}\\xi = \\langle \\xi^2\\rangle - \\langle\\xi\\rangle^2\\]There are also defition for standard deviation\\[\\sigma(\\xi) = [\\mathbf{D}\\xi]^{1/2}\\]2. Correlation Between Random VariablesSuppose we have \\(r\\) random variables \\(\\xi_1, \\dots,\\xi_r\\), they are fully depends on a unknown random variable \\(\\zeta\\)\\[\\begin{align}\\xi_1 &amp;= g_1(\\zeta)\\\\\\xi_2 &amp;= g_2(\\zeta)\\\\&amp; \\dots \\\\\\xi_r &amp;= g_r(\\zeta)\\end{align}\\]then\\[\\langle f(\\xi_1,\\dots,\\xi_n)\\rangle_{\\zeta} = \\lim_{n\\to\\infty} \\dfrac{f(g_1(\\zeta_1),\\dots,g_r(\\zeta_1))+\\dots + f(g_1(\\zeta_n)+\\dots + g_r(\\zeta_n))}{n}\\]The random variables \\(\\xi_1,\\dots,\\xi_r\\) are completely characterized by the (joint) \\(r\\)-dimensional probability density\\[w_{\\xi_1,\\dots,\\xi_r}(x_1,\\dots,x_r) = \\langle \\delta(\\xi_1-x_1)\\dots \\delta(\\xi_r - x_r) \\rangle\\]the mean value can be calculated as\\[\\langle f(\\xi_1,\\dots,\\xi_r) \\rangle = \\int \\dots \\int f(\\xi_1,\\dots,\\xi_r) w(\\xi_1,\\dots,\\xi_r) d\\xi_1 \\dots d\\xi_r\\]In the absence of knowledge of the actual realization (i.e., observed value) of \\(\\xi_2\\), the random variable \\(\\xi_1\\) has the probility density\\[w(\\xi_1) = \\int w(\\xi_1,\\xi_2)d \\xi_2\\]but\\[w(\\xi_1 | \\xi_2) = \\dfrac{w(\\xi_1,\\xi_2)}{\\int w(\\xi_1,\\xi_2)d\\xi_1}\\]If no information about \\(\\xi_1\\) is gained, regardless of the observed value of \\(\\xi_2\\), which means\\[\\begin{align}w(\\xi_1 | \\xi_2) &amp;= w(\\xi_1)\\\\w(\\xi_1, \\xi_2) &amp;= w(\\xi_1)w(\\xi_2)\\end{align}\\]If \\(\\xi_1\\) and \\(\\xi_2\\) are completely correlated\\[w(\\xi_1,\\xi_2) = \\delta[\\xi_1 - f(\\xi_2)]w(\\xi_2)\\]In general\\[\\begin{align}w(\\xi_1,\\dots,\\xi_k | \\xi_{k+1},\\dots,\\xi_r) &amp;= \\dfrac{w(\\xi_1,\\dots,\\xi_r)}{w(\\xi_{k+1},\\dots,\\xi_r)}\\\\w(\\xi_1,\\dots,\\xi_k) &amp;= \\int \\cdots \\int w(\\xi_1,\\dots,\\xi_r)d \\xi_{k+1}\\dots d\\xi_{r}\\end{align}\\]The covariance (synonymously, cross correlation or double correlation) describe the degree of statistical dependence between random variables.\\[\\mathbf{K}[\\xi_1,\\xi_2] = \\langle \\xi_1 \\xi_2 \\rangle - \\langle \\xi_1 \\rangle \\langle \\xi_2 \\rangle\\]If \\(\\xi_1\\) and \\(\\xi_2\\) are independent, we have\\[\\begin{align}\\langle \\xi_1 \\xi_2 \\rangle &amp;=\\langle \\xi_1 \\rangle\\langle \\xi_2 \\rangle\\mathbf{K}[\\xi_1,\\xi_2] &amp;= 0\\end{align}\\]By definition, the covariance of a random variable with itself is just its variance\\[\\mathbf{K}[\\xi,\\xi] = \\mathbf{D}\\xi\\]Sometimes, instead of \\(\\mathbf{K}[\\xi_1,\\xi_2]\\)m it is convenient to consider the dimensionless quantity\\[R_2 \\equiv R = \\dfrac{\\mathbf{K}[\\xi_1,\\xi_2]}{\\sigma(\\xi_1)\\sigma(\\xi_2)}\\]The quantity reduces to unity when the random variables coincide, and is called the correlation coefficient of \\(\\xi_1\\) and \\(\\xi_2\\).Besides double correlatoins, there are multiple correlations of higher orders. For example,\\[\\langle \\xi_1\\xi_2\\xi_3 \\rangle - \\langle \\xi_1 \\rangle\\langle \\xi_2 \\rangle\\langle \\xi_3 \\rangle\\]But the triple correlation is defined as\\[\\begin{align}\\mathbf{K}[\\xi_1,\\xi_2,\\xi_3] =\\langle \\xi_1 \\xi_2 \\xi_3 \\rangle -\\langle \\xi_1 \\rangle \\mathbf{K}[\\xi_2,\\xi_3] -\\langle \\xi_2 \\rangle \\mathbf{K}[\\xi_1,\\xi_2] -\\langle \\xi_3 \\rangle \\mathbf{K}[\\xi_1,\\xi_2] -\\langle \\xi_1 \\rangle\\langle \\xi_2 \\rangle\\langle \\xi_3 \\rangle\\end{align}\\]Chatacteristic function\\[\\begin{align}\\Theta(u_1,\\dots,u_r) &amp;=\\langle \\exp i(u_1 \\xi_1 + \\dots + u_r \\xi_r) \\rangle\\\\\\langle \\xi_1,\\dots,\\xi_r \\rangle &amp;= \\dfrac{1}{i^r} \\dfrac{\\partial^r \\Theta(u_1,\\dots,r_r)}{\\partial u_1 \\dots \\partial u_r} \\bigg|_{u_1 = \\dots = u_r = 0}\\\\\\mathbf{K}[\\xi_1,\\dots,\\xi_r] &amp;= \\dfrac{1}{i^r}\\dfrac{\\partial^r \\ln \\Theta(u_1,\\dots,u_r)}{\\partial u_1 \\dots \\partial u_r} \\bigg|_{u_1=\\dots=u_r=0}\\end{align}\\]3. Random FunctionsNext, we consider a random function \\(\\xi(t)\\) of a single real argument \\(t\\) (the time) which varies over the interval \\([0, T]\\), say.If we take \\(r\\) fixed values \\(t_1,\\dots,t_r\\) from the interval \\([0,T]\\), then the values \\(\\xi(t_1),\\dots,\\xi(t_r)\\) constitute a family of random variables.\\[w_r(x_1,\\dots,x_r;t_1,\\dots,t_r) = \\langle \\delta[x_1-\\xi(t_1)]\\dots\\delta[x_r-\\xi(t_r)] \\rangle\\]The characteristic functions:\\[\\begin{align}\\Theta(u_1; t_1) &amp;= \\langle e^{i u_1 \\xi(t_1)}\\\\\\Theta(u_1, u_2; t_1, t_2) &amp;= \\langle e^{i u_1 \\xi(t_1)+iu_2\\xi(t_2)} \\rangle\\\\&amp;\\dots\\end{align}\\]to the limit we get characteristic functional\\[\\Theta[u(t)] = \\bigg\\langle \\exp \\bigg[i\\int u(t)\\xi(t) dt\\bigg] \\bigg\\rangle\\]The each characteristic function can be obtained by using\\[u(t) = \\sum u_{\\alpha} \\delta(t-t_{\\alpha})\\]The term random process (or simply process) is used as a synonym for a random function of time.In its general from, a random process cannot be described by a finite number of functions of a finite number of variables, and hence a random process is either characterized by an infinite sequence of functions or by a functional.It is preferable to consider a sequence of functions such that the functions of higher order do not repeat the information contained in the preceding functions, but instead carry only new information.Therefore, we now introduce another method of describing random processes, which has this and other advantages.The mean values\\[\\langle \\xi(t_1)\\dots\\xi(t_r) \\rangle = m_r(t_1,\\dots,t_r)\\]are called moment functions, and the infinite sequence of moment functions\\[m_1(t_1),\\quad m_2(t_1,t_2),\\quad m_3(t_1,t_2,t_3), \\dots\\]gives an exhaustive description of the random process \\(\\xi(t)\\).It is even better to characterize the random process \\(\\xi(t)\\) by the sequence of correlation functions\\[k_1(t_1),\\quad k_2(t_1,t_2),\\quad k_3(t_1,t_2,t_3), \\quad\\dots\\]which are defined as the multiple correlations\\[k_r(t_1,\\dots,t_r) \\equiv \\mathbf{K}[\\xi(t_1),\\dots,\\xi(t_r)]\\]regarded as functions of the time \\(t_1,\\dots,t_r\\).The correlation functions, just like the moment functions, are symmetric functions of their arguments.The vast majority of random processes encountered in radio physics have the property that when the time \\(t_1,\\dots,t_r\\) are moved apart, the correlation between the corresponding values of the process falls off, i.e., the correlation functions goes to zero, which is very convenient.If we know the moment functions or the correlation functions, we can find the other characteristics of a random process, in particular, its characteristic functions or probability densities.Using formula \\(\\langle \\xi_1,\\dots,\\xi_r \\rangle = \\dfrac{1}{i^r} \\dfrac{\\partial^r \\Theta(u_1,\\dots,r_r)}{\\partial u_1 \\dots \\partial u_r} \\bigg\\vert_{u_1 = \\dots = u_r = 0}\\), we can write the characteristic function as a multidimentional Taylor’s series:\\[\\Theta_r(u_1,\\dots,u_r;t_1,\\dots,t_r) = 1 + \\sum_{s=1}^{\\infty} \\dfrac{s!}{i^s}\\sum_{\\alpha,\\dots,\\omega=1}^{r}m_s(t_{\\alpha},\\dots,t_{\\omega})u_{\\alpha}\\dots u_{\\omega}\\]In just the same way\\[\\Theta_r(u_1,\\dots,u_r;t_1,\\dots,t_r) = \\exp \\big\\{\\sum_{s=1}^{\\infty} \\dfrac{i^s}{s!} \\sum_{\\alpha,\\dots,\\omega=1}^{r} k_s(t_{\\alpha,\\dots,t_{\\omega}})u_{\\alpha}\\dots u_{\\omega}\\big\\}\\]and\\[\\Theta[u(t)] = \\exp\\big\\{ \\sum_{s=1}^{\\infty} \\dfrac{i^s}{s!} \\int \\cdots \\int k_s(t_1,\\dots,t_s) u(t_1) \\dots u(t_s) d t_1 \\dots d t_s \\big\\}\\]Because of the special role played by moment functions and correlation functions, we give the following formulas relating them:\\[\\begin{align}m_1(t_1) &amp;= k_1(t_1)\\\\m_2(t_1,t_2) &amp;= k_2(t_1,t_2) + k_1(t_1)k_1(t_2)\\\\m_3(t_1,t_2,t_3) &amp;= k_3(t_1,t_2,t_3) + 3\\{k_1(t_1)k_2(t_2,t_3)\\}_s + k_1(t_1)k_1(t_2)k_1(t_3)\\\\m_4(t_1,t_2,t_3,t_4) &amp;= k_4(t_1,t_2,t_3,t_4) + 3\\{k_2(t_1,t_2)k_2(t_3,t_4)\\}_s + 4\\{k_1(t_1)k_3(t_2,t_3,t_4)\\}_s + 6\\{k_1(t_1)k_1(t_2)k_2(t_3,t_4)\\}_s + k_1(t_1)k_1(t_2)k_1(t_3)k_1(t_4)\\end{align}\\]Here, the symbol \\(\\{\\dots\\}_s\\) denotes the operation of symmetrizing the expression in brackets with respect to all its arguments.It seems like if \\(k_1 = 0\\), then the correlation functions are the same as the central moments.But it is not true. The difference begins with \\(k_4\\)Chapter 2: Stationary Random Processes and Spectral DensitiesA random process is said to be stationary (in the strict sense) if its statistical characteristics are invariant under time shifts, i.e., if they reamin the same when \\(t\\) is replaced by \\(t+a\\), where \\(a\\) is arbitrary.Then, the probability densities \\(w_n(\\xi_1,\\dots,\\xi_n;t_1,\\dots,t_n)\\) together with the moment and correlation functions \\(m_n(t_1,\\dots,t_n)\\) and \\(k_n(t_1,\\dots,t_n)\\) do not depend on the absolute position of the points \\(t_1,\\dots,t_n\\) on the times axis, but only on their relative configuration.\\[k_n(t_1,\\dots,t_n) = k_n(0,t_2-t_1,\\dots,t_n-t_1) = k_n'(t_2-t_1,\\dots,t_n-t_1)\\]for which we introduce the special notation\\[k_2(t_1,t_2) = \\langle \\xi(t_1)\\xi(t_2) \\rangle - \\langle \\xi(t_1) \\rangle \\langle \\xi(t_2) \\rangle = k(t_2-t_1)\\]where \\(k(\\tau) = k(-\\tau)\\).If the process has zero mean\\[k(\\tau) = \\langle \\xi(0)\\xi(\\tau) \\rangle\\]We can also write \\(k(\\tau)\\) in the form\\[k(\\tau) = \\sigma^2 R(\\tau)\\]where \\(\\sigma = \\sigma[\\xi(t)] = \\sqrt{k(0)}\\) is the standard deviation of \\(\\xi(t)\\), and \\(R(\\tau) = k(\\tau)/\\sigma^2\\) is the dimensionless, normalized correlation coefficient, with the property that \\(R(0) = 1\\).Next, we introduce the important concept of the (power) spectral density \\(S[\\xi;\\omega]\\) of a stationary random process \\(\\xi(t)\\):\\[S[\\xi;\\omega] = 2\\int_{-\\infty}^{\\infty} e^{i\\omega \\tau} \\langle \\xi \\xi_\\tau \\rangle d\\tau = 4 \\int_{0}^{\\infty} \\cos \\omega \\tau \\langle \\xi \\xi_\\tau \\rangle d\\tau\\]Here, and subsequently, the subscript \\(\\tau\\) on a function like \\(\\xi_\\tau\\), denotes a shift of the argument by a amount \\(\\tau\\), i.e., \\(\\xi_\\tau = \\xi(t+\\tau)\\).For a process with zero mean value, \\(S[\\xi;\\omega]\\) is the Fourier transform of the correlation function:\\[S[\\xi;\\omega] = 2\\int_{-\\infty}^{\\infty} e^{i\\omega \\tau} k(\\tau) d\\tau\\]In the general case, where \\(\\langle \\xi \\rangle = m\\) is not zero\\[\\begin{align}S[\\xi-m;\\omega] &amp;= 2\\int_{-\\infty}^{\\infty} e^{i\\omega\\tau} k(\\tau)d\\tau\\\\S[\\xi;\\omega] &amp;= S[\\xi-m;\\omega] + 4\\pi m^2 \\delta(\\omega)\\end{align}\\]The inverse formula gives\\[\\sigma^2(\\xi) = k(0) = \\dfrac{1}{2\\pi} \\int_{0}^{\\infty} S[\\xi-m;\\omega]d\\omega\\]" }, { "title": "Oscillator Phase Noise: A Tutorial", "url": "/posts/oscillator-phase-noise-a-tutorial/", "categories": "analog-circuit", "tags": "analog", "date": "2022-12-20 05:00:00 +0000", "snippet": "From Ali Hajimiri’s Paper\\[h_{\\phi}(t,\\tau) = \\dfrac{\\Gamma(\\omega_0 \\tau)}{q_{max}} u(t-\\tau)\\]\\[\\mathcal{L}(\\Delta \\omega) = \\dfrac{\\dfrac{\\overline{i_n^2}}{\\Delta f} \\Gamma_{rms}^2}{2 q_{max}^2 \\Delta \\omega^2}\\]" }, { "title": "Minimum Achievable Phase Noise of RC Oscillators", "url": "/posts/paper-minimum-achievable-phase-noise-of-rc-oscillators/", "categories": "analog-circuit", "tags": "analog", "date": "2022-12-19 05:00:00 +0000", "snippet": "From Reza Navid’s PaperAnalytical Formulation of Phase NoiseTime-Domain Phase Noise Analysis for Switching-Based OscillatorsThe transfer function from \\(I_{n}(s)\\) to \\(V_c(s)\\) is\\[\\dfrac{V_c(s)}{I_n(s)} = R \\,\\Vert\\, \\dfrac{1}{sC} = \\dfrac{\\dfrac{1}{C}}{s + \\dfrac{1}{RC}}\\]it has a impulse response\\[v_c(t) = \\dfrac{1}{C}\\cdot e^{-t/RC}\\]for an initial condition \\(v_c(0)\\), the output will be\\[v_c(0) e^{-t/RC}\\]then for any input \\(i_n(t)\\)\\[v_c(t) = \\int_{0}^{t} i_n(\\tau) \\cdot \\dfrac{1}{C} \\cdot e^{-(t-\\tau)/RC} d\\tau + v_c(0) e^{-t/RC}\\]Since \\(i_n\\) is a Gaussian process with zero mean, this indicates that \\(v_c\\) is also a Gaussian process with zero mean.(TODO: why? see book_stochastic_differential_equations.)Then it can be used to calculate \\(\\overline{v_c^2(t)}\\) and also \\(\\overline{\\Delta T_o^2}\\)Once the period jitter is calculated, phase noise can easily be calculated.In most cases (including relaxation oscillators) the output of the switching oscillators can be approximated by a stochastic square wave signal with mutually independent, Gaussian-distribution period jitter.As presented in book_topics_in_the_theory_of_random_noise_vol_2 (page 160) and also this paper, the phase noise of such a signal has a nearly Lorentzian shape around each harmonic.The phase noise around the first harmonic at an offset frequency of \\(\\Delta f\\) is given by\\[\\mathcal{L}(\\Delta f) = \\dfrac{f_o^3 \\overline{(\\Delta T_o)^2}}{(\\pi f_o^3 \\overline{(\\Delta T_o)^2})^2 + (\\Delta f)^2}\\]where \\(f_o\\) and \\(\\Delta f\\) are center frequency and offset frequency, respectively, and \\(\\overline{(\\Delta T_o)^2}\\) is the variance of the period.Phase Noise in Ring Oscillators\\[\\mathcal{L}_{min}(\\Delta f) = \\dfrac{7.33 kT}{P_{min}} \\Big( \\dfrac{f_o}{\\Delta f} \\Big)^2\\]" }, { "title": "Probability and Stochastic Processes", "url": "/posts/probability-and-stochastic-processes-01/", "categories": "math", "tags": "probability", "date": "2022-12-17 03:00:00 +0000", "snippet": "Stochastic Processes A random variable maps the output of a random experiment, $\\zeta$, to a real number. A stochastic process maps the output of a random experiment, $\\zeta$, to a time domain waveform. Examples of stochastic processes.   \\(X(t) = \\cos(\\omega_o t + \\Phi)\\) where \\(\\Phi \\sim U(-\\pi,\\pi)\\).   \\(X(t) = e^{-Yt}\\) where \\(Y \\sim U(0,b)\\) and \\(t \\ge 0\\). First Order Moments.   \\(E\\{X(t)\\} = E\\{e^{-Yt}\\} = \\int_{0}^b e^{-yt} \\dfrac{1}{b} dy = \\dfrac{1}{bt}(1-e^{-bt})\\)   \\(E\\{X(t)\\} = E\\{\\cos(\\omega_o t + \\Phi)\\} = \\int_{-\\pi}^{\\pi} \\cos(\\omega_o t + \\phi) \\dfrac{1}{2\\pi} d\\phi = 0\\) Autocorrelation (Second order moments): \\(R(t_1, t_2) = E\\{X(t_1) X(t_2)\\}\\) \\[\\begin{align}R(t_1, t_2) &amp;= E\\{X(t_1)X(t_2)\\}\\\\&amp;= E\\{\\cos(\\omega t_1 + \\Phi) \\cos(\\omega t_2 + \\Phi)\\}\\\\&amp;= \\dfrac{1}{2} E\\{\\cos[\\omega(t_1 - t_2)]\\} + \\dfrac{1}{2} E\\{\\cos[\\omega(t_1 + t_2) + 2\\Phi]\\}\\\\&amp;= \\dfrac{1}{2}\\cos[\\omega(t_1 - t_2)]\\end{align}\\]\\[\\begin{align}R(t_1, t_2) &amp;= E\\{X(t_1) X(t_2)\\}\\\\&amp;= E\\{e^{-Y(t_1 + t_2)}\\}\\\\&amp;= \\dfrac{1}{b(t_1+t_2)}\\big[1-e^{-b(t_1 + t_2)}\\big]\\end{align}\\] Auto-covariance: \\(C(t_1, t_2) = R(t_1, t_2) - \\mu(t_1)\\mu(t_2)\\) Correlation coefficient: \\(r(t_1, t_2) = \\dfrac{C(t_1,t_2)}{\\sigma(t_1)\\sigma(t_2)}\\) Stationarity and Ergodicity A process is strict sense stationary (SSS) if its joint PDF and CDF are independent of a shift in the time axis.\\[f(x_1,\\dots,x_n;t_1,\\dots,t_n) = f(x_1,\\dots,x_n;t_1+c,\\dots,t_n+c)\\]Then obviously \\(E\\{X(t)\\} = \\mu, R(t_1, t_2) = R(t_2 - t_1) = R(\\tau)\\) A process is weak sense stationary (WSS) if\\[E\\{X(t)\\} = \\mu, \\quad R(t_1,t_2) = R(t_2 - t_1)\\]White Process A white process means: \\(C(t_1,t_2) = 0\\) for \\(t_1 \\ne t_2\\). A stricly white process means \\(C(t_1, t_2) = q(t_1) \\delta(t_2 - t_1)\\) Gaussian Processes\\[f_{\\mathbf{X}}(\\mathbf{x}) = \\dfrac{1}{\\sqrt{(2\\pi)^{N/2} \\text{det}(\\mathbf{C}_{\\mathbf{X}})}} \\cdot \\exp [-\\dfrac{1}{2}(\\mathbf{x}-\\mu_{X})^{\\text{T}} \\mathbf{C_X}^{-1} (\\mathbf{x}-\\mu_X) ]\\]Ergodicity in the MeanErgodicity in the mean means\\[E\\{X(t)\\} = \\mu_X, \\quad \\mu_X = \\lim_{T\\to\\infty} \\dfrac{1}{2T}\\int_{-T}^{T} X(t) dt\\] The ensemble average has to be a constant that doesn’t dependent on time. Then it coincides with the time average. Slutsky’s Theorem If \\(E\\{X(t)\\} = \\mu_X\\) is a constant.For finite \\(T\\), the average of one stochastic process relization is a random waveform\\[\\mu_{X,T} = \\dfrac{1}{2T} \\int_{-T}^{T} X(t)dt\\]Mean of the random variable\\[E\\{\\mu_{X,T}\\} = \\dfrac{1}{2T} \\int_{-T}^{T} E\\{X(t)\\} dt = \\mu_{X}\\]Variance of the random varibla\\[\\begin{align}\\sigma_{\\mu_{X,T}}^2 &amp;= E\\{(\\mu_{X,T} - \\mu_X)^2\\}\\\\&amp;= E\\bigg\\{ \\Big( \\dfrac{1}{2T} \\int_{-T}^{T} X(\\alpha)d\\alpha -\\mu_X \\Big) \\Big( \\dfrac{1}{2T} \\int_{-T}^{T} X(\\beta)d \\beta -\\mu_X \\Big)\\bigg\\}\\\\&amp;= \\dfrac{1}{4T^2} \\int_{-T}^{T} \\int_{-T}^{T} E\\{[X(\\alpha)-\\mu_X][X(\\beta)-\\mu_X]\\} d\\alpha d\\beta\\\\&amp;= \\dfrac{1}{4T^2} \\int_{-T}^{T} C_X(\\alpha,\\beta) d\\alpha d\\beta\\end{align}\\]Our stochastic process if ergodic in the mean if\\[\\lim_{T \\to \\infty} \\dfrac{1}{4T^2} \\int_{-T}^{T} \\int_{-T}^{T} C_X(t_1, t_2) d t_1 d t_2 = 0\\]If the process is WSS, then it is ergodic in the mean if and only if\\[\\lim_{T \\to \\infty} \\dfrac{1}{2T} \\int_{-T}^{T} C_X(\\tau) d\\tau = 0\\]The proof can be get by google “Slutsky’s Theorem ergodicity” or this link.Example: Ergodicity in the meanConsider a sinusoid with random phase uniformy distributed between \\(\\pi\\) and \\(-\\pi\\), \\(X(t) = \\cos(\\omega t + \\Phi)\\).Then \\(C(\\tau) = \\dfrac{1}{2}\\cos(\\omega \\tau)\\).\\[\\lim_{T\\to\\infty} \\dfrac{1}{2T} \\int_{-T}^{T} C(\\tau) d\\tau = 0\\]Egrodicity of the CovarianceThe covariance time average\\[C_{X,T}(\\tau) = \\dfrac{1}{2T} \\int_{-T}^{T} X(t)X(t+\\tau)d t - \\mu_X^2\\]Spectrum of a Random SignalDeprecatedYoutube Lecture 2 Partition: A partition of a set, \\(S\\), is a finite series of mutually exclusive subsets that completely define \\(S\\) such that\\[S = A_1 + A_2 + \\dots + A_N\\] The Axioms of Probability\\[\\begin{align}P(A) &amp;\\ge 0\\\\P(S) &amp;= 1\\\\\\text{If } AB &amp;= \\emptyset, \\text{ then } P(A \\cup B) = P(A) + P(B)\\end{align}\\] Conditional Probability\\[P(A|M) = \\dfrac{P(AM)}{P(M)}\\] Example: IC Testing ProposalLet \\(G_1\\) and \\(G_2\\) represent chips 1 and 2 being good, respectively.\\[\\begin{align}P(\\{\\overline{G_1} \\,\\overline{G_2}\\}) &amp;= 0.01 \\quad P(\\{G_1 \\overline{G_2}\\}) = 0.01\\\\P(\\{\\overline{G_1} G_2\\}) &amp;= 0.01 \\quad P(\\{G_1 G_2\\}) = 0.97\\end{align}\\]To calculate \\(P(\\{\\overline{G_1}\\,\\overline{G_2}, G_1 \\overline{G_2}\\} \\vert \\{\\overline{G_1}\\,\\overline{G_2}, \\overline{G_1}G_2\\})\\)\\[\\begin{align}\\{\\overline{G_1}\\,\\overline{G_2}, G_1 \\overline{G_2}\\} \\cap \\{\\overline{G_1}\\,\\overline{G_2}, \\overline{G_1}G_2\\} &amp;= \\{\\overline{G_1}\\,\\overline{G_2}\\}\\\\P(\\{\\overline{G_1}\\,\\overline{G_2}, G_1 \\overline{G_2}\\} \\vert \\{\\overline{G_1}\\,\\overline{G_2}, \\overline{G_1}G_2\\}) &amp;= \\dfrac{0.01}{0.02} = 0.5\\end{align}\\] Total Probability TheoremIf \\(A_1, A_2, \\dots, A_N\\) is a partition of the universal set \\(S\\), then\\[P(B) = \\sum_{i} P(B|A_i)P(A_i)\\] Bayes Theoremusing\\[P(AB) = P(BA) = P(A|B)P(B) = P(B|A)P(A)\\]then\\[P(A|B) = \\dfrac{P(B|A)P(A)}{P(B)} = \\dfrac{P(B|A)P(A)}{P(B|A)P(A)+P(B|\\overline{A})P(\\overline{A})}\\]Why this can be useful?Assume the input to the system is either \\(A\\) or \\(\\overline{A}\\), and the output is either \\(B\\) or \\(\\overline{B}\\).The thing is the system includes some random behavior (like noise) such that \\(B\\) is not fully determined by \\(A\\).For a real application, we may have an estimation for \\(P(A)\\) and \\(P(\\overline{A})\\),and we may know the noise model of the system to determine the values such as \\(P(B|A)\\).Then we can use the Bayes Theorem to calculate \\(P(A|B)\\) Example: Disease TestingYou’re designing a test for a disease. \\(D\\), that occurs in 0.5% of the population.If the disease is present, your test alerts the medical staff 97% of the time.If the disease is not present, it also give a false positive 1%.If your machine gives an alert, what’s the probability the person actually has the disease?\\[\\begin{align}P(D) &amp;= 0.005\\\\P(A|D) &amp;= 0.97\\\\P(A|\\overline{D}) &amp;= 0.01\\\\P(A) &amp;= P(A|D)P(D) + P(A|\\overline{D})P(\\overline{D}) = 0.0148\\end{align}\\]then\\[P(D|A) = \\dfrac{P(A|D)P(D)}{P(A)} = 0.328\\] Independence: two events \\(A\\) and \\(B\\) are independent iff\\[P(AB) = P(A)P(B)\\]and thus\\[P(A|B) = P(AB)/P(B) = P(A)\\]For \\(N\\) independent events: \\(P(A_1 A_2 \\cdots A_N) = P(A_1)P(A_2)\\cdots P(A_N)\\)Sometimes two events seems like to influence each others, but they still satisfy the definition of independence.Example: roll two six sided dice and define \\(A = \\{ \\text{rolling a 3 on the first dice} \\}\\) and \\(B = \\{ \\text{ both dice add to 7 } \\}\\).\\[\\begin{align}A &amp;= (3,1), (3,2), (3,3), (3,4), (3,5), (3,6)\\\\B &amp;= (1,6), (2,5), (3,4), (4,3), (5,2), (6,1)\\\\P(A) &amp;= 1/6\\\\P(B) &amp;= 1/6\\\\P(AB) &amp;= 1/36\\end{align}\\]Youtube Lecture 3" }, { "title": "Signal And System Chapter", "url": "/posts/signal-and-system-chapter/", "categories": "math", "tags": "fourier-analysis", "date": "2022-12-11 06:00:00 +0000", "snippet": "Signal and SystemSignalSignal is numbers with index.For example the signal \\(x(t-t_0)\\) actually meansdef signal(t): return x(t-t_0)orsignal = lambda t: x(t-t_0)Linear SystemThe response to \\(x_1(t) + x_2(t)\\) is \\(y_1(t) + y_2(t)\\)Time Invariant SystemThe response to \\(x(t-t_0)\\) is \\(y(t-t_0)\\).Is linear constant coefficient ODE LTI?For example, consider an simple ODE\\[y'' + y' + y = x(t)\\]if we let the system has all zero initial conditions before we apply any signal (initial rest), then it is LTI.We can verify the response to \\(x_1(t) + x_2(t)\\) is \\(y_1(t) + y_2(t)\\)\\[y_1''(t) + y_1'(t) + y_1(t) = x_1(t)\\]\\[y_2''(t) + y_2'(t) + y_2(t) = x_2(t)\\]\\[y_1''(t) + y_2''(t) + y_1'(t)+y_2'(t) + y_1(t) + y_2(t) = x_1(t) + x_2(t)\\]Since both \\(y_1(t)\\) and \\(y_2(t)\\) satisfy the initial rest, zero initial conditin at some time \\(\\tau\\), their addition also satisfy zero inisital consition at \\(\\tau\\).We can also verify the response to \\(x(t-t_0)\\) is \\(y(t-t_0)\\)\\[y''(t) + y'(t) + y(t) = x(t)\\]\\[y''(t-t_0) + y'(t-t_0) + y(t-t_0) = x(t-t_0)\\]before \\(\\tau\\) either \\(x(t)\\) or \\(x(t-t_0)\\) is zero, thus \\(y(t)\\) and \\(y(t-t_0)\\) satisfy the initial condition.$e^{j\\omega t}$ is eigen signal for LTI system" }, { "title": "Signal And System Explain", "url": "/posts/signal-and-system-explain/", "categories": "math", "tags": "fourier-analysis", "date": "2022-12-10 03:00:00 +0000", "snippet": "Signal and Systemconvolution is commutative: prooffrom\\[\\sum_{k=-\\infty}^{\\infty} x[k]h[n-k]\\]let \\(k=n-r\\)\\[\\sum_{r=-\\infty}^{\\infty} x[n-r]h[r]\\]LTI system eigen function: proof\\[\\begin{align}y(t) &amp;= \\int_{-\\infty}^{\\infty} h(\\tau) x(t-\\tau) d\\tau\\\\&amp;= \\int_{-\\infty}^{\\infty} h(\\tau) e^{s(t-\\tau)} d\\tau\\\\&amp;= e^{st} \\int_{-\\infty}^{\\infty} h(\\tau)e^{-s\\tau} d\\tau\\end{align}\\]\\[\\begin{align}y[n] &amp;= \\sum_{k=-\\infty}^{\\infty} h[k]x[n-k]\\\\&amp;= \\sum_{k=-\\infty}^{\\infty} h[k]z^{n-k}\\\\&amp;= z^n \\sum_{k=-\\infty}^{\\infty} h[k]z^{-k}\\end{align}\\]Paeseval’s theorem: proof\\[\\begin{align}&amp;\\sum_{k=-\\infty}^{\\infty} \\vert a_k \\vert^2\\\\=&amp; \\sum_{k=-\\infty}^{\\infty} a_k a_k^*\\\\=&amp; \\sum_{k=-\\infty}^{\\infty} a_k \\dfrac{1}{T} \\int_T x^*(t) e^{jk\\omega_0 t} dt\\\\=&amp; \\dfrac{1}{T} \\sum_{k=-\\infty}^{\\infty} \\int_T x^*(t) a_k e^{jk\\omega_0 t} dt\\\\=&amp; \\dfrac{1}{T} \\int_T x^*(t) \\sum_{k=-\\infty}^{\\infty} a_k e^{jk\\omega_0 t} dt\\\\=&amp; \\dfrac{1}{T} \\int_T x^*(t) x(t) dt\\end{align}\\]\\[\\begin{align}&amp;\\sum_{k=0}^{N-1} \\vert a_k \\vert^2\\\\=&amp; \\sum_{k=0}^{N-1} a_k a_k^*\\\\=&amp; \\sum_{k=0}^{N-1} a_k \\dfrac{1}{N} \\sum_{n=0}^{N-1} x^*[n] e^{jk\\omega_0 n}\\\\=&amp; \\dfrac{1}{N} \\sum_{n=0}^{N-1} x^*[n] \\sum_{k=0}^{N-1} a_k e^{jk\\omega_0 n}\\\\=&amp; \\dfrac{1}{N} \\sum_{n=0}^{N-1} x^*[n] x[n]\\end{align}\\]Fourier TransformFourier Transform: proof\\[\\begin{align}x(t) &amp;= \\sum_{k=-\\infty}^{\\infty} \\left( \\int_{T} x(\\tau) e^{-jk\\omega_0 \\tau} d\\tau \\right) e^{jk\\omega_0 t} \\dfrac{1}{T} \\\\&amp;= \\dfrac{1}{2\\pi}\\sum_{k=-\\infty}^{\\infty} \\left( \\int_{T} x(\\tau) e^{-jk\\omega_0 \\tau} d\\tau \\right) e^{jk\\omega_0 t} \\dfrac{2\\pi}{T} \\\\&amp;= \\dfrac{1}{2\\pi}\\sum_{k=-\\infty}^{\\infty} \\left( \\int_{T} x(\\tau) e^{-jk\\omega_0 \\tau} d\\tau \\right) e^{jk\\omega_0 t} \\omega_0 \\\\&amp;= \\dfrac{1}{2\\pi} \\int_{-\\infty}^{\\infty} X(j\\omega) e^{j\\omega t} d\\omega\\end{align}\\\\\\]\\[X(j\\omega) = \\int_{-\\infty}^{\\infty} x(t) e^{-j\\omega t} dt\\]book_xinhaoyuxianxingxitongfenxiIntroSignalContinuous-Time Signal and Discrete-Time SignalPeriodic Signal and Non-Periodic SignalReal Signal and Complex Signal Deterministic signal, stochastic signal. Continuous-time signal, discrete-time signal. Periodic signal, non-periodic signal. Energy signal, power signal. Causal signal, anti-causal signal.The total energy and average power of a signal \\(f(t)\\) is\\[\\begin{align}E &amp;= \\int_{-\\infty}^{\\infty} \\vert f(t) \\vert^2 dt\\\\P &amp;= \\lim_{T \\to \\infty} \\dfrac{1}{T} \\int_{-T/2}^{T/2} \\vert f(t) \\vert^2 dt\\end{align}\\] Energy signal: \\(E &lt; \\infty\\). Clearly it implies \\(P = 0\\). Power signal: \\(0 &lt; P &lt; \\infty\\). Clearly it implies \\(E = \\infty\\). Causal signal: \\(f(t) = 0 \\quad \\forall t &lt; 0\\). Anti-causal signal: \\(f(t) = 0 \\quad \\forall t \\ge 0\\).Step function\\[u(t) = \\begin{cases}0, &amp;\\quad t &lt; 0\\\\1, &amp;\\quad t &gt; 0\\end{cases}\\]the value at \\(t = 0\\) is not important, as long as we define \\(u(0) &lt; \\infty\\).Since in the theory of generalized function, it is always evaluated by integration.Impulse function is defined by using generalized function.For well behaved function \\(\\phi(t)\\), impulse function \\(\\delta(t)\\) is the function that satisfy\\[\\int_{-\\infty}^{\\infty} \\delta(t)\\phi(t) dt = \\phi(0)\\]e.g.,\\[\\begin{align}\\delta(t) &amp;= \\lim_{b\\to\\infty} b e^{-\\pi (bt)^2}\\\\\\delta(t) &amp;= \\lim_{b\\to\\infty} \\dfrac{\\sin(bt)}{\\pi t}\\end{align}\\] The derivitive of \\(\\delta(t)\\)using \\(f(t) \\delta'(t) = f(0) \\delta'(t) - f'(0) \\delta(t)\\), we have\\[\\int_{-\\infty}^{\\infty} f(t) \\delta'(t) dt = -f'(0)\\]and also\\[\\int_{-\\infty}^{\\infty} f(t) \\delta^{(n)}(t) dt = (-1)^{n} f^{(n)}(0)\\]another equation (can only be used in the integral)\\[\\begin{align}\\delta(at) &amp;= \\dfrac{1}{|a|}\\delta(t)\\\\\\delta^{(n)}(at) &amp;= \\dfrac{1}{\\vert a \\vert} \\dfrac{1}{a^n} \\delta^{(n)}(t)\\end{align}\\] In summary The definitions   \\(\\int_{-\\infty}^{\\infty} f(t) \\delta^{(n)}(t) dt = (-1)^{n} f^{(n)}(0)\\) The formula can be used in anywhere (e.g., in the arguments for the derivitive definition)   \\(f(t)\\delta^{(n)}(t) = f(0) \\delta^{(n)}(t)\\) The formula can only be used in the integral   \\(\\delta^{(n)}(at) = \\dfrac{1}{\\vert a \\vert} \\dfrac{1}{a^n} \\delta^{(n)}(t)\\)   \\(f(t)\\delta'(t) = f(0)\\delta'(t) - f'(0)\\delta(t)\\) Exercises\\[\\begin{align}\\int_{-\\infty}^{\\infty} \\dfrac{\\sin(2t)}{t} \\delta(t) dt &amp;= \\lim_{t \\to 0} \\dfrac{\\sin(2t)}{t} = 2\\end{align}\\]\\[\\begin{align}\\int_{-\\infty}^{\\infty} (t^3 - 3t^2 + 5t - 1) \\delta'(t-1)dt &amp;= -g'(1) = -2\\end{align}\\]\\[\\int_{-\\infty}^{\\infty} (t^3+5) \\delta(\\dfrac{t}{2})dt = 2 \\int_{-\\infty}^{\\infty} (t^3+5) \\delta(t)dt = 10\\]\\[\\begin{align}\\int_{-\\infty}^{t}(2-x) \\delta'(x)dx = \\int_{-\\infty}^{t} [2 \\delta'(x) - (-1)\\delta(t)]dt = 2\\delta(t) + u(t)\\end{align}\\] Discrete-time impulse and step function\\[u[k] = \\begin{cases}1, \\quad k \\ge 0\\\\0, \\quad k &lt; 0\\end{cases}\\]\\[\\sum_{k=-\\infty}^{\\infty} f[k] \\delta[k] = f[0]\\]Discrete-time difference\\[\\delta[k] = u[k] - u[k-1]\\]Discrete-time summation\\[u[k] = \\sum_{i=-\\infty}^{k} \\delta[i]\\]Decomposition\\[u[k] = \\sum_{j=0}^{\\infty} \\delta[k-j]\\] Signal reflect, move, and scale give the waveform \\(f(t)\\), \\(f(-t)\\) is reflect at 0. \\((t \\times -1)\\) give the waveform \\(f(t+2)\\), \\(f(-t+2)\\) is reflect at 0. \\((t \\times -1)\\) give the waveform \\(f(t)\\), \\(f(t-3)\\) is move right to 3 units. \\((t - 3)\\) give the waveform \\(f(t+2)\\), \\(f(t-1)\\) is move right to 3 units. \\((t - 3)\\) give the waveform \\(f(t)\\), \\(f(3t)\\) is shrink 3 times at 0. \\((t \\times 3)\\) give the waveform \\(f(t+2)\\), \\(f(3t+2)\\) is shrink 3 times at 0. \\((t \\times 3)\\) give the waveform \\(f(t)\\), \\(f(0.2 t)\\) is expand 5 times at 0. \\((t \\times 0.2)\\) give the waveform \\(f(t+3)\\), \\(f(0.2 t + 3)\\) is expand 5 times at 0. \\((t \\times 0.2)\\) Usually first move, then scale, then reflect. Do derivitive one the waveform, add the impulse function.Introduction to System Linear system\\[T[a f_1(.) + b f_2 (.)] = a T[f_1(.)] + b T[f_2(.)]\\] Dynamic linear system   \\(y(t) = y_{zi}(t) + y_{zs}(t)\\) The system are linear for the input and states respectively. \\[y(.) = y_{zs}(.) + y_{zi}(.)\\]\\[T[\\{af_1(t) + b f_2(t)\\}, \\{0\\}] = a T[\\{f_1(.)\\}, \\{0\\}] + b T[\\{f_2(.)\\}, \\{0\\}]\\]\\[T[\\{0\\}, \\{a x_1(0) + b x_2(0)\\}] = a T[\\{0\\}, \\{x_1(0)\\}] + b T[\\{0\\}, \\{x_2(0)\\}]\\] Time varying or time invariant: only look zero state response.\\[T[\\{0\\}, f(t-td)] = y_{zs}(t-t_d)\\] LTI system. if \\(f(t) \\to y_{zs}(t)\\), then \\(f'(t) \\to y_{zs}'(t)\\) if \\(f(t) \\to y_{zs}(t)\\), then \\(\\int_{-\\infty}^{t} f(x)dx \\to \\int_{-\\infty}^{t} y_{zs}(x)dx\\) TODO: how to prove? Causal system and non-causal system. It is defined for \\(y_{zs}(t)\\) Example:For a causal LTI causal system with initial state \\(x(0\\_)\\), if \\(x(0\\_) = 1\\) and with a causal input \\(f_1(t)\\), the full response is\\[y_1(t) = [e^{-t} + \\cos(\\pi t)] u(t)\\]if \\(x(0\\_) = 2\\) and input \\(3f_1(t)\\), the full response is\\[y_2(t) = [-2 e^{-t} + 3\\cos(\\pi t)] u(t)\\]then for input \\(f'_1(t) + 2 f_1(t-1)\\), find the zero state response. Solution: \\(y_{zs}(t) = -3\\delta(t) + [4e^{-t} - \\pi \\sin(\\pi t)] u(t) + 2\\{-4e^{-(t-1)} + \\cos[\\pi(t-1)]\\} u(t-1)\\)Continuous-Time Time-Domain AnalysisResponse of LTI SystemDifferential EquationsThe differential equation\\[a_2 \\dfrac{d^2 y(t)}{dt^2} + a_1 \\dfrac{y(t)}{dt} + a_0 y(t) = f(t)\\]with initial conditions\\(y(0_+), \\quad y'(0_+)\\)we can determine the responses for the given \\(f(t)\\) and initial conditions.Block Diagram of Differential EquationsTake the example of\\[y''(t) + ay'(t) + by(t) = f(t)\\]Another example\\[y''(t) + 3y'(t) + 2y(t) = 4f'(t) + f(t)\\]Classic Solution of Differential EquationsIf \\(f(t)\\) has finite derivative up to \\(f^{(m)}(t)\\), for differential equations\\[\\begin{align}&amp;y^{(n)}(t) + a_{n-1} y^{(n-1)}(t) + \\dots + a_1 y^{(1)}(t) + a_0 y(t) \\\\&amp;= b_m f^{(m)}(t) + b_{m-1} f^{(m-1)}(t) + \\dots + b_1 f^{(1)}(t) + b_0 f(t)\\end{align}\\]and \\(n\\) initial conditions\\[y^{(n-1)}(0), y^{(n-2)}(0),\\dots, y(0)\\]The solution is composed of homogeneous solution and special solution\\[y(t) = y_h(t) + y_p(t)\\]where homogeneous solution is the complete solution of the homogeneous differential equation with all possible initial conditions\\[y^{(n)}(t) + a_{n-1} y^{(n-1)}(t) + \\dots + a_1 y^{(1)}(t) + a_0 y(t) = 0\\]the corresponding eigen-function has \\(n\\) roots\\[F(\\lambda) = \\lambda^n + a_{n-1} \\lambda^{n-1} + \\dots + a_1 \\lambda + a_0 = 0\\]If the \\(n\\) roots \\(\\lambda_1, \\dots, \\lambda_n\\) are not equal to each other\\[y_h(t) = C_1 e^{\\lambda_1 t} + C_2 e^{\\lambda_2 t} + \\dots + C_n e^{\\lambda_n t}\\]If there are \\(e_i\\) same roots \\(\\lambda\\), the corresponding term is\\[(C_0 + C_1 t + \\dots + C_{e_i - 1} t^{e_i - 1}) e^{\\lambda t}\\]and the special solution is one arbitrary solution satisfy the original differential equation without worring about the initial condition.TODO: finish the table for special solution.Example:\\[\\begin{align}&amp; y''(t) + 5y'(t) + 6y(t) = f(t)\\\\&amp; f(t) = 2e^{-t}, t \\ge 0\\\\&amp; y'(0) = -1, y(0) = 2\\end{align}\\]On Initial ConditionsZero Input ResponseZero State ResponseDeprecatedDiscrete-Time SignalsThe definition of discrete-time signal is a function from non-negative integer set to real number set.\\[f: \\mathbb{Z}_{\\ge 0} \\to R\\]Basic Signals the discrete-time impulse function\\[\\delta[k] = \\begin{cases}1, &amp; \\text{$k = 0$} \\\\0, &amp; \\text{$k &gt; 0$}\\end{cases}\\] the discrete-time unit step function\\[u[k] = 1\\] the discrete-time ramp signal\\[r[k] = k\\]The \\(z\\)-transformInstead of describing the value at each point, there is another way to describe a signal by only using one single equation, that is \\(Z\\)-transform.\\[F(z) = \\sum_{k=0}^{\\infty} f[k] z^{-k}\\]It can be proved its ROC is\\[|z| &gt; r_1 = \\limsup_{k \\to \\infty} |f[k]|^{1/k}\\]Please note the term ROC doesn’t literally match to “region of convergence”, since we always neglect \\(r_1\\), whether \\(F(z)\\) really converge at \\(r_1\\) or not. Signal \\(x[.]\\quad \\mathbb{Z}_{\\ge 0} \\to \\mathrm{R}\\) \\(z\\)-Transform ROC Notes \\(\\delta[.]\\) \\(1\\) \\(\\vert z \\vert &gt; 0\\)   \\(u[n] = 1\\) \\(\\dfrac{1}{1-z^{-1}}\\) \\(\\vert z\\vert&gt;1\\)   \\(r[n] = n\\) \\(\\dfrac{z^{-1}}{(1-z^{-1})^2}\\) \\(\\vert z \\vert &gt; 1\\)   \\(x[n] = a^n\\) \\(\\dfrac{z}{z-a}\\) \\(\\vert z \\vert &gt; \\vert a \\vert\\)   \\(x[n] = f[n]+g[n]\\) \\(F(z)+G(z)\\) \\(\\mathrm{ROC} \\supseteq \\mathrm{ROC}(f) \\cap \\mathrm{ROC}(g)\\) \\(\\mathrm{ROC}(f) \\cap \\mathrm{ROC}(g) \\ne \\emptyset\\) \\(x[n] = af[n], \\quad a \\ne 0\\) \\(aF(z)\\) \\(\\mathrm{ROC} = \\mathrm{ROC}(f)\\) \\(\\mathrm{ROC}(f) \\ne \\emptyset\\) \\(x[n] = f_{delay,m}[n]\\) \\(z^{-m}F(z)\\) \\(\\mathrm{ROC} = \\mathrm{ROC}(f)\\) \\(\\mathrm{ROC}(f) \\ne \\emptyset\\) \\(x[n] = (f*g)[n]\\) \\(F(z)G(z)\\) \\(\\mathrm{ROC} \\supseteq \\mathrm{ROC}(f) \\cap \\mathrm{ROC}(g)\\) \\(\\mathrm{ROC}(f) \\cap \\mathrm{ROC}(g) \\ne \\emptyset\\) \\(x[n] = f[n] - f_{delay,1}[n]\\) \\((1-z^{-1})F(z)\\) \\(\\mathrm{ROC} \\supseteq \\mathrm{ROC}(f)\\) Difference Almost all the discrete-time signals in practice will have \\(z\\)-Transform with some \\(\\mathrm{ROC}\\ne\\emptyset\\). Although signals like \\(x[n] = n^n\\) doesn’t have \\(z\\)-Transform, perhaps they never appear in real practice. Time delayed by \\(m\\) with initial rest is defined as \\[f_{delay,m}[n] = \\begin{cases}0, &amp; \\text{$n &lt; m$} \\\\f[n-m], &amp; \\text{$n \\ge m$}\\end{cases}\\] Convolution is defined as\\[(f * g)[n] = \\sum_{k=0}^{n} f[k]g[n-k] = \\sum_{k=0}^{n}f[n-k]g[k]\\]below is how to visualize the convolution for \\(F(z) = 1 + z^{-1} + z^{-2}\\) and \\(G(z) = 1 + z^{-1} + z^{-2} + z^{-3}\\).From Time-Domain to \\(z\\)-DomainFrom \\(z\\)-Domain to Time-DomainDiscrete-Time Systems\\[y[.] = a_0 x[.] + a_1 \\cdot x_{delay,1}[.] + a_2 \\cdot x_{delay,2}[.] + \\dots + a_m \\cdot x_{delay,m}[.] + b_1 \\cdot y_{delay,1}[.] + b_2 \\cdot y_{delay,2}[.] + \\dots + b_r \\cdot y_{delay,r}[.]\\]Example 01\\[x[k] = \\begin{cases}1, &amp; \\text{$k = 0, 1, 2$} \\\\0, &amp; \\text{otherwise}\\end{cases}\\]\\[X(z) = 1 + z^{-1} + z^{-2}, \\quad \\mathrm{ROC} = \\{z \\ne 0\\}\\]Example 02\\[x[k] = a^n + 1\\]\\[X(z) = \\dfrac{1}{1-z^{-1}} + \\dfrac{z}{z-a}, \\quad \\mathrm{ROC} = \\{|z| &gt; \\mathrm{max}(1, |a|)\\}\\]Example 03\\[\\begin{align}u_{delay,1}[n] &amp;= \\begin{cases}0, &amp; \\text{$k = 0$} \\\\1, &amp; \\text{$k &gt; 0$}\\end{cases}\\\\U_{delay,1}(z) &amp;= \\dfrac{z^{-1}}{1-z^{-1}}, \\quad \\mathrm{ROC} = \\{|z| &gt; 1\\}\\end{align}\\]Example 04calculate the difference of \\(u[.]\\)\\[\\begin{align}u[.] - u_{delay,1}[.] &amp;= \\delta[.]\\\\(1-z^{-1}) \\cdot \\dfrac{1}{1-z^{-1}} &amp;= 1, \\quad \\mathrm{ROC} = \\{|z| &gt; 0\\}\\end{align}\\]Theorem 01Let \\(f[.]\\) and \\(g[.]\\) be discrete-time signals with\\[\\mathrm{ROC}(f) \\cap \\mathrm{ROC}(g) \\ne \\emptyset\\]If \\(F(z)=G(z)\\) for \\(z \\in \\mathrm{ROC}(f)\\cap\\mathrm{ROC}(g)\\), then \\(f[k] = g[k]\\) for all \\(k\\in\\mathbb{Z}\\) It implies, for an unkown signal \\(f[.]\\), if we know its \\(F(z)\\) expression and its \\(\\mathrm{ROC}(f)\\) is non empty, then in principle we completely know the signal. So, we have a two directional table!Example 05Calculate the \\(z\\)-Transform of \\(x[n]=n+1\\), without calculate explicitly.We know \\(x[.]\\) must have \\(X(z)\\) in \\(\\mathrm{ROC}(x)\\ne\\emptyset\\). \\(\\quad \\leftarrow \\quad\\) ROC \\(\\limsup\\) rule.\\(x_{delay,1}[.]\\) have \\(z\\)-Transform \\(z^{-1}X(z)\\) in \\(\\mathrm{ROC}(x)\\). \\(\\quad \\leftarrow \\quad\\) \\(z\\)-Transform of time delay.\\(x_{delay,1}[.] = r[.]\\), which has \\(z\\)-Transform \\(\\dfrac{z^{-1}}{(1-z^{-1})^2}\\) with \\(\\mathrm{ROC} = \\{\\vert z\\vert &gt; 1\\}\\). \\(\\quad \\leftarrow \\quad\\) tableSo, \\(X(z) = \\dfrac{1}{(1-z^{-1})^2}\\) with \\(\\mathrm{ROC}(x) = \\{\\vert z \\vert &gt; 1\\}\\).Example 06Calculate the signal \\((u*u)[.]\\).Let \\(g[.] = (u*u)[.]\\), then \\(G(z) = \\dfrac{1}{(1-z^{-1})^2}\\) for \\(\\mathrm{ROC}(g)\\supseteq\\{\\vert z \\vert &gt; 1\\}\\) \\(\\quad \\leftarrow \\quad\\) convolution.Then \\((u*u)[n]=g[n]=n+1 \\quad \\leftarrow \\quad\\) tableExample 07 (Summation)define \\(x_{sum}[.]\\) with\\[x_{sum}[n] = \\sum_{k=0}^{n} x[k]\\]Then if exists \\(X(z)\\) with \\(\\mathrm{ROC}(x)\\ne\\emptyset\\) and \\(X_{sum}(z)\\) with \\(\\mathrm{ROC}(x_{sum})\\ne\\emptyset\\), then\\[\\begin{align}X_{sum}(z) &amp;= \\dfrac{X(z)}{1-z^{-1}} \\quad \\text{ with } \\mathrm{ROC}(x_{sum})\\\\\\mathrm{ROC}(x_{sum}) &amp;\\subseteq \\mathrm{ROC}(x)\\end{align}\\]LUT ExercisesTODONOTDiscrete-Time LTI SystemsA system will transform a input signal \\(x_{in}[.]\\) to an output signal \\(x_{out}[.]\\)\\[x_{out}[.] = H(z)\\{x_{in}[.]\\}\\]Basic Systems and Block Diagram Amplify or attennuation or identity.\\[x_{out}[.] = A \\cdot x_{in}[.]\\] Delayed by \\(m\\)\\[x_{out}[.] = x_{in}[.-m]\\] Superposition\\[x_{out}[.] = H_1(x)\\{x_{in}[.]\\} + H_2(x)\\{x_{in}[.]\\}\\] Cascade Feedback linkDiscrete-Time Systems\\(y[n] = x[n] - x[n-1]\\) is an example of difference equation.this drawing explains how to reflect \\(x[n-1]\\) is delayed \\(x[x]\\).Knowing that \\(x[n-1]\\) is the delayed \\(x[n]\\), the difference equation can also be represented by a block diagram.Let \\(x[n]\\) equal the “unit sample” signal \\(\\delta[n]\\) (the most simple non-trivial signal you can imagine!)To be more precise, we need to mention the systen start “at rest”, meaning all the output at the delay start at 0.Then you can completely compute the output \\(y[n]\\) for any input \\(x[n]\\), without any ambigious.Is there any differences between the difference equation and block diagram? operators in the block diagram are more easier to be understood as an manipulation of the whole signal, instead of just single samples.We can denote the delay operator as \\(z^{-1}\\).This drawing shows how the \\(z^{-1}\\) signal is applied to the unit sample signal.The the block diagram can be denoted by\\[1 - z^{-1}\\]" }, { "title": "Signal And System 01", "url": "/posts/signal-and-system-01/", "categories": "math", "tags": "fourier-analysis", "date": "2022-12-09 03:00:00 +0000", "snippet": "Signal and SystemHow to understand signal \\(x(t-t_0)\\)?If \\(t_0\\) is positive, it is a delayed version of \\(x(t)\\);If \\(t_0\\) is negative, it is a advanced version of \\(x(t)\\).You can use the impulse function as an example.How to systematically sketch \\(x(\\alpha t + \\beta)\\)?First sketch the waveform of \\(x(t + \\beta)\\), you can use the impulse function to know if it is delay or advance.Then reflect/compress/expand the waveform around origin.Linear SystemThe response to \\(x_1(t) + x_2(t)\\) is \\(y_1(t) + y_2(t)\\)Time Invariant SystemThe response to \\(x(t-t_0)\\) is \\(y(t-t_0)\\).convolution sum\\[x[n] = \\sum_{k=-\\infty}^{\\infty} x[k] \\delta[n-k]\\]\\[x(t) = \\int_{-\\infty}^{\\infty} x(\\tau) \\delta(t-\\tau) d\\tau\\]note that now \\(\\delta[n-k]\\) and \\(\\delta(t-\\tau)\\) is signal, where \\(x[k]\\) and \\(x(\\tau)\\) are just number.convolution is commutative\\[x[n]*h[n] = h[n]*x[n]\\]\\[x(t)*h(t) = h(t)*x(t)\\]linear system response\\[y[n] = \\sum_{k=-\\infty}^{\\infty} x[k]h_k[n]\\]where \\(h_k[n]\\) is the response to \\(\\delta[n-k]\\).\\[y(t) = \\int_{-\\infty}^{\\infty} x(\\tau) h_{\\tau}(t) d\\tau\\]where \\(h_{\\tau}(t)\\) is the respones to \\(\\delta(t-\\tau)\\)linear time invariant response\\[y[n] = x[n] * h[n] = \\sum_{k=-\\infty}^{\\infty} x[k]h[n-k]\\]where \\(h[n]\\) is the response to \\(\\delta[n]\\).\\[y(t) = x(t)*y(t) = \\int_{-\\infty}^{\\infty} x(\\tau) h(t-\\tau) d\\tau\\]where \\(h(t)\\) is the respones to \\(\\delta(t)\\)initial rest (without proof), what is initial rest?For differential equation with initial rest, the system is LTI and causal.\\[y(t) = \\int_{-\\infty}^{t} x(\\tau) h(t-\\tau) d\\tau\\]if \\(x(t) = x(t)u(t)\\)\\[y(t) = \\int_{0}^{t} x(\\tau) h(t-\\tau) d\\tau\\]LTI system eigen functionThe response to \\(e^{st}\\) (not \\(e^{st}u(t)\\)) is \\(H(s)e^{st}\\), \\(H(s)= \\int_{-\\infty}^{\\infty} h(\\tau)e^{-s\\tau} d\\tau\\).The response to \\(z^n\\) (not \\(z^n u[n]\\)) is \\(H(z)z^n\\) is \\(H(z) = \\sum_{k=-\\infty}^{\\infty} h[k]z^{-k}\\).Fourier SeriesFourier Series\\[x(t) = \\sum_{k=-\\infty}^{\\infty} a_k e^{jk\\omega_0 t}\\]\\[a_k = \\dfrac{1}{T}\\int_{T} x(t) e^{-jk\\omega_0 t} dt\\]\\[x[n] = \\sum_{k=0}^{N-1} a_k e^{jk\\omega_0 n}\\]\\[a_k = \\dfrac{1}{N} \\sum_{n=0}^{N-1} x[n] e^{-jk\\omega_0 n}\\]although it is not obvious if or not Fourier series is unique, as an engineer we assume it is unique, meaning there is only one set of \\(a_k\\) such that\\[x(t) = \\sum_{k=-\\infty}^{\\infty} a_k e^{jk\\omega_0 t}\\]Fourier Series: Time Shifting\\[x(t-t_0) \\overset{FS}{\\longleftrightarrow} e^{-jk\\omega_0 t_0} a_k\\]\\[x[n-n_0] \\overset{FS}{\\longleftrightarrow} e^{-jk\\omega_0 n_0} a_k\\]Fourier Series: Time Reversal\\[x(-t) \\overset{FS}{\\longleftrightarrow} a_{-k}\\]\\[x[-n] \\overset{FS}{\\longleftrightarrow} a_{-k}\\]Fourier Series: even and oddIf \\(x(t)\\) is even, then \\(a_{k}\\) is even; If \\(x(t)\\) is odd, then \\(a_{k}\\) is odd.If \\(x[n]\\) is even, then \\(a_{k}\\) is even; If \\(x[n]\\) is odd, then \\(a_{k}\\) is odd.Fourier Series: Multiplication\\[x(t)y(t) \\overset{FS}{\\longleftrightarrow} h_k = \\sum_{l=-\\infty}^{\\infty} a_l b_{k-l}\\]\\[x[n]y[n] \\overset{FS}{\\longleftrightarrow} h_k = \\sum_{l=0}^{N-1} a_l b_{k-l}\\]Fourier Series: Conjugation\\[x^{*}(t) \\overset{FS}{\\longleftrightarrow} a_{-k}^{*}\\]\\[x^{*}[n] \\overset{FS}{\\longleftrightarrow} a_{-k}^{*}\\]Fourier Series: real \\(x(t)\\), real even \\(x(t)\\) and real odd \\(x(t)\\)for real \\(x(t)\\), \\(a_{k} = a_{-k}^{*}\\)for real even \\(x(t)\\), \\(a_{k}\\) is pure real and even.for real odd \\(x(t)\\), \\(a_{k}\\) is pure imaginary and odd.for real \\(x[t]\\), \\(a_{k} = a_{-k}^{*}\\)for real even \\(x[t]\\), \\(a_{k}\\) is pure real and even.for real odd \\(x[t]\\), \\(a_{k}\\) is pure imaginary and odd.Paeseval’s theorem\\[\\dfrac{1}{T}\\int_T \\vert x(t) \\vert^2 dt = \\sum_{-\\infty}^{\\infty} \\vert a_k \\vert^2\\]\\[\\dfrac{1}{N} \\sum_{n=0}^{N-1} \\vert x[n] \\vert^2 = \\sum_{k=0}^{N-1} \\vert a_k \\vert^2\\]filtering\\[y(t) = \\sum_{k=-\\infty}^{\\infty} a_k H(jk\\omega_0) e^{jk\\omega_0 t}\\]\\[y[n] = \\sum_{n=0}^{N-1} a_k H(e^{jk\\omega_0}) e^{jk\\omega_0 n}\\]obtain frequency respones from ode\\[RC \\dfrac{d y(t)}{dt} + y(t) = x(t)\\]\\[RC \\dfrac{d}{dt}[H(j\\omega)e^{j\\omega t}] + H(j\\omega) e^{j\\omega t} = e^{j\\omega t}\\]\\[H(j\\omega) = \\dfrac{1}{1+RCj\\omega}\\]convergence of fourier seriesbook_signals_and_systems, section 3.4Fourier TransformFourier Transform\\[x(t) = \\dfrac{1}{2\\pi} \\int_{-\\infty}^{\\infty} X(\\omega) e^{j\\omega t} d\\omega\\]\\[X(\\omega) = \\int_{-\\infty}^{\\infty} x(t) e^{-j\\omega t} dt\\]\\[X(\\omega) = \\sum_{k=-\\infty}^{\\infty} 2\\pi a_k \\delta(\\omega - k\\omega_0)\\]We assume Fourier transform is unique, e.g., there is at most one \\(X(\\omega)\\) such that\\[x(t) = \\dfrac{1}{2\\pi} \\int_{-\\infty}^{\\infty} X(\\omega) e^{j\\omega t} d\\omega\\]Fourier Transform: Differentiation and Integration\\[\\dfrac{d x(t)}{dt} \\overset{FT}{\\longleftrightarrow} j\\omega X(\\omega)\\]\\[\\int_{-\\infty}^{t} x(\\tau) d\\tau \\overset{FT}{\\longleftrightarrow} \\dfrac{1}{j\\omega} X(\\omega) + \\pi X(0)\\delta(\\omega)\\]Fourier Transform: Duality\\[g(t) \\overset{FT}{\\longleftrightarrow} f(\\omega)\\]\\[f(t) \\overset{FT}{\\longleftrightarrow} 2\\pi g(-\\omega)\\]Fourier Transform: Parseval’s Relation\\[\\int_{-\\infty}^{\\infty} \\vert x(t) \\vert^2 dt = \\dfrac{1}{2\\pi} \\int_{-\\infty}^{\\infty} \\vert X(\\omega) \\vert^2 d\\omega\\]Fourier Transform: conlvlution property\\[h(t)*x(t) \\overset{FT}{\\longleftrightarrow} H(\\omega)X(\\omega)\\]Fourier Transform: modulation property\\[s(t)p(t) \\overset{FT}{\\longleftrightarrow} \\dfrac{1}{2\\pi} [S(\\omega)*P(\\omega)]\\]DTFT\\[x[n] = \\dfrac{1}{2\\pi}\\int_{0}^{2\\pi} X(\\Omega) e^{j\\Omega n} d\\Omega\\]\\[X(\\Omega) = \\sum_{n=-\\infty}^{\\infty} x[n] e^{-j\\Omega n}\\]\\[X(\\Omega) = \\sum_{k=-\\infty}^{\\infty} 2\\pi a_k \\delta(\\Omega - k\\omega_0)\\]" }, { "title": "VCO 01", "url": "/posts/vco-01/", "categories": "analog-circuit", "tags": "analog, pll", "date": "2022-12-06 10:00:00 +0000", "snippet": "\\[\\mathcal{L}_{VCO}(f_m) = \\dfrac{10^{\\mathrm{FOM}_{\\mathrm{VCO}}/10}}{P_{\\mathrm{VCO}}/1 \\mathrm{mW}} \\cdot \\dfrac{f_{\\mathrm{VCO}}^2}{f_m^2}\\]for \\(LC\\) oscillator, take the simple model shown below\\[\\begin{align}\\mathcal{L}(\\omega) &amp;= \\dfrac{4kTR}{\\dfrac{1}{2}V_0^2} \\bigg(\\dfrac{\\omega_0}{2Q\\omega}\\bigg)^2\\\\&amp;= \\dfrac{4kT}{P_{dissipation}} \\bigg( \\dfrac{\\omega_0}{2Q\\omega} \\bigg)^2\\end{align}\\]where\\[\\begin{align}Q &amp;= \\dfrac{R}{\\omega_0 L}\\\\\\omega_0 &amp;= \\dfrac{1}{\\sqrt{LC}}\\end{align}\\]If we use such implementation like belowIt has a peak differential output swing of\\[V_{XY} \\approx \\dfrac{4}{\\pi} I_{ss}R_p\\]VCO Model SpecificationLet’s specify \\(V_0, P_{\\mathrm{VCO}}, \\mathrm{FOM_{VCO}}\\)\\[\\begin{align}P_{\\mathrm{VCO}} &amp;= \\dfrac{V_0^2}{2R_p}\\\\R_p &amp;= \\dfrac{V_0^2}{2 P_{\\mathrm{VCO}}}\\\\Q^2 &amp;= \\dfrac{kT}{1mW} \\cdot 10^{-\\mathrm{FOM_{VCO}}/10}\\\\Q &amp;= \\sqrt{\\dfrac{kT}{1mW} \\cdot 10^{-\\mathrm{FOM_{VCO}}/10}}\\\\L_1 &amp;= \\dfrac{R_p}{2\\pi f_{target} Q}\\\\I_{ss} &amp;= \\dfrac{V_0}{R_p} \\cdot \\dfrac{\\pi}{4}\\\\g_m &amp;= \\dfrac{2}{R_p} = \\sqrt{4 K_N \\cdot \\dfrac{1}{2} I_{ss}}\\\\K_N &amp;= \\dfrac{2}{I_{ss} R_p^2}\\end{align}\\]let the control voltage range from \\(V_{cl}\\) to \\(V_{ch}\\)\\[C_x = C_m \\dfrac{V_{ch} - V_{x}}{V_{ch} - V_{cl}}\\]we can specify \\(f_{min}, f_{max}\\), let’s have\\[\\begin{align}f_{min} &amp;= \\dfrac{1}{2\\pi \\sqrt{L_1 (C_1 + C_m)}}\\\\f_{max} &amp;= \\dfrac{1}{2\\pi\\sqrt{L_1 C_1}}\\end{align}\\]\\[\\begin{align}C_1 &amp;= \\dfrac{1}{L_1 \\cdot (2\\pi f_{max})^2}\\\\C_m &amp;= \\dfrac{1}{L_1 \\cdot (2\\pi f_{min})^2} - \\dfrac{1}{L_1 \\cdot (2\\pi f_{max})^2}\\\\K_{\\mathrm{VCO}} &amp;\\approx \\dfrac{2\\pi (f_{max} - f_{min})}{V_{ch}-V_{cl}}\\end{align}\\]SummaryIn summary, the value we specified is \\(V_0\\) \\(P_{\\mathrm{VCO}}\\) \\(\\mathrm{FOM_{VCO}}\\) \\(V_{cl}\\) \\(V_{ch}\\) \\(f_{min}\\) \\(f_{target}\\) \\(f_{max}\\) \\[\\begin{align}R_p &amp;= \\dfrac{V_0^2}{2 P_{\\mathrm{VCO}}}\\\\Q &amp;= \\sqrt{\\dfrac{10^{\\mathrm{FOM_{VCO}}/10}}{kT/1mW}}\\\\L_1 &amp;= \\dfrac{R_p}{2\\pi f_{target} Q}\\\\I_{ss} &amp;= \\dfrac{V_0}{R_p} \\cdot \\dfrac{\\pi}{4}\\\\K_N &amp;= \\dfrac{2}{I_{ss} R_p^2}\\\\C_1 &amp;= \\dfrac{1}{L_1 \\cdot (2\\pi f_{max})^2}\\\\C_m &amp;= \\dfrac{1}{L_1 \\cdot (2\\pi f_{min})^2} - \\dfrac{1}{L_1 \\cdot (2\\pi f_{max})^2}\\\\K_{\\mathrm{VCO}} &amp;\\approx \\dfrac{f_{max} - f_{min}}{V_{ch}-V_{cl}}\\end{align}\\]" }, { "title": "Paper A Low Noise Sub-Sampling PLL in Which Divider Noise is Eliminated and PD/CP Noise is Not Multiplied by $$N^2$$", "url": "/posts/paper-sspll/", "categories": "analog-circuit", "tags": "analog, pll", "date": "2022-12-04 10:00:00 +0000", "snippet": "From Xiang Gao’s PaperLow Noise Phase DetectionClassical Three-Stage PFD/CPThe in-band phase noise caused by CP can be calculated as\\[\\begin{align}H_{CP}(s) &amp;= \\dfrac{\\phi_{out,n}}{i_{CP,n}} = \\dfrac{1}{\\beta_{CP}} \\cdot \\dfrac{\\beta_{CP}\\cdot F_{LF}(s)\\cdot\\dfrac{K_{VCO}}{s}}{1 + \\beta_{CP}\\cdot F_{LF}(s)\\cdot\\dfrac{K_{VCO}}{s}}\\\\&amp;= \\dfrac{1}{\\beta_{CP}} \\cdot \\dfrac{G(s)}{1+G(s)}\\end{align}\\]where \\(G(s)\\) is the PLL open loop transfer function.\\[\\mathcal{L}_{in-band,CP} \\approx \\dfrac{1}{2} \\cdot S_{iCP,n} \\cdot |H_{CP}(s)|^2 \\approx \\dfrac{S_{iCP,n}}{2 \\beta_{CP}^2}\\]for classical three-stage PFD/CP\\[\\begin{align}\\beta_{CP} &amp;= \\dfrac{I_{CP}}{2\\pi} \\cdot \\dfrac{1}{N}\\\\S_{iCP,n} &amp;= 8kT\\gamma \\cdot g_m \\cdot \\dfrac{\\tau_{PFD}}{T_{ref}}\\end{align}\\]Proposed Sub-Sampling PD/CPstill\\[\\begin{align}H_{CP}(s) &amp;= \\dfrac{\\phi_{out,n}}{i_{CP,n}} = \\dfrac{1}{\\beta_{CP}} \\cdot \\dfrac{\\beta_{CP}\\cdot F_{LF}(s)\\cdot\\dfrac{K_{VCO}}{s}}{1 + \\beta_{CP}\\cdot F_{LF}(s)\\cdot\\dfrac{K_{VCO}}{s}}\\\\&amp;= \\dfrac{1}{\\beta_{CP}} \\cdot \\dfrac{G(s)}{1+G(s)}\\end{align}\\]but for sub-sampling PD/CP\\[\\begin{align}\\beta_{CP} &amp;= A_{VCO} \\cdot g_m\\\\S_{iCP,n} &amp;= 8kT\\gamma \\cdot g_m\\end{align}\\]" }, { "title": "TikZ Tutorial", "url": "/posts/tikz-tutorial/", "categories": "cs", "tags": "tikz", "date": "2022-12-01 11:30:00 +0000", "snippet": "From link.The doc can also be invoked by texdoc tikz.In LaTex you can use inline code, e.g., \\tikz \\draw (0pt,0pt) -- (20pt,6pt); to produce inline figures.Tutorial: A Picture for Karl’s Students\\documentclass[tikz]{standalone}\\begin{document}\\begin{tikzpicture} \\draw (-1.5,0) -- (1.5,0); \\draw (0,-1.5) -- (0,1.5);\\end{tikzpicture}\\end{document}Straight Path Construction\\draw (-1.5,0) -- (1.5,0) -- (0,-1.5) -- (0,1.5);Curved Path ConstructionFor this, TikZ provides a special syntax.One ot two “control points” are needed.The math behind them is not quite trivial, but here is the basic idea:Suppose you are at point \\(x\\) and the first control point is \\(y\\).Then the curve will start “going in the direction of \\(y\\) at \\(x\\),” that is, the tangent of the curve at \\(x\\) will point toward \\(y\\).Next, suppose the curve should end at \\(z\\) and the second support point is \\(w\\).Then the curve will, indeed, end at \\(z\\) and the tangent of the curve at point \\(z\\) will go through \\(w\\).\\begin{tikzpicture} \\filldraw [gray] (0,0) circle (2pt) (1,1) circle (2pt) (2,1) circle (2pt) (2,0) circle (2pt); \\draw (0,0) .. controls (1,1) and (2,1) .. (2,0);\\end{tikzpicture}The general syntax for extending a path in a “curved” way is .. controls &lt;first control point&gt; and &lt;second control point&gt; .. &lt;end point&gt;\\begin{tikzpicture} \\draw (-1.5,0) -- (1.5,0); \\draw (0,-1.5) -- (0,1.5); \\draw (-1,0) .. controls (-1,0.555) and (-0.555,1) .. (0,1) .. controls (0.555,1) and (1,0.555) .. (1,0);\\end{tikzpicture}Circle Path Construction\\draw (0,0) circle (10pt);\\draw (0,0) ellipse (20pt and 10pt);\\draw[rotate=30] (0,0) ellipse (6pt and 3pt);\\begin{tikzpicture} \\draw (-1.5,0) -- (1.5,0); \\draw (0,-1.5) -- (0,1.5); \\draw (0,0) circle (1cm);\\end{tikzpicture}Rectangle Path Construction\\begin{tikzpicture} \\draw (-1.5,0) -- (1.5,0); \\draw (0,-1.5) -- (0,1.5); \\draw (0,0) circle (1cm); \\draw (0,0) rectangle (0.5,0.5); \\draw (-0.5,-0.5) rectangle (-1,-1);\\end{tikzpicture}Grid Path Construction\\draw[step=2pt] (0,0) grid (10pt,10pt);Note how the optional argument for \\draw can be used to specify a grid width (there are also xstep and ystep to define the steppings independently).\\begin{tikzpicture} \\draw (-1.5,0) -- (1.5,0); \\draw (0,-1.5) -- (0,1.5); \\draw (0,0) circle (1cm); \\draw[step=.5cm] (-1.4,-1.4) grid (1.4,1.4);\\end{tikzpicture}\\begin{tikzpicture} \\draw[step=.5cm,gray,very thin] (-1.4,-1.4) grid (1.4,1.4); \\draw (-1.5,0) -- (1.5,0); \\draw (0,-1.5) -- (0,1.5); \\draw (0,0) circle (1cm);\\end{tikzpicture}To subdue the grid, Karl adds two more options to the \\draw command that draws the grid.First, he uses the color gray for the grid lines.Second, he reduces the line width to very thin.Finally, he swaps the ordering of the commands so that the grid is drawn first and everything else on top.Adding a Touch of StyleInstead of the options gray, very thin Karl could also have said help lines.Styles are predefined sets of options that can be used to organize how a graphic is drawn.help lines/.style={color=blue!50,very thin}The effect of this “style setter” is that in the current scope or environment the ‘help lines’ option has the same effect as color=blue!50,very thin.Normally, styles are defined at the beginning of a picture.However, you may sometimes wish to define a style globally, so that all pictures of your document can use this style.In this situation you can use the \\tikzset command at the beginning of the document as in\\tikzset{help lines/.style=very thin}To build a hierarchy of styles you can have one style use another.So in order to define a style Karl's grid that is based on the grid style Karl could say\\tikzset{Karl's grid/.style={help lines,color=blue!50}}...\\draw[Karl's grid] (0,0) grid (5,5);Styles are made even more powerful py parameterization.This means that, like other options, styles can also be used with a parameter.For instance, Karl could parameterize his grid so that, by default, it is blue, but he could also use anther color.\\begin{tikzpicture} [Karl's grid/.style ={help lines,color=#1!50}, Karl's grid/.default=blue] \\draw[Karl's grid] (0,0) grid (1.5,2); \\draw[Karl's grid=red] (2,0) grid (3.5,2);\\end{tikzpicture}Drawing OptionsThere are many options simiar to\\draw[step=.5cm,gray,very thin] (-1.4,-1.4) grid (1.4,1.4);ultra thin, very thin, thin, semithick, thick, very thick, ultra thick, where thin is the default thickness.Another useful thing is dashed and dotted. And also lossely dashed, densely dashed, loosely dotted, densely dotted.Arc Path Construction\\begin{tikzpicture} \\draw[step=.5cm,gray,very thin] (-1.4,-1.4) grid (1.4,1.4); \\draw (-1.5,0) -- (1.5,0); \\draw (0,-1.5) -- (0,1.5); \\draw (0,0) circle [radius=1cm]; \\draw (3mm,0mm) arc [start angle=0, end angle=30, radius=3mm];\\end{tikzpicture}\\begin{tikzpicture}[scale=3]\\draw[step=.5cm,gray,very thin] (-1.4,-1.4) grid (1.4,1.4);\\draw (-1.5,0) -- (1.5,0);\\draw (0,-1.5) -- (0,1.5);\\draw (0,0) circle [radius=1cm];\\draw (3mm,0mm) arc [start angle=0, end angle=30, radius=3mm];\\end{tikzpicture}Clipping a Path\\begin{tikzpicture}[scale=3] \\clip (-0.1,-0.2) rectangle (1.1,0.75); \\draw[step=.5cm,gray,very thin] (-1.4,-1.4) grid (1.4,1.4); \\draw (-1.5,0) -- (1.5,0); \\draw (0,-1.5) -- (0,1.5); \\draw (0,0) circle [radius=1cm]; \\draw (3mm,0mm) arc [start angle=0, end angle=30, radius=3mm];\\end{tikzpicture}You can also do both at the same time:Draw and clip a path.For this, use the \\draw command and add the clip option.(This is not the whole pecture:You can also use the \\clip command and add the draw option.Well, that is also not the whole picture:In reality, \\draw is just a shorthand for \\path[draw] and \\clip is a shorthand for \\path[clip] and you could also say \\path[draw,clip].)Here is an example:\\begin{tikzpicture}[scale=3] \\clip[draw] (0.5,0.5) circle (.6cm); \\draw[step=.5cm,gray,very thin] (-1.4,-1.4) grid (1.4,1.4); \\draw (-1.5,0) -- (1.5,0); \\draw (0,-1.5) -- (0,1.5); \\draw (0,0) circle [radius=1cm]; \\draw (3mm,0mm) arc [start angle=0, end angle=30, radius=3mm];\\end{tikzpicture}Parabola and Sine Path Construction\\draw (0,0) rectangle (1,1) (0,0) parabola (1,1);It is also possible to place the bend somewhere else:\\draw[x=1pt,y=1pt] (0,0) parabola bend (4,16) (6,12);The operations sin and cos add a sine or cosine curve in the interval \\([0,\\pi/2]\\) such that the previous current point is at the start of the curve and the curve ends at the given end point.\\draw[x=1.57ex,y=1ex] (0,0) sin (1,1) cos (2,0) sin (3,-1) cos (4,0) (0,1) cos (1,0) sin (2,-1) cos (3,0) sin (4,1);Filling and Drawing\\begin{tikzpicture}[scale=3] \\clip (-0.1,-0.2) rectangle (1.1,0.75); \\draw[step=.5cm,gray,very thin] (-1.4,-1.4) grid (1.4,1.4); \\draw (-1.5,0) -- (1.5,0); \\draw (0,-1.5) -- (0,1.5); \\draw (0,0) circle [radius=1cm]; \\fill[green!20!white] (0,0) -- (3mm,0mm) arc [start angle=0, end angle=30, radius=3mm] -- (0,0);\\end{tikzpicture}The color green!20!white means 20% green and 80% white mixed together.Such color expression are possible since TikZ uses Uwe Kern’s xcolor package, see the documentation of that package for details on color expressions.What would have happened, if Karl had not “closed” the path using --(0,0) at the end?In this case, the path is closed automatically, so this could have been omitted.Indeed, it would even have been better to write the following, instead:\\fill[green!20!white] (0,0) -- (3mm,0mm) arc [start angle=0, end angle=30, radius=3mm] -- cycle;The --cycle causes the current path to be closed (actually the current part of the current path) by smoothly joining the first and last point.To appreciate the difference, consider the following example:\\begin{tikzpicture}[line width=5pt] \\draw (0,0) -- (1,0) -- (1,1) -- (0,0); \\draw (2,0) -- (3,0) -- (3,1) -- cycle; \\useasboundingbox (0,1.5); % make bounding box higher\\end{tikzpicture}You can also fill and draw a path at the same time using the \\filldraw command.You can speficy different colors to be used for filling and for stroking.\\begin{tikzpicture}[scale=3] \\clip (-0.1,-0.2) rectangle (1.1,0.75); \\draw[step=.5cm,gray,very thin] (-1.4,-1.4) grid (1.4,1.4); \\draw (-1.5,0) -- (1.5,0); \\draw (0,-1.5) -- (0,1.5); \\draw (0,0) circle [radius=1cm]; \\filldraw[fill=green!20!white, draw=green!50!black] (0,0) -- (3mm,0mm) arc [start angle=0, end angle=30, radius=3mm] -- cycle;\\end{tikzpicture}Shading\\shade (0,0) rectangle (2,1) (3,0.5) circle (.5cm);The default shading is a smooth transition from gray to white.To specify different colors, you can use options:\\begin{tikzpicture}[rounded corners,ultra thick] \\shade[top color=yellow,bottom color=black] (0,0) rectangle +(2,1); \\shade[left color=yellow,right color=black] (3,0) rectangle +(2,1); \\shadedraw[inner color=yellow,outer color=black,draw=yellow] (6,0) rectangle +(2,1); \\shade[ball color=green] (9,.5) circle (.5cm);\\end{tikzpicture}\\begin{tikzpicture}[scale=3] \\clip (-0.1,-0.2) rectangle (1.1,0.75); \\draw[step=.5cm,gray,very thin] (-1.4,-1.4) grid (1.4,1.4); \\draw (-1.5,0) -- (1.5,0); \\draw (0,-1.5) -- (0,1.5); \\draw (0,0) circle [radius=1cm]; \\shadedraw[left color=gray,right color=green, draw=green!50!black] (0,0) -- (3mm,0mm) arc [start angle=0, end angle=30, radius=3mm] -- cycle;\\end{tikzpicture}Specifying CoordinatesThere are different ways of specifying coordinates.The easiest way is to say something like (10pt,2cm).This means 10pt in \\(x\\)-direction and 2cm in \\(y\\)-directions.Alternatively, you can also leave out the units as in (1,2), which means “one times the current \\(x\\)-vector pule twice the current \\(y\\)-vector”.These vectors default to 1cm. (30:1cm) means 1cm in direction 30 degree. +(0cm,1cm) means “1cm upwards from the previous speficied position”. ++(2cm,0cm) means “2cm to the right of the previous specified position, making this the new specified position”.\\begin{tikzpicture}[scale=3] \\clip (-0.1,-0.2) rectangle (1.1,0.75); \\draw[step=.5cm,gray,very thin] (-1.4,-1.4) grid (1.4,1.4); \\draw (-1.5,0) -- (1.5,0); \\draw (0,-1.5) -- (0,1.5); \\draw (0,0) circle [radius=1cm]; \\shadedraw[left color=gray,right color=green, draw=green!50!black] (0,0) -- (3mm,0mm) arc [start angle=0, end angle=30, radius=3mm] -- cycle;\\end{tikzpicture}\\begin{tikzpicture}[scale=3] \\clip (-0.1,-0.2) rectangle (1.1,0.75); \\draw[step=.5cm,gray,very thin] (-1.4,-1.4) grid (1.4,1.4); \\draw (-1.5,0) -- (1.5,0); \\draw (0,-1.5) -- (0,1.5); \\draw (0,0) circle [radius=1cm]; \\filldraw[fill=green!20,draw=green!50!black] (0,0) -- (3mm,0mm) arc [start angle=0, end angle=30, radius=3mm] -- cycle; \\draw[red,very thick] (30:1cm) -- +(0,-0.5);\\end{tikzpicture} (30:1cm |- 0,0) means “the point straight down from (30:1cm) that lies on the \\(x\\)-axis”. (&lt;p&gt; |- &lt;q&gt;) means “the intersection of a vertical line through \\(p\\) and a horizontal line through \\(q\\)”.\\begin{tikzpicture}[scale=3] \\clip (-0.1,-0.2) rectangle (1.1,0.75); \\draw[step=.5cm,gray,very thin] (-1.4,-1.4) grid (1.4,1.4); \\draw (-1.5,0) -- (1.5,0); \\draw (0,-1.5) -- (0,1.5); \\draw (0,0) circle [radius=1cm]; \\filldraw[fill=green!20,draw=green!50!black] (0,0) -- (3mm,0mm) arc [start angle=0, end angle=30, radius=3mm] -- cycle; \\draw[red,very thick] (30:1cm) -- +(0,-0.5); \\draw[blue,very thick] (30:1cm) ++(0,-0.5) -- (0,0);\\end{tikzpicture}To appreciate the difference between + and ++ consider the following example% Example using ++\\begin{tikzpicture} \\def\\rectanglepath{-- ++(1cm,0cm) -- ++(0cm,1cm) -- ++(-1cm,0cm) -- cycle} \\draw (0,0) \\rectanglepath; \\draw (1.5,0) \\rectanglepath;\\end{tikzpicture}% Example using +\\begin{tikzpicture} \\def\\rectanglepath{-- +(1cm,0cm) -- +(1cm,1cm) -- +(0cm,1cm) -- cycle} \\draw (0,0) \\rectanglepath; \\draw (1.5,0) \\rectanglepath;\\end{tikzpicture}all of this could have been written as\\draw (0,0) rectangle +(1,1) (1.5,0) rectangle +(1,1);Intersecting Paths (1,{tan(30)}): TikZ’s math engine knows how to compute things like tan(30).Note the added braces since, otherwise, TikZ’s parser would think that the first closing parenthesis ends the coordinate (in general, you need to add braces around components of coordinates when these components contain parentheses). \\path command without any options like draw or fill creates “invisible” path. \\path [name path=upward line] (1,0) -- (1,1);\\path [name path=sloped line] (0,0) -- (30:1.5cm); % a bit longer, so that there is an intersection% (add `\\usetikzlibrary{intersections}' after loading tikz in the preamble)\\draw [name intersections={of=upward line and sloped line, by=x}] [very thick,orange] (1,0) -- (x);Adding Arrow Tips Adds the option -&gt; to the drawing commands. -&gt; puts arrow tips at the end of the path. &lt;- puts arrow tips at the begining of the path. &lt;-&gt; puts arrow tips at both ends of the path.\\usetikzlibrary {intersections}\\begin{tikzpicture}[scale=3] \\clip (-0.1,-0.2) rectangle (1.1,1.51); \\draw[step=.5cm,gray,very thin] (-1.4,-1.4) grid (1.4,1.4); \\draw[-&gt;] (-1.5,0) -- (1.5,0); \\draw[-&gt;] (0,-1.5) -- (0,1.5); \\draw (0,0) circle [radius=1cm]; \\filldraw[fill=green!20,draw=green!50!black] (0,0) -- (3mm,0mm) arc [start angle=0, end angle=30, radius=3mm] -- cycle; \\draw[red,very thick] (30:1cm) -- +(0,-0.5); \\draw[blue,very thick] (30:1cm) ++(0,-0.5) -- (0,0); \\path [name path=upward line] (1,0) -- (1,1); \\path [name path=sloped line] (0,0) -- (30:1.5cm); \\draw [name intersections={of=upward line and sloped line, by=x}] [very thick,orange] (1,0) -- (x);\\end{tikzpicture} You canadd arrow tips only to a sinle open “line”. For example, you cannot add tips to, say, a rectangle or circuit.However, you can add arrow tips to curved paths and to paths that have several segments.\\begin{tikzpicture} \\draw [&lt;-&gt;] (0,0) arc [start angle=180, end angle=30, radius=10pt]; \\draw [&lt;-&gt;] (1,0) -- (1.5cm,10pt) -- (2cm,0pt) -- (2.5cm,10pt);\\end{tikzpicture} There are different arrow styles. (see section 106.)\\usetikzlibrary {arrows.meta}\\begin{tikzpicture}[&gt;=Stealth] \\draw [-&gt;] (0,0) arc [start angle=180, end angle=30, radius=10pt]; \\draw [&lt;&lt;-,very thick] (1,0) -- (1.5cm,10pt) -- (2cm,0pt) -- (2.5cm,10pt);\\end{tikzpicture}Scoping\\begin{tikzpicture}[ultra thick] \\draw (0,0) -- (0,1); \\begin{scope}[thin] \\draw (1,0) -- (1,1); \\draw (2,0) -- (2,1); \\end{scope} \\draw (3,0) -- (3,1);\\end{tikzpicture} Scoping has another interesing effect:Any changes to the clipping area are local to the scope.Transformations TikZ provides numerous options that allow you to transform coordinates. For example, the xshift option allows you to shift all subsequent points by certain amount.\\draw (0,0) -- (0,0.5) [xshift=2pt] (0,0) -- (0,0.5); You can change transformation “in the middle of a path”\\begin{tikzpicture}[even odd rule,rounded corners=2pt,x=10pt,y=10pt] \\filldraw[fill=yellow!80!black] (0,0) rectangle (1,1) [xshift=5pt,yshift=5pt] (0,0) rectangle (1,1) [rotate=30] (-1,-1) rectangle (2,2);\\end{tikzpicture}Repeating Things: For-Loops\\foreach \\x in {1,2,3} {$x=\\x$,}The general syntax is \\foreach &lt;variable&gt; in { &lt;list of values&gt; } &lt;commands&gt;.If the &lt;commands&gt; do not start with a brace, everyhing up to the next semicolon is used as &lt;commands&gt;.\\begin{tikzpicture}[scale=3] \\clip (-0.1,-0.2) rectangle (1.1,1.51); \\draw[step=.5cm,gray,very thin] (-1.4,-1.4) grid (1.4,1.4); \\filldraw[fill=green!20,draw=green!50!black] (0,0) -- (3mm,0mm) arc [start angle=0, end angle=30, radius=3mm] -- cycle; \\draw[-&gt;] (-1.5,0) -- (1.5,0); \\draw[-&gt;] (0,-1.5) -- (0,1.5); \\draw (0,0) circle [radius=1cm]; \\foreach \\x in {-1cm,-0.5cm,1cm} \\draw (\\x,-1pt) -- (\\x,1pt); \\foreach \\y in {-1cm,-0.5cm,0.5cm,1cm} \\draw (-1pt,\\y) -- (1pt,\\y);\\end{tikzpicture}\\foreach \\x in {-1,-0.5,1} \\draw[xshift=\\x cm] (0pt,-1pt) -- (0pt,1pt);\\tikz \\foreach \\x in {1,...,10} \\draw (\\x,0) circle (0.4cm);\\tikz \\foreach \\x in {-1,-0.5,...,1} \\draw (\\x cm,-1pt) -- (\\x cm,1pt);\\begin{tikzpicture}\\foreach \\x in {1,2,...,5,7,8,...,12} \\foreach \\y in {1,...,5} { \\draw (\\x,\\y) +(-.5,-.5) rectangle ++(.5,.5); \\draw (\\x,\\y) node{\\x,\\y}; }\\end{tikzpicture}Adding Text" }, { "title": "Introduction to Probability 001", "url": "/posts/intro-prob-001/", "categories": "math", "tags": "probability", "date": "2022-11-02 14:00:00 +0000", "snippet": "Probability Models And AxiomsLecture 01.1 YoutubeLecture 01.2 YoutubeSample Space Sample space is a list (set) of possible outcomes, \\(\\Omega\\) Event: a subset of the sample space. Probability is assigned to events.Probability AxiomsLecture 01.4 Youtube Nonnegativity: \\(P(A) \\ge 0\\) Normalization: \\(P(\\Omega) = 1\\) (Finite) additivity: (to be strengthened later)If \\(A \\cap B = \\varnothing\\), then \\(P(A \\cup B) = P(A) + P(B)\\) \\(A \\cap B\\) read as \\(A\\) intersects \\(B\\), \\(A \\cup B\\) read as \\(A\\) unions \\(B\\)Lecture 01.5 YoutubeLecture 01.6 YoutubeLecture 01.7 YoutubeLecture 01.8 YoutubeLecture 01.9 Youtube Countable Additivity Axiom:If \\(A_1, A_2, A_3, \\dots\\) is an infinite sequence of disjoint events, then \\(P(A_1 \\cup A_2 \\cup A_3 \\cup \\cdots) = P(A_1) + P(A_2) + P(A_3) + \\dots\\)Lecture 01.10 YoutubeMathematical Background" }, { "title": "Fourier Analysis 002 Proof of Fejer's Theorem", "url": "/posts/fourier-analysis-002/", "categories": "math", "tags": "fourier-analysis", "date": "2022-10-31 14:00:00 +0000", "snippet": "From book Fourier AnalysisProof of Fejer’s TheoremTheorem 1.5(i)If \\(f: \\mathbb{T} \\to \\mathbb{C}\\) is Riemann integrable then, if \\(f\\) is continuous at \\(t\\),\\[\\sigma_{n}(f,t) = \\sum_{r=-n}^{n} \\frac{n+1-|r|}{n+1} \\hat{f}(r) \\exp irt \\to f(t)\\](ii)If \\(f:\\mathbb{T} \\to \\mathbb{C}\\) is continuous then\\[\\sigma_{n}(f,t) = \\sum_{r=-n}^{n} \\frac{n+1-|r|}{n+1} \\hat{f}(r) \\exp irt \\to f(t)\\]uniformly on \\(\\mathbb{T}\\).We start with the following remark.Suppose \\(f: \\mathbb{T} \\to \\mathbb{C}\\) is Riemann integrable.Then\\[\\begin{align}\\sigma_{n}(f,t) &amp;= \\sum_{r=-n}^{n} \\frac{n+1-|r|}{n+1} \\hat{f}(r) \\exp irt\\\\&amp;= \\sum_{r=-n}^{n} \\frac{n+1-|r|}{n+1}\\frac{1}{2\\pi} \\bigg(\\int_{\\mathbb{T}} f(x)\\exp(-irx)dx\\bigg) \\exp irt\\\\&amp;= \\frac{1}{2\\pi}\\int_{\\mathbb{T}} f(x)\\sum_{r=-n}^{n}\\frac{n+1-|r|}{n+1} \\exp(ir(t-x))dx\\\\&amp;= \\frac{1}{2\\pi} \\int_{\\mathbb{T}} f(x)K_n(t-x)dx\\end{align}\\]where\\[K_n(s) = \\sum_{r=-n}^{n} \\frac{n+1 - |r|}{n+1} \\exp irs\\]Further, making the substitution \\(y = t-x\\), we have\\[\\begin{align}\\sigma_{n}(f,t) &amp;= \\frac{1}{2\\pi} \\int_{-\\pi}^{\\pi} f(x)K_n(t-x)dx\\\\&amp;= -\\frac{1}{2\\pi} \\int_{t+\\pi}^{t-\\pi} f(t-y)K_n(y)dy\\\\&amp;= \\frac{1}{2\\pi} \\int_{t-\\pi}^{t+\\pi}f(t-y)K_n(y)dy\\\\&amp;= \\frac{1}{2\\pi}\\int_{\\mathbb{T}}f(t-y)K_n(y)dy\\end{align}\\]We are therefore led to examine the structure of \\(K_n\\) in some detail.Lemma 2.1.\\[\\begin{align}K_n(s) &amp;= \\frac{1}{n+1} \\Bigg( \\frac{\\sin \\frac{(n+1)s}{2}}{\\sin \\frac{s}{2}} \\Bigg)^2 \\quad \\quad [s \\ne 0]\\\\K_n(0) &amp;= n+1\\end{align}\\]Proof.If \\(s \\ne 0\\) then\\[\\begin{align}&amp;\\sum_{r=-n}^{n} (n+1-|r|) \\exp irs\\\\=&amp; \\bigg(\\sum_{k=0}^{n} \\exp i (k - \\frac{n}{2})s\\bigg)^2\\end{align}\\] Considering the number of situations of drawing two cards individually from two decks, the sum of two number gives \\(x\\).\\[\\begin{align}&amp;= \\bigg( \\exp - \\dfrac{ins}{2} \\sum_{k=0}^{n}\\exp iks \\bigg)^2\\\\&amp;= \\bigg( \\exp \\bigg( -\\dfrac{ins}{2} \\bigg) \\dfrac{1-\\exp i(n+1)s}{1 - \\exp is} \\bigg)^2\\\\&amp;= \\Bigg(\\dfrac{\\exp \\bigg( - \\dfrac{i(n+1)s}{2} \\bigg) - \\exp\\bigg( \\dfrac{i(n+1)s}{2} \\bigg)}{\\exp -\\dfrac{is}{2} - \\exp\\dfrac{is}{2}} \\Bigg)^2\\\\&amp;= \\Bigg( \\dfrac{\\sin \\dfrac{(n+1)s}{2}}{\\sin \\dfrac{s}{2}} \\Bigg)^2\\end{align}\\]If \\(s = 0\\) then \\(K_n(0) = n+1\\) by direct computation. $\\square$ " }, { "title": "Paper A Theory of Nonsubtractive Dither", "url": "/posts/paper-a-theory-of-dither/", "categories": "analog-circuit", "tags": "pll", "date": "2022-10-31 04:00:00 +0000", "snippet": "From Robert A. Wannamaker’s Paper1. IntroductionA. The Classical Model of QuantizationFor Mid-tread quantization\\[Q(w) = \\Delta \\lfloor \\frac{w}{\\Delta} + \\frac{1}{2} \\rfloor\\]where $\\lfloor . \\rfloor$ is floor function, e.g.,\\[\\begin{align}\\lfloor 0.5 \\rfloor &amp;= 0\\\\\\lfloor 1 \\rfloor &amp;= 1\\\\\\lfloor 1.5 \\rfloor &amp;= 1\\end{align}\\]B. Dither: Subtractive versus NonsubtractiveFor SD (subtractively diththered), total error is\\[\\varepsilon = q(x+\\nu)\\]For nonsubstractively dithered system\\[\\varepsilon = q(x+\\nu) + \\nu\\]Nonsubtractively ditherdefine total error\\[\\varepsilon \\overset{\\triangle}{=} output - input = D_{OUT} - D_{ACC}\\]It has been shown by Schuchman thatNonsubtractive Dither TheoryA. Total Error PDF’sThe input to the quantizer is \\(w=x+\\nu\\), which is the sum of the system input and the statistically independent dither process.This sum has a cpdf \\(p_{w|x}(w,x) = p_{\\nu}(w-x)\\).If the quantizer outputs \\(k\\Delta\\), the total error is \\(k\\Delta - x\\)\\[p_{\\varepsilon|x}(\\varepsilon,x) = \\sum_{k=-\\infty}^{\\infty} \\delta(\\varepsilon + x - k\\Delta) \\int_{-\\frac{\\Delta}{2} + k\\Delta}^{\\frac{\\Delta}{2} + k\\Delta} p_{\\nu}(w-x)dw\\]given \\(\\varepsilon\\) and \\(x\\), there is at most one \\(k\\) such that \\(\\delta(\\varepsilon + x - k \\Delta)\\) is not zero.Writing the integral in the last equation as a convolution (which is denoted by \\(*\\)) of \\(p_{\\nu}\\) with a rectangular window function \\(\\Delta \\Pi_{\\Delta}\\) reduces it to\\[p_{\\varepsilon|x}(\\varepsilon,x) = [\\Delta \\Pi_{\\Delta} * p_{\\nu}](\\varepsilon)W_{\\Delta}(\\varepsilon + x)\\]where\\[W_{\\Gamma}(\\varepsilon) \\overset{\\triangle}{=} \\sum_{k=-\\infty}^{\\infty} \\delta(\\varepsilon - k\\Gamma)\\]Thus, the pdf of \\(\\varepsilon\\) is given by (since \\(W_{\\Delta}(\\varepsilon)\\) is even function)\\[\\begin{align}p_{\\varepsilon}(\\varepsilon) &amp;= \\int_{-\\infty}^{\\infty} p_{\\varepsilon|x}(\\varepsilon,x) p_x(x)dx\\\\&amp;= [\\Delta\\Pi_{\\Delta} * p_{\\nu}](\\varepsilon) [W_{\\Delta} * p_x](-\\varepsilon)\\end{align}\\]Theorem:\\(E[\\varepsilon^m]\\) is independent of the distribution of the input \\(D_{ACC}\\) if and only if\\[\\frac{\\partial^m}{\\partial u^m}[\\mathrm{sinc}(u) \\cdot \\mathcal{F}[p_{\\nu}](u)] \\bigg|_{u=k} = 0\\]for all integer \\(k\\) with exception of 0. Where\\[\\begin{align*}\\mathrm{sinc}(u) &amp;= \\frac{\\sin(\\pi u)}{\\pi u}\\\\\\mathcal{F}[p_{\\nu}](u) &amp;= \\int_{-\\infty}^{\\infty} p_{\\nu}(x) e^{-j2\\pi u x} dx\\end{align*}\\]" }, { "title": "Fourier Analysis 001 Introduction", "url": "/posts/fourier-analysis-001/", "categories": "math", "tags": "fourier-analysis", "date": "2022-10-28 10:00:00 +0000", "snippet": "From book Fourier AnalysisIntroductionSuppose \\(f\\) is a Riemann integrable function from \\(0\\) to \\(2\\pi\\).\\[\\int_{0}^{2\\pi} f(t)dt \\quad \\text{exists}\\]Then we define the Fourier coefficients of \\(f\\) by\\[\\hat{f}(r) = (2\\pi)^{-1} \\int_{0}^{2\\pi} f(t)\\exp(-irt)dt\\]Mathematicians of the eighteenth century (Bernoulli, Euler, Lagrange, etc.) knew ‘experimentally’ that for some simple functions\\[S_n(f,t)=\\sum_{-n}^{n}\\hat{f}(r)\\exp irt \\to f(t) \\quad \\text{ as } n \\to \\infty\\]Fourier claimed that this was alwaus true and in a book of outstanding importance in the history of physics (Theorie Analytique de la Chaleur) showed how formulae of the kind \\(\\sum_{-\\infty}^{\\infty} \\hat{f}(r)\\exp irt\\) could be used to solve linear partial differential equations of the kind which dominated 19th century physics.After several mathematicians (including Cauchy) has produced more or less fallacious proofs of convergence, Dirichlet took up the problem.In a paper which set up new and previously undreamed of standards of rigour and clarity in analysis, he was able to prove convergence under quite general conditions.For example, the following theorem is a consequence of his results.Theorem 1.1If \\(f\\) is continuous and has a bounded continuous derivative except, possibly, at a finite number of points then \\(S_n(f,t) = \\sum_{-n}^{n}f(r)\\exp irt \\to f(t)\\) as \\(n \\to \\infty\\) at all points \\(t\\) where \\(f\\) is continuous.However, it turned out that the conditions on \\(f\\) could not be relaxed indefinitely for Du Bois-Reymond constructed the following counter example.Example 1.2There exists a continuous function \\(f\\) such that \\(\\lim \\sup_{n \\to \\infty} S_n (f,0) = \\infty\\).(Theorem 1.1 will be proved in Chapter 15 in the case \\(f\\) everywhere continuous and in Chapter 16 in the general case, whilst Example 1.2 will be constructed in Chpater 18.)A new question was thus posed.Question 1.3If \\(f\\) is a continuous function from \\(\\mathbb{T}\\) to \\(\\mathbb{C}\\) then, given the Fourier coefficients \\(\\hat{f}(r)[r \\in \\mathbb{Z}]\\) of \\(f\\), can we find \\(f(t)\\) for $t \\in \\mathbb{T}$?To the surprise of everybody Fejer (then aged only 19) showed that the answer is yes.He started from the observation that if a sequence $s_0, s_1, \\dots$ is not terribly well behaved, its behaviour may be improved by considering averages $s_0, (s_0+s_1)/2, (s_0+s_1+s_2)/3, \\dots$This has, of course, been known since the time of Euler, but the first person to study the phenomenon in detail was Cesaro about then years before Fejer’s discovery.Lemma 1.4(i)If $s_n \\to s$ then $(n+1)^{-1}\\sum_{j=0}^{n}s_j \\to s$.(ii)There exist sequences $s_n$ such that $s_n$ does not tend to a limit but $(n+1)^{-1}\\sum_{j=0}^{n}s_j$ does.(Thus the ‘Cesaro limit’ exists, and is equal to the usual limit, whenever the usual limit exists and may exist even if the usual limit does not.)Proof.(i) Let \\(\\varepsilon &gt; 0\\) be given.Since \\(s_n \\to s\\) we can find an \\(N(\\varepsilon)\\) such that \\(|s_n-s| \\le \\varepsilon/2\\) for \\(n \\ge N(\\varepsilon)\\).Set \\(A = \\sum_{j=0}^{N(\\varepsilon)} |s_j - s|\\) and choose \\(M(\\varepsilon)\\) such that \\(M(\\varepsilon) \\ge 2A \\varepsilon^{-1}\\).Then if \\(n \\ge M(\\varepsilon)\\), we have \\(A \\le M(\\varepsilon)\\varepsilon/2 \\le n\\varepsilon/2\\)\\[\\begin{align}&amp;|(n+1)^{-1} \\sum_{j=0}^{n} s_j - s|\\\\=&amp; (n+1)^{-1}|\\sum_{j=0}^{n} (s_j - s)|\\\\\\le &amp; (n+1)^{-1} \\sum_{j=0}^{n} |s_j - s|\\\\= &amp; (n+1)^{-1}(\\sum_{j=0}^{N(\\varepsilon)} |s_j - s| + \\sum_{j=N(\\varepsilon)+1}^{n} |s_j - s| )\\\\\\le &amp; (n+1)^{-1} (A + n \\varepsilon/2)\\\\\\le &amp; (n+1)^{-1} ((n+1)\\varepsilon/2 + (n+1)\\varepsilon/2)\\\\= &amp; \\varepsilon\\end{align}\\](ii) Let \\(s_n = (-1)^n\\) $\\square$ Fejer saw that although partial sums \\(S_n(f,t)=\\sum_{r=-n}^{n} \\hat{f}(t)\\exp irt\\) could fail to converge their averages\\[\\begin{align}\\sigma_{n}(f,t) &amp;= \\frac{1}{n+1}\\sum_{j=0}^{n} S_j(f,t)\\\\&amp;= \\frac{1}{n+1} \\sum_{j=0}^{n} \\sum_{r=-j}^{j} \\hat{f}(r)\\exp irt\\\\&amp;= \\sum_{r=-n}^{n} \\frac{n+1-|r|}{n+1} \\hat{f}(r) \\exp irt\\end{align}\\]might behave rather better and that a Casaro limit could take the place of the usual limit.Theorem 1.5(i)If \\(f: \\mathbb{T} \\to \\mathbb{C}\\) is Riemann integrable then, if \\(f\\) is continuous at \\(t\\),\\[\\sigma_{n}(f,t) = \\sum_{r=-n}^{n} \\frac{n+1-|r|}{n+1} \\hat{f}(r) \\exp irt \\to f(t)\\](ii)If \\(f:\\mathbb{T} \\to \\mathbb{C}\\) is continuous then\\[\\sigma_{n}(f,t) = \\sum_{r=-n}^{n} \\frac{n+1-|r|}{n+1} \\hat{f}(r) \\exp irt \\to f(t)\\]uniformly on \\(\\mathbb{T}\\).(Any reader discouraged by Fejer’s precocity should note that a few years earlier his school considered him so weak in mathematics as to require extra tuition.)" }, { "title": "Paper A -58dBc-Worst-Fractional-Spur and -234dB-FoM, 5.5GHz Ring-DCO-Based Fractional-N DPLL Using a Time-Invariant-Probability Modulator, Generating a Nonlinearity-Robust DTC-Control Word", "url": "/posts/pll-time-invariant-probaility-modulator/", "categories": "analog-circuit", "tags": "pll", "date": "2022-10-28 04:00:00 +0000", "snippet": "ISSCC 2020 17.3From Jaehyouk Choi’s PaperThe concept of this work is from Micheal Peter Kennedy’s Paper that showed that if the expected value of an arbitrary analog or digial signal \\(X\\), \\(E[X](t)\\), is constant over time, the power spectral density (PSD) of \\(X\\) shows no spurious tones.This leads to the idea that, if we can modulate \\(D_{DCW}\\) such that its probability density function (PDF) is time-invatiant, \\(E[\\tau_{DTC}](t)\\) becomes constant over time even after passing a nonlinear \\(f_{DTC}(D_{DCW})\\), so the PSD of \\(\\tau_{DTC}\\) has no fractoinal spurs. For a random process \\(Y\\), \\(E[Y](t)\\) is constant over time \\(\\implies\\) PSD of \\(Y\\) shows no spurs." }, { "title": "Paper Design of Crystal-Oscillator Frequency Quadrupler for Low-Jitter Clock Multipliers", "url": "/posts/design-of-xo-frequency-quadrupler-for-low-jitter-clock-multipliers/", "categories": "analog-circuit", "tags": "pll", "date": "2022-10-27 04:00:00 +0000", "snippet": "From Karim M. Megawer’s Paper integer-N 54-MHz-4.752-GHz RO-based ILCM 366fs 6.5-mW.IntroductionAssuming reference clock is generated by a low-noise XO, the in-band noise is typically dominated by PD and CP while VCO limits the out-of-band noise performance.One straightforward method to improve noise performance involves increasing the reference frequency, \\(F_{REF}\\). Higher \\(F_{REF}\\) lowers PD/CP noise contribution by reducing the multiplication factor, \\(N\\). reduces in-band quantization error of the \\(\\Delta \\Sigma\\) fractional divider by increasing over-sampling frequency. second ponit not clear to me helps to increase noise suppression bandwidth (BW) of VCO, which is about \\(F_{REF}/10\\).Higher \\(F_{REF}\\) also brings similar benefits to multiplying delay-locked loops (MDLLs), and injection-locked clock multipliers (ILCMs) by increasing their VCO noise supperssion BW.State-of-the-art (before this paper) achieve an excellent figure of merit (FoM) of arount -248 dB by limiting \\(N\\) to 4 and using 375-MHz and 2-GHz reference clocks, respectively.However higher frequency XO means higher cost.Thus this work designs a low jitter frequency quadrupler used for low frequency XO, then the signal can act as new reference for the following frequency synthesizers.Further study of the literature on ring oscillator (RO)-based clock multipliers reveals that to achieve better than −240-dB FoM, \\(N\\) is typically less than 40.To this end, we present a method to quadruple the frequency of a conventional 54-MHz Pierce XO.We then use the quadrupler output as the reference clock to an ILCM as illustrated in Fig. 1 and demonstrate a 54-MHz–4.752-GHz RO-based ILCM that achieves 366fsrms integrated jitter while consuming 6.5-mW power of which the reference generator (XO and quadrupler) consumes less than 1.5 mW.Reference Frequency GenerationOne possible way to achieve this goal is by using a frequency doubler circuit shown in Fig. 2.Implementing such a large delay either degrades phase noise or incurs a large power penalty.In view of this, we present an alternate method to quadruple the frequency of a conventional Pierce XO without needing large delays.This makes it practical for low-jitter low-power applications.Proposed XO Frequency QuadruplerConcept First doubler is composed of two comparator with two different threshold voltages. Second doubler is conventional doubler with small delay.Non-IdealitiesThree error patterns.Duty-Cycle Error CorrectionBecause ILCM can not exactly follow the period jitter of the reference, it can be used to produce the error signal." }, { "title": "Paper Clock Multiplication Techniques Using Digitial Multiplying Delay-Locked-Loops", "url": "/posts/clock-multiplication-techniques-using-digital-mdll/", "categories": "analog-circuit", "tags": "pll", "date": "2022-10-26 14:40:00 +0000", "snippet": "From Amr Elshazly’s PaperIntroductionA classical digital PLL composed of TDC, digital loop filter (DLF) and DCO (possibly DAC followed by VCO).Digital Multiplying DLL is different from classical digital PLL, in terms of noise transfers and many other things.Overview of Multiplying Delay-Locked LoopsUnlike classical PLLs, MDLLs in a sense remove jitter accumulation by edge realignment.Sheng Ye’s paper shows edge relaignment can effectively remove phase noise up to about \\(2 \\omega_{loop}\\) instead of \\(\\omega_{loop}\\) by high-pass filtering." }, { "title": "Applied Electromagnetics For Engineers 01", "url": "/posts/applied-em-01/", "categories": "analog-circuit", "tags": "tline", "date": "2022-10-24 14:40:00 +0000", "snippet": "Index Lecture 01 Lecture 02 Circuit Model of Transmission Line Lecture 03 Solution of Wave Equation Lecture 04 Reflection Coefficient and Transmission Coefficient Lecture 05 Circuit Parameters of a T-lineLecture 01Lecture 01 YoutubeLecture 02Lecture 02 YoutubeCircuit Model of Transmission Line The model only include inductors on the one side. However the calculatoin result will be the sametransmission line modelUsing KVL and KCL for the model\\[\\begin{align}v(z) + L \\Delta z \\frac{\\partial i(z,t)}{\\partial t} + v(z+\\Delta z) &amp;= 0\\\\i(z) - i(z+\\Delta z) - C \\Delta z \\frac{\\partial v(z+\\Delta z)}{\\partial t} &amp;= 0\\end{align}\\]that is\\[\\begin{align}\\frac{\\partial v(z,t)}{\\partial z} &amp;= -L \\frac{\\partial i(z,t)}{\\partial t}\\\\\\frac{\\partial i(z,t)}{\\partial z} &amp;= -C \\frac{\\partial v(z,t)}{\\partial t}\\end{align}\\]that is\\[\\begin{align}\\frac{\\partial^2 v}{\\partial z^2} &amp;= LC \\frac{\\partial^2 v}{\\partial t^2}\\\\\\frac{\\partial^2 i}{\\partial z^2} &amp;= LC \\frac{\\partial^2 i}{\\partial t^2}\\end{align}\\]Lecture 03Lecture 03 YoutubeSolution of Wave EquationFor the wave equation like the form\\[\\begin{align}\\frac{\\partial^2 v}{\\partial z^2} &amp;= LC \\frac{\\partial^2 v}{\\partial t^2}\\end{align}\\]the solution is \\(v(t,z) = f(t - \\sqrt{LC} z)\\) or \\(v(t,z) = f(t + \\sqrt{LC} z)\\) with reasonable \\(f(.)\\)if\\[v(t,z) = f(t-\\sqrt{LC}z)\\]then the wave is travelling to the positive direction of \\(z\\), with phase velocity\\[\\mu_{p} = \\dfrac{\\Delta z}{\\Delta t} = \\dfrac{1}{\\sqrt{LC}}\\]e.g.,\\[v(t,z) = \\cos(\\omega t - \\beta z)\\]where\\[\\dfrac{\\omega}{\\beta} = \\dfrac{1}{\\sqrt{LC}}\\]Phasor RepresentationIf we assume all the signal have the same angular frequency \\(\\omega\\), we can represent the signal by phasor. e.g., if the wave is transmited to the positive side of \\(z\\) axis,\\[\\begin{align}V(z) &amp;= V_0^{+} e^{-j\\beta z}\\\\I(z) &amp;= I_0^{+} e^{-j\\beta z}\\end{align}\\]is the phasor representation of $v(z,t) = V_0^{+}\\cos(\\omega t - \\beta z)$, and \\(i(z,t) = I_0^{+}\\cos(\\omega t - \\beta z)\\)using equation\\[\\begin{align}\\frac{\\partial v(z,t)}{\\partial z} &amp;= -L \\frac{\\partial i(z,t)}{\\partial t}\\\\\\end{align}\\]we can get\\[\\begin{align}I_0^+ &amp;= \\dfrac{V_0^+}{Z_0}\\\\Z_0 &amp;= \\sqrt{\\frac{L}{C}}\\end{align}\\]the \\(\\sqrt{L/C}\\) is called characteristic impedance of the transmission line.Lecture 04Lecture 04 YoutubeReflection Coefficientassume the wave is the superposition of two directions\\[\\begin{align}\\widetilde{V}(z) &amp;= V_0^+ e^{-j\\beta z} + V_0^- e^{j \\beta z}\\end{align}\\]then using equation\\[\\begin{align}\\frac{\\partial v(z,t)}{\\partial z} &amp;= -L \\frac{\\partial i(z,t)}{\\partial t}\\\\\\end{align}\\]we can get\\[\\begin{align}\\widetilde{I}(z) &amp;= I_0^{+} e^{-j\\beta z} + I_0^{-} e^{j\\beta z}\\\\&amp;= \\dfrac{V_{0}^+}{Z_0} e^{-j\\beta z} - \\dfrac{V_0^-}{Z_0} e^{j\\beta z}\\end{align}\\]now assume we put a load \\(Z_L\\) at \\(z=0\\) (the wave is coming from negative infinity size)\\[\\begin{align}\\widetilde{V}(z=0) &amp;= V_0^{+} + V_0^- = V_L\\\\\\widetilde{I}(z=0) &amp;= \\dfrac{V_0^+}{Z_0} - \\dfrac{V_0^-}{Z_0} = I_L\\\\Z_L &amp;= \\dfrac{V_L}{I_L}\\end{align}\\]from the equations we can get load reflection coefficient \\(\\Gamma_L\\)\\[\\Gamma_L = \\dfrac{V_0^-}{V_0^+} = \\dfrac{Z_L-Z_0}{Z_L+Z_0}\\] if \\(Z_L = \\infty\\) (the T-line is terminated by open circuit)\\[\\Gamma_L = 1\\]and\\[\\begin{align}\\widetilde{V}(z) &amp;= V_0^+ (e^{-j\\beta z} + e^{j\\beta z}) = 2 V_0^+ \\cos\\beta z\\\\v(z,t) &amp;= 2 V_0^+ \\cos\\beta z\\cdot \\cos\\omega t\\end{align}\\]the above is a standing wave. if \\(Z_L = 0\\) (the T-line is terminated by short circuit)\\[\\Gamma_L = -1\\]and\\[\\begin{align}\\widetilde{V}(z) &amp;= V_0^+(e^{-j\\beta z} - e^{j\\beta z}) = -2jV_0^+ \\sin \\beta z\\end{align}\\] if \\(Z_L = Z_0\\)\\[\\Gamma_L = 0\\]\\[\\begin{align}\\widetilde{V}(z) &amp;= V_0^+ e^{-j\\beta z}\\\\v(z,t) &amp;= V_0^+ \\cos(\\omega t - \\beta t)\\end{align}\\]Transmission Coefficienttransmission coefficient modelIn the model we didn’t consider the reflected wave after \\(Z_L\\), since we assume the next load is at infinty, thus it will take very long time to reflect back.We already made calculation for \\(V_i^+\\) and \\(V_r^-\\) if there is a \\(Z_L\\). Now we want to determine \\(V_t^+\\).\\[\\begin{align}V_i^+(z=0) + V_r^-(z=0) &amp;= V_L = V_t^+(z=0)\\\\\\dfrac{V_i^+(z=0)}{Z_{01}} - \\dfrac{V_r^-(z=0)}{Z_{01}} &amp;= I_L + \\dfrac{V_t^+(z=0)}{Z_{02}}\\end{align}\\]you can get reflection coefficient\\[\\begin{align}\\dfrac{V_r^+(z=0)}{V_i^+(z=0)} &amp;= \\Gamma_L = \\dfrac{Z_{L\\Vert} - Z_{01}}{Z_{L\\Vert} + Z_{01}}\\end{align}\\]where\\[Z_{L\\Vert} = \\dfrac{Z_L Z_{02}}{Z_L + Z_{02}} = Z_L \\Vert Z_{02}\\]and transmission coefficient\\[\\tau = \\dfrac{V_t^+(z=0)}{V_i^+(z=0)} = \\dfrac{2Z_{L\\Vert}}{Z_{L\\Vert} + Z_{01}}\\]Lecture 05Lecture 05 YoutubeA Paradox ExampleAssume a ideal voltage source (at \\(z=-l\\)) with voltage \\(V_s \\cos \\omega t\\) to drive a lossless T-line.The other end (at \\(z=0\\)) is terminated by a scope (with infity impedance).For the open circuit termination, reflection coefficient is 1, and\\[\\begin{align}\\widetilde{V}_{line}(z) &amp;= 2 V_0^+ \\cos \\beta z\\end{align}\\]thus at \\(z = 0\\)\\[v(t) = 2 V_0^+ \\cos\\omega t\\]at \\(z = -l\\)\\[v(t) = 2 V_0^+ \\cos \\beta l \\cdot \\cos \\omega t = V_s \\cos \\omega t\\]thus\\[\\begin{align}2V_0^+ &amp;= V_s / \\cos\\beta l\\end{align}\\]then by choosing some \\(l\\), e.g., \\(\\beta l=\\pi/2\\) ,you can get infinity amplified voltage at the scope.Real Voltage SourceIdeal voltage source doesn’t exist (otherwise it can provide infinity power to drive a short circuit load).Now we assume the voltage source \\(\\widetilde{V}_{s}\\) is followed by impedance \\(Z_s\\).And the load is modeled by \\(Z_L\\).\\[\\widetilde{V}_s = \\widetilde{I}_{line}(-l) \\cdot Z_s + \\widetilde{V}_{line}(-l)\\]if we assume \\(Z_s = \\infty\\) (open circuit), we know \\(\\Gamma_L = 1\\), and\\[\\begin{align}V_0^+ &amp;= V_0^-\\\\\\widetilde{V}_{line}(z) &amp;= 2 V_0^+ \\cos \\beta z\\\\\\widetilde{I}_{line}(z) &amp;= \\dfrac{V_0^+}{Z_0} e^{-j\\beta z} - \\dfrac{V_0^-}{Z_0} e^{j\\beta z}\\\\&amp;= \\dfrac{V_0^+}{Z_0} e^{-j\\beta z} - \\dfrac{V_0^+}{Z_0} e^{j\\beta z}\\end{align}\\]if we assume \\(\\beta l = \\pi/2\\)\\[\\widetilde{V}_s = 2 j \\dfrac{V_0^+}{Z_0} \\cdot Z_s\\]and\\[V_0^+ = -j\\dfrac{Z_0}{2Z_s} \\widetilde{V}_s\\]Line Impedance and Input Impedancewe know\\[\\begin{align}\\widetilde{V}_{line}(z) &amp;= V_0^+ e^{-j\\beta z} + V_0^- e^{j\\beta z}\\\\\\widetilde{I}_{line}(z) &amp;= \\dfrac{V_0^+}{Z_0} e^{-j \\beta z} - \\dfrac{V_0^-}{Z_0} e^{j \\beta z}\\end{align}\\]define line impedance (note the difference between line impedance and characteristic impedance)\\[\\begin{align}Z_{line}(z) &amp;= \\dfrac{\\widetilde{V}_{line}(z)}{\\widetilde{I}_{line}(z)}\\\\&amp;= \\dfrac{V_0^+ e^{-j\\beta z} + V_0^- e^{j\\beta z}}{\\dfrac{V_0^+}{Z_0} e^{-j \\beta z} - \\dfrac{V_0^-}{Z_0} e^{j \\beta z}}\\end{align}\\]thus we can find the input impedance\\[Z_{in} = Z_{line}(-l)\\]using\\[\\begin{align}V_0^- &amp;= \\Gamma_L V_0^+\\\\\\Gamma_L &amp;= \\dfrac{Z_L - Z_0}{Z_L + Z_0}\\end{align}\\]we have\\[\\begin{align}Z_{in} &amp;= Z_0 \\Bigg( \\dfrac{Z_L \\cos \\beta l + j Z_0 \\sin \\beta l}{Z_0 \\cos \\beta l + j Z_L \\sin \\beta l} \\Bigg)\\\\&amp;= Z_0 \\Bigg( \\dfrac{Z_L + j Z_0 \\tan \\beta l}{Z_0 + j Z_L \\tan \\beta l} \\Bigg)\\end{align}\\]we call \\(\\beta l\\) as electrical length.ExampleA voltage source \\(V_s = 10 \\cos (\\omega t + \\pi/6)\\), or phasor \\(10 \\cdot e^{j \\pi/6}\\)" }, { "title": "High-Frequency Design Techniques 03", "url": "/posts/hfdt-03/", "categories": "analog-circuit", "tags": "rf", "date": "2022-10-21 16:00:00 +0000", "snippet": "Quality FactorsThe Q of an inductor or capacitor is the ratio of its reactance to its resistance.Example 01A inductor and a resistor in series\\[Q = \\frac{\\omega L}{R}\\]Example 02A capacitor and a resistor in series\\[Q = \\frac{1}{\\omega CR}\\] In general \\(Q\\) will depend on frequency \\(\\omega\\)Example 03Four elements \\(L, R_L, C, R_C\\) in series, at the resonant frequency \\(\\omega_0\\)\\[Q = \\frac{\\omega_0 L}{R_L + R_C} = \\frac{1}{\\omega_0 C (R_L + R_C)}\\]" }, { "title": "Phase Noise in Signal 01", "url": "/posts/phase-noise-in-signal-01/", "categories": "analog-circuit", "tags": "analog", "date": "2022-10-11 15:00:00 +0000", "snippet": "Chapter 0 Signal’s Spectrum and Powerconsider a periodic signal \\(x(t)\\) with period \\(T_0\\)\\[\\begin{align}a_n &amp;= \\frac{1}{T_0} \\int_{T_0} x(t) e^{-jn\\omega_{0}t}dt\\\\x(t) &amp;= \\sum_{k=-\\infty}^{\\infty} a_k e^{jk\\omega_0 t} = \\sum_{k=-\\infty}^{\\infty} A_k e^{j\\phi_k} e^{jk\\omega_0 t}\\end{align}\\]Its spectrum can be described as tones with amplitude and phase \\(A_ke^{j\\phi_k}\\), for frequencies \\(k\\omega_0 (k = 0, \\pm 1, \\pm 2, \\dots)\\), and power\\[\\begin{align}\\frac{1}{T_0} \\int_{T_0} |x(t)|^2 dt = \\sum_{k=-\\infty}^{\\infty} |a_k|^2 = \\sum_{k=-\\infty}^{\\infty} |A_k|^2\\end{align}\\]For real signal \\(x(t)\\)\\[\\begin{align}A_k &amp;= A_{-k}\\\\\\phi_k &amp;= -\\phi_{-k}\\end{align}\\]thus\\[\\begin{align}x(t) &amp;= B_0 + \\sum_{k=1}^{\\infty} B_k \\cos(k\\omega_0 t + \\phi_k)\\\\B_0 &amp;= A_0\\\\B_k &amp;= 2 A_k\\end{align}\\]Its spectrum can also be described as tones with amplitude and phase \\(B_k \\angle \\phi_k\\), for frequencies \\(k\\omega_0 (k = 0, 1, 2, \\dots)\\) with cosine base, and power\\[\\begin{align}\\frac{1}{T_0} \\int_{T_0} |x(t)|^2 dt = |B_0|^2 + \\sum_{k=1}^{\\infty} \\frac{|B_k|^2}{2}\\end{align}\\]Chapter 2 Review of Modulation TheoryFrom Modulatoin Spectrum to Signal SpectrumAmplitude ModulationFor a carrier power \\(C\\)\\[V(t) = \\sqrt{2C} [1 + M'(t)] \\cos\\omega t\\]If\\[M'(t) = M'\\cos pt\\]the result\\[V(t) = \\sqrt{2C} [\\cos(\\omega t) + \\frac{M'}{2}\\cos(\\omega -p)t + \\frac{M'}{2}\\cos(\\omega + p)t]\\]It shows for AM, a tone in \\(M'(t) (M'\\angle 0 \\text{ at frequency } p)\\) with carrier \\(\\sqrt{2C}\\angle 0 \\text{ at frequency } \\omega\\), they will create three tones in the signal\\[\\begin{align}&amp; M'\\sqrt{C/2}\\angle 0 \\text{ at } \\omega - p\\\\&amp; \\sqrt{2C}\\angle 0 \\text{ at } \\omega\\\\&amp; M'\\sqrt{C/2}\\angle 0 \\text{ at } \\omega + p\\end{align}\\]Phase Modulation\\[\\begin{align}V(t) &amp;= \\sqrt{2C} \\cos(\\omega t + \\theta(t))\\\\V(t) &amp;= \\sqrt{2C} \\cos(\\omega t + \\theta \\cos p t)\\\\V(t) &amp; \\approx \\sqrt{2C} [\\cos \\omega t + \\frac{\\theta}{2} \\cos((\\omega+p)t+\\frac{\\pi}{2}) + \\cos((\\omega-p)t + \\frac{\\pi}{2})]\\end{align}\\]It shows for PM, a tone in \\(\\theta(t) (\\theta\\angle 0) \\text{ at } p\\) with carrier \\(\\sqrt{2C} \\angle 0 \\text{ at } \\omega\\), they will create three tones in the signal\\[\\begin{align}&amp; \\theta \\sqrt{C/2}\\angle \\frac{\\pi}{2} \\text{ at } \\omega - p\\\\&amp; \\sqrt{2C} \\angle 0 \\text{ at } \\omega\\\\&amp; \\theta \\sqrt{C/2}\\angle \\frac{\\pi}{2} \\text{ at } \\omega + p\\end{align}\\]Frequency ModulationThe Addition of an Arbitrary Phase AngleIn general for AM, a tone in \\(M'(t) (M'\\angle \\phi_m \\text{ at frequency } p)\\) with carrier \\(\\sqrt{2C}\\angle \\phi_c \\text{ at frequency } \\omega\\), they will create three tones in the signal\\[\\begin{align}&amp; M'\\sqrt{C/2}\\angle \\phi_c - \\phi_m \\text{ at } \\omega - p\\\\&amp; \\sqrt{2C}\\angle \\phi_c \\text{ at } \\omega\\\\&amp; M'\\sqrt{C/2}\\angle \\phi_c + \\phi_m \\text{ at } \\omega + p\\end{align}\\]Thus for AM, the two sidebands has phases sum to \\(2\\phi_c\\)In general for PM, a tone in \\(\\theta(t) (\\theta\\angle \\phi_m) \\text{ at } p\\) with carrier \\(\\sqrt{2C} \\angle \\phi_c \\text{ at } \\omega\\), they will create three tones in the signal\\[\\begin{align}&amp; \\theta \\sqrt{C/2}\\angle \\frac{\\pi}{2} + \\phi_c - \\phi_m \\text{ at } \\omega - p\\\\&amp; \\sqrt{2C} \\angle \\phi_c \\text{ at } \\omega\\\\&amp; \\theta \\sqrt{C/2}\\angle \\frac{\\pi}{2} + \\phi_c + \\phi_m \\text{ at } \\omega + p\\end{align}\\]Thus for AM, the two sidebands has phases sum to \\(2\\phi_c + \\pi\\)Linear ApproximationPM are not linear.However, for very small AM and PM amplitude, we assume we can use superposition, meaning it we have both AM and PM with many tones\\[V(t) = \\sqrt{2C}[1+M'(t)]\\cos(\\omega t + \\theta(t))\\]we assume by appliying each tones in AM or PM, they will give all the correct sidebands for the signal.Phasor RepresentationPower InterpretationA tone in the modulation with power \\(N_0\\) creates two sidebands, each has power\\[\\overline{\\phi_o^2} = \\frac{2N_{op}}{C} = 2L\\]or amplitude\\[\\theta = \\sqrt{4N_{op}/C}\\]Chapter 3 The Relationship Between Phase Jitter And Noise DensityFrom Signal Spectrum to Modulatoin SpectrumThe Representation of Narrow Band Noise\\[V_n(t) = \\sqrt{n_1} \\cos[(\\omega + p_1)t + \\phi_1] + \\sqrt{n_2} \\cos[(\\omega + p_2) t + \\phi_2] + \\dots\\]Phase Jitter Due to Superposed Single Sideband NoiseConvert Signal Spectrum to PM Modulation SpectrumConsidering signal spectrum (assume \\(p &gt; 0\\))\\[\\begin{align}&amp; \\sqrt{2C} \\angle 0 \\text{ at } \\omega\\\\&amp; \\sqrt{2N_0} \\angle \\phi \\text{ at } \\omega + p\\end{align}\\]This can be convert to spectrum due to PM\\[\\begin{align}&amp; \\sqrt{N_0/2} \\angle \\pi - \\phi \\text{ at } \\omega - p\\\\&amp; \\sqrt{2C} \\angle 0 \\text{ at } \\omega\\\\&amp; \\sqrt{N_0/2} \\angle \\phi \\text{ at } \\omega + p\\end{align}\\]That is modulation spectrum\\[\\begin{align}N_{op} &amp;= N_0/4\\\\&amp;\\sqrt{N_0/C}\\angle \\phi - \\frac{\\pi}{2} \\text{ at } p\\end{align}\\]Considering signal spectrum (assume \\(p &gt; 0\\))\\[\\begin{align}&amp; \\sqrt{2C} \\angle 0 \\text{ at } \\omega\\\\&amp; \\sqrt{2N_0} \\angle \\phi \\text{ at } \\omega - p\\end{align}\\]This can be convert to spectrum due to PM\\[\\begin{align}&amp; \\sqrt{N_0/2} \\angle \\phi \\text{ at } \\omega - p\\\\&amp; \\sqrt{2C} \\angle 0 \\text{ at } \\omega\\\\&amp; \\sqrt{N_0/2} \\angle \\pi - \\phi \\text{ at } \\omega + p\\end{align}\\]That is modulation spectrum\\[\\sqrt{N_0/C}\\angle \\frac{\\pi}{2} - \\phi \\text{ at } p\\]Convert Signal Spectrum to AM Modulation SpectrumPhase Jitter due to Superposed Double Sideband NoiseConsidering signal spectrum (assume p &gt; 0)\\[\\begin{align}&amp; \\sqrt{2n_1}\\angle \\phi_1 \\text{ at } \\omega-p\\\\&amp; \\sqrt{2N_0} \\angle 0 \\text{ at } \\omega\\\\&amp; \\sqrt{2n_2} \\angle \\phi_2 \\text{ at } \\omega+p\\end{align}\\]spectrum due to PM\\[\\begin{align}&amp; \\sqrt{n_1/2} \\angle \\phi_1 \\text{ at } \\omega - p\\\\&amp; \\sqrt{n_1/2} \\angle \\pi - \\phi \\text{ at } \\omega + p\\\\&amp; \\sqrt{2N_0} \\angle 0 \\text{ at } \\omega\\\\&amp; \\sqrt{n_2/2} \\angle \\pi - \\phi_2 \\text{ at } \\omega - p\\\\&amp; \\sqrt{n_2/2} \\angle \\phi_2 \\text{ at } \\omega + p\\end{align}\\]Thus\\[\\begin{align}L &amp;= \\frac{N_0}{2C}\\\\S_\\phi &amp;= 2L = \\frac{N_0}{C}\\end{align}\\]Chapter 4 Noise Induced Frequency Deviation" }, { "title": "Trigonometry", "url": "/posts/trigonometry/", "categories": "math", "tags": "basics", "date": "2022-10-10 10:00:00 +0000", "snippet": "Remember\\[\\begin{align*} \\sin(\\alpha + \\beta) &amp;= \\sin\\alpha\\cos\\beta + \\cos\\alpha\\sin\\beta\\\\ \\sin(\\alpha - \\beta) &amp;= \\sin\\alpha \\cos\\beta - \\cos\\alpha \\sin \\beta\\\\ \\cos(\\alpha + \\beta) &amp;= \\cos\\alpha \\cos\\beta - \\sin\\alpha \\sin\\beta\\\\ \\cos(\\alpha - \\beta) &amp;= \\cos\\alpha\\cos\\beta + \\sin\\alpha \\sin\\beta\\\\\\end{align*}\\]TODO\\[\\begin{align*} \\cot \\theta &amp;= \\frac{1}{\\tan\\theta}\\\\ \\sec \\theta &amp;= \\frac{1}{\\cos\\theta}\\\\ \\csc \\theta &amp;= \\frac{1}{\\sin\\theta}\\\\ \\frac{e^{i\\theta} + e^{-i\\theta}}{2} &amp;= \\cos\\theta\\\\ \\frac{e^{i\\theta} - e^{-i\\theta}}{2i} &amp;= \\sin\\theta\\\\ sinc(x) &amp;= \\frac{\\sin(\\pi x)}{ \\pi x}\\\\ \\sin(\\alpha + \\beta) &amp;= \\sin\\alpha\\cos\\beta + \\cos\\alpha\\sin\\beta\\\\ \\sin(\\alpha - \\beta) &amp;= \\sin\\alpha \\cos\\beta - \\cos\\alpha \\sin \\beta\\\\ \\cos(\\alpha + \\beta) &amp;= \\cos\\alpha \\cos\\beta - \\sin\\alpha \\sin\\beta\\\\ \\cos(\\alpha - \\beta) &amp;= \\cos\\alpha\\cos\\beta + \\sin\\alpha \\sin\\beta\\\\ \\sin \\alpha \\cos \\beta &amp;= \\frac{1}{2}\\big( \\sin(\\alpha + \\beta) + \\sin(\\alpha -\\beta) \\big)\\\\ \\cos\\alpha \\sin\\beta &amp;= \\frac{1}{2}\\big( \\sin(\\alpha + \\beta) - \\sin(\\alpha - \\beta) \\big)\\\\ \\cos\\alpha \\cos\\beta &amp;= \\frac{1}{2}\\big( \\cos(\\alpha + \\beta) + \\cos(\\alpha - \\beta) \\big)\\\\ \\sin\\alpha \\sin\\beta &amp;= -\\frac{1}{2}\\big( \\cos(\\alpha + \\beta) - \\cos(\\alpha - \\beta) \\big)\\\\ A\\sin\\alpha + B\\cos\\alpha &amp;= \\sin(\\alpha+\\varphi)\\quad\\textrm{where }\\tan\\varphi = \\frac{B}{A}\\\\ A\\sin\\alpha - B\\cos\\alpha &amp;= \\sin(\\alpha - \\varphi)\\quad\\textrm{where }\\tan\\varphi = \\frac{B}{A}\\\\ A\\cos\\alpha - B\\sin\\alpha &amp;= \\cos(\\alpha + \\varphi)\\quad\\textrm{where }\\tan\\varphi = \\frac{B}{A}\\\\ A\\cos\\alpha + B\\sin\\alpha &amp;= \\cos(\\alpha - \\varphi)\\quad\\textrm{where }\\tan\\varphi = \\frac{B}{A}\\\\ \\sin^2\\theta &amp;= \\frac{1 - \\cos(2\\theta)}{2}\\\\ \\cos^2(\\theta) &amp;= \\frac{1 + \\cos(2\\theta)}{2}\\\\ \\cos^3(\\theta) &amp;= \\frac{3}{4}\\cos\\theta + \\frac{1}{4}\\cos 3\\theta\\\\ \\mathrm{E}[\\sin^2(t)] &amp;= \\frac{1}{2}\\\\\\end{align*}\\]" }, { "title": "An Introduction to Cyclostationary Noise", "url": "/posts/intro-cyclostationary-noise/", "categories": "analog-circuit", "tags": "analog", "date": "2022-10-08 14:00:00 +0000", "snippet": "from resources paper introduction to cyclostationary noise.IntroductionThis paper introduces the noise analysis for mixers, oscillators, samplers, and logic gates.Ensemble AverageNoise free systems are deterministic, meaning that repeating the same experiment produces the same result.Noisy systems are stochastic, repeating the same experiment produces slightly different results each time.An experiment is referred to as a trial and a group of experiments is referred to as an ensemble of trials, or simply an ensemble.Noise can be characterized by using a averages over the ensemble, called expectations, and denoted by the operator E.The expection is the limit of the ensemble average as the number of trial approaches infinity.Let \\(v_n\\) be a noisy signal.It can be separated into a purely noise free, or deterministic signal \\(v\\), and a stochastic signal that is pure noise\\[\\begin{align}v_n(t) &amp;= v(t) + n(t)\\\\E\\{v_n(t)\\} &amp;= v(t)\\\\E\\{n(t)\\} &amp;= 0\\\\var\\{n(t)\\} &amp;= E\\{n(t)^2\\}\\end{align}\\]A more general power-like quantity is the autocorrelation\\[R_n(t,\\tau) = E\\{n(t) n(t-\\tau)\\}\\]It is a measure of how points on the same signal separated by \\(\\tau\\) seconds are correlated.We have\\[var\\{(t)\\} = R_n(t,0)\\]By performing the Fourier transform of the autocorrelation function with respect to the variable \\(\\tau\\) and then averaging over \\(t\\), we obtain the time-averaged power spectral density, or PSD, that is measured by spectrum analyzers.Colored or Time-Correlated NoiseWhite noise after a LTI filter becomes colored or time-correlated noise.Cyclostationary or Frequency-Correlated NoiseFrequency-uncorrelated noise after a LTPV system becomes frequency-correlated noise.Considering its single-sided spectrum (for frequency greater than 0). noise is correlated at frequency \\(kf_0 + f\\) is correlated to \\(kf_0 - f\\) noise is correlated at frequency \\(kf_0 + f\\) for different integer \\(k\\)Circuits with time-varying operating points can cause the ensemble averages that describe noise to vary with time \\(t\\).If they vary in a periodic fashion, the noise is said to have cyclostationary properties, and the ensemble averages referred to as being cyclostationary.It can be said that cyclostationary noise is “shaped in time \\(t\\)”.One cannot tell that noise is cyclostationary by just observing the time-average PSD.In short shape in frequency\\time \\(\\iff\\) correlation in time\\frequencyCalculating NoiseWe then linearize the circuit about the periodic large signal operating point and apply the small stochastic signal to this linearized system.These linear time-varying systems generally are quite large and require special numerical techniques to be practical.The reader is referred to this and this for details of numerical implementations.Characterizing Cyclostationary NoiseThere are three common methods of characterizing cyclostationary noise.The time-average power spectral density is similar to what would be measured with a conventional spectrum analyzer.Since the analyzer has a very small effective input bandwidth, it ignores correlations in the noise and so ignores the cyclostationary nature of the noise.The second method is to use the spectrum along with information about the correlations in the noise between sidebands.The third.Time-Average Power Spectral DensityIf a stage that generates cyclostationary noise is followed by a filter whose passband is constrained to a single sidedband (the passband does not contain a harmonic and has a bandwidth of less than \\(f_0/2\\), where \\(f_0\\) is the fundamental frequency of the cyclostationarity), then the output of the filyer will be stationary.This is true because noise at different frequency will be uncorrelated.Consider a stage that generates cyclostationary noise with modulatoin frequency \\(f_1\\) that is followed by a stage whose transfer characteristics vary periodically at a frequency of \\(f_2\\) (such as mixer, sampler, etc.).Assume that \\(f_1\\) and \\(f_2\\) are non commensurate (ther is no \\(f_0\\) such that \\(f_1 = nf_0\\) and \\(f_2 = mf_0\\) with \\(n\\) and \\(m\\) both integers).Then there is no way to shift \\(f_1\\) by a multiple of \\(f_2\\) and have it fall on a correlated copy of itself.As a result, the cyclostationary nature of the noise at the output of the first stage can be ignored.The time-averaged power spectral density (PSD) can be used as the basis of a noise model when the subsequent stages eliminate or ignore the cyclostationary nature of the noise.That is saying we can use time-averaged PSD to model the input noise, however, the output noise are still cyclostationary by default.When a stage producing cyclostationary noise drives a subsequent stage that has a time-varying transfer function that is synchronous with the first, then ignoring the cyclostationary nature of the noise from the first stage (say by using the time-average PSD) generates incorrect results.AM &amp; PM Noise" }, { "title": "RF Simulation 01", "url": "/posts/rf-simulation-01/", "categories": "analog-circuit", "tags": "spectre", "date": "2022-10-08 08:00:00 +0000", "snippet": "TheoryAC analysisFor example, \\(v_{out} = v_{in}^2\\), then\\[\\begin{align}v_{out} &amp;= (v_{in,DC} + \\Delta v_{in})^2 \\\\ &amp;\\approx v_{in,DC}^2 + 2 \\cdot v_{in,DC} \\cdot \\Delta v_{in}\\end{align}\\]Then\\[\\Delta v_{out} \\approx 2 \\cdot v_{in,DC} \\cdot \\Delta v_{in}\\]PAC analysis\\[\\begin{align}\\left(a \\cos(\\omega_0 t) + \\Delta v_{in}\\right)^2 = \\left(\\dfrac{a}{2} e^{j \\omega_0 t} + \\dfrac{a}{2} e^{-j\\omega_0 t} + \\Delta v_{in}\\right)^2\\end{align}\\]OldFrom resources: book introduction to rf simulatoin and its application.Characteristics of RF CircuitsRF circuits have several unique characteristics that are barriers to the application of traditional circuit simmulation techniques.Researchers have developed many special purpose algorithms that overcome these barriers to provide practical simulation for RF circuits.Narrowband SignalsModulated carriers are characterized as having a periodic high-frequency carrier signal and a low-frequency modulation signal that using either AM, PM or FM.For example, a typical mobile telephone transmission has a 10-30 kHz modulation bandwidth riding on a 1-2 GHz carrier.The ratio between the lowest frequency present in the modulation and the frequency of the carrier is a measure of the relative frequency resolution required of the simulation.Thus transient analysis is expensive.Passing a narrowband signal through a nonliear circuit results in a broadband signal whose spectrum is relatively sparse, as shown in Figure 3.In general, this spectrum consists of clusters of frequencies near the harmonics of the carrier.RF simulators exploit the sparse nature of this spectrum in various ways and with varying degrees of success.Time-Varying Linear Nature of the RF Signal PathA simple change of perspective allows the mixer to be treated as having a single input and a near-linear, though oeriodicaly time-varying, transfer function.As an example, consider a mixer made from an ideal multiplier and followed by a low-pass filter.\\[v_{out}(t) = LPF\\{\\cos(\\omega_{LO} t) \\cdot v_{in}(t)\\}\\]or\\[\\begin{align}v_{out}(t) &amp;= \\int_{-\\infty}^{t} h_{LPF}(t-\\tau) \\cos(\\omega_{LO}\\tau) v_{in}(\\tau) d\\tau\\\\&amp;= \\int_{-\\infty}^{t} h(t,\\tau) v_{in}(\\tau) d\\tau\\end{align}\\]It is clearly a linear time varying (LTV) system with input \\(v_{in}\\), and the impulse response (for a impulse input at \\(\\tau\\)) is \\(h(t,\\tau)\\).Especially, it is a periodic LTV system, meaning\\[h(t,\\tau) = h(t + N \\cdot T, \\tau + N \\cdot T)\\]To show LTV system will induce frequency translation, consider \\(v_{in}(t) = m(t) \\cos(\\omega_c)t\\)\\[\\begin{align}v_{out}(t) &amp;= LPF\\{m(t) \\cos(\\omega_c t) \\cos(\\omega_{LO} t)\\}\\\\&amp;\\approx m(t) \\cos((\\omega_c - \\omega_{LO})t)\\end{align}\\]This demonstrates that a linear periodically-time-varying system (LPTV) implements frequency translation.Often we can assume that the information signal is small enough to allow the use of a linear approximation of the circuit from its input to its output.Linearizing a nonlinear circuit about a periodically-varying operating point extends small-signal analysis to circuits such as mixers, switched filters, samplers, and oscillators.Linear Passive ComponentsAt the high frequencies present in RF circuits, the passive components, such as transmission lines, spiral inductors, packages (including bond wires) and substrates, often play a significant role in the behavior of the circuit.The nature of such components often make then difficult to include in the simulation.There are different techniques to deal with such distributed components.Basic RF Building BlocksRF systems are constructed primarily using four basic building blocks: amplifiers, filters, mixers, and oscillators.Mixers" }, { "title": "Fourier Transforms", "url": "/posts/fourier-transforms/", "categories": "math", "tags": "fourier-analysis", "date": "2022-10-01 15:00:00 +0000", "snippet": "Fourier Seriesconsider a periodic signal \\(x(t)\\) with period \\(T_0\\)\\[\\begin{align}a_n &amp;= \\frac{1}{T_0} \\int_{T_0} x(t) e^{-jn\\omega_{0}t}dt\\\\x(t) &amp;= \\sum_{k=-\\infty}^{\\infty} a_k e^{jk\\omega_0 t}\\end{align}\\]Parseval’s Theorem\\[\\begin{align}\\frac{1}{T_0} \\int_{T_0} |x(t)|^2 dt = \\sum_{k=-\\infty}^{\\infty} |a_k|^2\\end{align}\\]Fourier TransformFor a signal \\(x(t)\\), if the integration \\(\\int_{-\\infty}^{\\infty} x(t) e^{-j\\omega t} dt\\) exists for all \\(-\\infty &lt; \\omega &lt; \\infty\\).\\[\\begin{align}X(\\omega) &amp;= \\int_{-\\infty}^{\\infty} x(t) e^{-j \\omega t} dt\\\\x(t) &amp;= \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} X(\\omega) e^{j\\omega t} d\\omega\\end{align}\\]   \\(x(t) \\text{ is real function } \\quad \\iff \\quad X(\\omega) = \\overline{X(-\\omega)}\\)\\[\\begin{align}\\overline{X(-\\omega)} &amp;= \\overline{\\int_{-\\infty}^{\\infty} x(t) e^{j\\omega t} dt}\\\\&amp;= \\int_{-\\infty}^{\\infty} x(t) e^{-j\\omega t} dt\\\\&amp;= X(\\omega)\\end{align}\\] Since Fourier transform and inverse Fourier transform is dual, we also have\\[X(\\omega) \\text{ is real function } \\iff x(t) = \\overline{x(-t)}\\] for example, if \\(x(t)\\) is real and even function, then we know \\(X(\\omega)\\) is also real and even function.Parseval’s Theorem\\[\\int_{-\\infty}^{\\infty} |x(t)|^2 dt = \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} |X(\\omega)|^2 d\\omega\\]Discrete-Time Fourier Seriesfor a discrete-time signal \\(x[n]\\) with period \\(N\\), let \\(\\Omega_0 = 2\\pi/N\\)\\[\\begin{align}a_k &amp;= \\frac{1}{N} \\sum_{n=0}^{N-1} x[n] e^{-jk\\Omega_0 n}\\\\x[n] &amp;= \\sum_{k=0}^{N-1} a_k e^{jk\\Omega_0 n}\\end{align}\\]Parseval’s Theorem\\[\\frac{1}{N} \\sum_{n=0}^{N-1} |x[n]|^2 = \\sum_{k=0}^{N-1} |a_k|^2\\]Sampling Theorem If \\(x[n]\\) is a sampling (with sampling frequency \\(f_s = 1/T_s\\)) of \\(x(t)\\), such that \\(N \\cdot T_s\\) is the period of \\(x(t)\\), may or may not be the fundamental period. If \\(x(t)\\) does not includes any tones with frequency higher or equal than \\(f_s/2\\) Then DT FS gives exactly the same coefficients as \\(x(t)\\) for frequencies \\(0, f_s/N, \\dots, (N/2 - 1) f_s/N\\).Discrete Fourier Transform\\[X[k] = \\sum_{n=0}^{N-1} x[n] e^{-jk \\frac{2\\pi}{N} n}\\]function psd = power_spectrum(signal, Fs) N = length(signal); spectrum = fft(signal)/N; psd = abs(spectrum).^2; psd = psd(1:N/2+1); psd(2:N/2) = 2*psd(2:N/2); f = (0:N/2)*Fs/N; semilogy(f, psd); title(\"SSB Power Spectrum Density\"); xlabel(\"f (Hz)\");endFourier Transform TableNon-Causal Signal \\(f(t)\\) with \\(\\, t \\in R\\) \\(F(\\omega)\\) ROC? \\(f(t)\\) \\(\\int_{-\\infty}^{\\infty} f(t)e^{-j\\omega t}\\)   \\(e^{-a\\vert t \\vert}, \\quad a &gt; 0\\) \\(\\dfrac{2a}{a^2+\\omega^2}\\)   \\(\\delta(t)\\) \\(1\\)   \\(1\\) \\(2\\pi \\delta(\\omega)\\)   " }, { "title": "TI Precision Lab 01", "url": "/posts/ti-precision-lab-01/", "categories": "analog-circuit", "tags": "analog", "date": "2022-09-30 17:30:00 +0000", "snippet": "2.1 Vos and IbInput Offset Voltage changing supply voltage or common mode voltage will affect Vos. Vos is typically caused by mismatch between the differential input pair. the tail resistor mismatch can be minimized by laser trimmed resistor. e.g., at TA=25, RL=10k, VCM = VOUT = midsupply. e.g., Vs = \\(\\pm\\)15V, VCM=0V TYP = 75uV, MAX = 150uV. where TYP means \\(\\pm \\sigma\\)=75uV, corresponding to 68% probability. you will never find one device exceed MAX.Input Bias CCCurrent, Input Offset Current bipolar base current. for CMOS it is due to leakage of input ESD protection diodes." }, { "title": "Online Resources", "url": "/posts/online-resources/", "categories": "ee", "tags": "analog", "date": "2022-09-29 17:00:00 +0000", "snippet": " TI Precision Lab CppSim Lectures" }, { "title": "Spectre DC Analysis", "url": "/posts/spectre-dc-analysis/", "categories": "analog-circuit", "tags": "spectre", "date": "2022-09-25 14:00:00 +0000", "snippet": "In DC analysis, equilibrium points are calculated.It is important to understand that: Circuits sometimes have more than one DC solution. The DC solution computed by the citcuit simulator may be unstable.Newton-Raphson MethodNewton’s method solves nonlinear algebraic equation by iteration.For equations\\[f(\\hat{v}) = 0\\]starting with an initial guess \\(v^{(0)}\\) and repeatedly solving the Newton-Raphson iteration equation\\[v^{(k+1)} = v^{(k)} - J^{-1}(v^{k}) f(v^{(k)})\\]where \\(J(v) = \\frac{d}{dv} f(v)\\) is called the Jacobian of \\(f\\) at \\(v\\).It represents the circuit linearized about \\(v\\).It is guaranteed to converge to \\(\\hat{v}\\) if \\(f\\) is continuously differentiable, if the solution is isolated, and if \\(v^{(0)}\\) is sufficiently close to \\(\\hat{v}\\).In circuit simulation, none of these three conditions are guaranteed, and so neither is convergence.Convergence CriteriaIn Spectre it uses both of two criteria.The first one is called Newton update convergence criterion.\\[|v_n^{(k)} - v_n^{(k-1)}| &lt; \\text{reltol} \\cdot v_{n,max} + \\text{vntol}\\]where typically\\[v_{n,max} = \\text{max}(|v_{n}^{(k)}|, |v_{n}^{(k-1)}|)\\]By default, reltol is 0.001 and vntol (called vabstol in Spectre) is 1uV.The second one is called Newton residue convergence criterion\\[|f_n(v^{(k)})| &lt; \\text{reltol} f_{n,max} + \\text{abstol}\\]where typically \\(f_{n,max}\\) is the absolute value of the largest current entering node \\(n\\) from any one branch.Typicaly, abstol (called iabstol in Spectre) is 1pA.Very Small ResistorsVery small resistors put difficulties to the Newton residue convergence criterion.Since very small voltage error can create large currents. Try to avoid small resistors. Use 0-volt voltage sources as current probes rather than small-valued resistors. Discard overly small parasitic resistors in semiconductors. Spectre automatically deletes all parasitic resistors smaller than a particular value, given by the model parameter minr. If it is necessary to use small valued parasitic resistors, increase iabstol to avoid convergence problems.Isolated SolutionsIf you attempt to simulate a circuit that does not have isolated solutions, the simulator usually fails with an obscure complaint about the matrix or the Jacobian being singular.Examples: Voltages of the subcitcuit can be raised or lowered by any amounts. loop of ideal inductors." }, { "title": "Zero Order Hold Fourier Series", "url": "/posts/zero-order-hold-fourier-series/", "categories": "math", "tags": "fourier-analysis", "date": "2022-09-23 07:00:00 +0000", "snippet": "For a function \\(\\sin(\\omega_0 t)\\), it has Fourier Series\\[\\sin(\\omega_0 t) = \\frac{1}{2j} e^{j\\omega_0 t} - \\frac{1}{2j}e^{-j\\omega_0 t}\\]For such a function with zero order hold with frequency \\(\\omega_s\\), assuming\\[\\omega_s = N \\omega_0\\]and\\[T_s = \\frac{2\\pi}{\\omega_s}\\]then we can calculate its Fourier Series \\(\\sum a_k e^{j k \\omega_0 t}\\)\\[\\begin{align}a_k &amp;= \\frac{\\omega_0}{2\\pi} \\sum_{m=0}^{N-1} \\int_{m \\cdot T_s}^{(m+1)T_s} \\sin(\\omega_0 \\cdot m \\cdot T_s) e^{-jk\\omega_0 t} dt\\end{align}\\]we can calculate, if \\(k=0\\)\\[\\begin{align}a_0 &amp;= \\frac{\\omega_0}{2\\pi} \\cdot T_s \\sum_{m=0}^{N-1} \\sin(\\omega_0 \\cdot m \\cdot T_s)\\\\&amp;= \\frac{1}{N} \\cdot \\frac{1}{2j} \\sum_{m=0}^{N-1} \\bigg( e^{j m \\omega_0 T_s} - e^{-jm\\omega_0 T_s}\\bigg)\\\\&amp;= 0\\end{align}\\]if \\(k \\ne 0\\)\\[\\begin{align}&amp;\\int_{m \\cdot T_s}^{(m+1)T_s} \\sin(\\omega_0 \\cdot m \\cdot T_s) e^{-jk\\omega_0 t} dt = \\sin(\\omega_0 \\cdot m \\cdot T_s) \\int_{m\\cdot T_s}^{(m+1)T_s} e^{-jk\\omega_0 t}\\\\=&amp; \\frac{1}{2j}\\bigg( e^{jm\\omega_0 T_s} - e^{-jm\\omega_0 T_s}\\bigg) \\cdot \\frac{1}{-jk\\omega_0} \\bigg( e^{-jk(m+1)\\omega_0 T_s} - e^{-jkm\\omega_0 T_s} \\bigg)\\\\=&amp; \\frac{1}{2k \\omega_0} \\bigg( Terms \\bigg)\\end{align}\\]where\\[\\begin{align}Terms =&amp; e^{-jk\\omega_0 T_s} \\cdot e^{j(1-k)m \\omega_0 T_s}\\\\&amp; - e^{j(1-k)m \\omega_0 T_s}\\\\&amp; - e^{-jk\\omega_0 T_s} e^{-j(1+k)m \\omega_0 T_s}\\\\&amp; + e^{-j(1+k)m \\omega_0 T_s}\\end{align}\\]using the theorm from DSSP (FFT chapter), summation of terms will be nonzero only when\\[e^{j(1-k)m\\omega_0 T_s} = 1 \\quad \\text{or} \\quad e^{-j(1+k)m\\omega_0 T_s} = 1\\]that is when\\[k = 1 + qN \\quad \\text{or} -1 + qN\\]the results\\[a_k = \\frac{1}{2j} \\cdot \\frac{1}{1+qN} \\cdot \\frac{\\sin(\\pi/N)}{\\pi/N} \\cdot e^{-j\\pi/N} \\quad \\text{when} \\quad k = 1+qN\\]and\\[a_k = \\frac{1}{2j} \\cdot \\frac{1}{-1+qN} \\cdot \\frac{\\sin(\\pi/N)}{\\pi/N} \\cdot e^{j\\pi/N} \\quad \\text{when} \\quad k = -1+qN\\]If subtract the sampling signal from the original signal, the remaining parts has fourier series as (approximately)\\[\\begin{equation*} a_k = \\left\\{ \\begin{aligned} &amp; -\\frac{\\pi}{2N}, \\quad \\quad \\quad \\quad \\quad \\quad \\quad k=1 \\\\ &amp;- \\frac{\\pi}{2N}, \\quad \\quad \\quad \\quad \\quad \\quad k=-1\\\\ &amp; \\frac{1}{2j} \\cdot \\frac{1}{1+qN}, \\quad \\quad \\quad k = 1 + qN\\\\ &amp; \\frac{1}{2j} \\cdot \\frac{1}{-1+qN}, \\quad \\quad k = -1 + qN\\\\ &amp; 0, \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\text{otherwise} \\end{aligned} \\right.\\end{equation*}\\]" }, { "title": "PLL Jitter Analysis", "url": "/posts/pll-jitter-analysis/", "categories": "analog-circuit", "tags": "analog, pll", "date": "2022-09-19 10:00:00 +0000", "snippet": "From PaperVCO Phase Noise and Benchmarking\\[\\begin{align}FOM_{VCO} &amp;= 10 \\log \\bigg(L_{VCO}(f_m) \\cdot \\frac{f_m^2}{f_{VCO}^2} \\cdot \\frac{P_{VCO}}{1mW}\\bigg)\\\\L_{VCO}(f_m) &amp;= \\frac{10^{FOM_{VCO}/10}}{P_{VCO}/1mW} \\cdot \\frac{f_{VCO}^2}{f_m^2}\\end{align}\\]where \\(L\\) is single sideband noise power to carrier power ratio.Loop Phase Noise and BenchmarkingTo calculate the output phase noise due to the reference, frequency divider, phase detector and the charge pump.Since they are low-pass filtered, at low frequency the phase noise level is\\[\\begin{align}L_{loop} = \\frac{S_{\\phi,loop}}{2} = \\frac{1}{2} \\cdot N^2 \\cdot \\bigg( S_{\\phi,ref} + S_{\\phi,div} + S_{\\phi, PD} + \\frac{S_{i,CP}}{K_d^2} \\bigg)\\end{align}\\]Then at high frequency\\[L_{loop}(f) = L_{loop} \\cdot \\big|\\frac{G(s)}{1+G(s)}\\big|^2\\]Phase Noise Due to the Reference Path, Divider, and PDAccording to this paper, the time jitter can be written in terms of the integral of the single-sided power spectral density (PSD) of the phase \\(S_{\\phi}\\) within the Nyquist band (why?)\\[\\begin{align}\\sigma_{t_0}^2 = \\frac{1}{4 \\pi^2 f_{out}^2} \\int_{0}^{f_{out}/2} S_{\\phi}(f) df\\end{align}\\]So\\[\\begin{align}S_{\\phi} = 8 \\pi^2 \\cdot f_{ref} \\cdot \\sigma_{t}^2\\end{align}\\]time jitter depending on the output voltage slope \\(\\alpha_{out}\\)\\[\\begin{align}\\sigma_{t}^2 = \\frac{\\bar{v_{n}^2}}{\\alpha_{out}^2} = \\frac{F_{n}\\cdot kT/C_{out}}{\\alpha_{out}^2}\\end{align}\\]the total power in for the reference path, divider and PD can be expressed as\\[P = f_{ref} \\cdot C_{tot} \\cdot V_{dd}^2\\]then time jitter can be expressed as\\[\\sigma_{t}^2 = \\frac{f_{ref}}{P} \\cdot \\bigg( \\frac{F_{n} \\cdot kT \\cdot V_{dd}^2 \\cdot C_{tot}/C_{out}}{\\alpha_{out}^2} \\bigg)\\]To minimize the output jitter, designer can optimize the circuit by choosing the relative sizes of components, e.g., to maximize \\(\\alpha_{out}\\).Once this optimization has been done, the jitter can always be reduced on a system level via admittance level scaling.Then the voltage slope at every node does not change.Thus, \\(C_{tot}/C_{out}\\) as well as \\(F_{n}\\) remains the same as all the node admittances scale toether.Therefore, on the system level, we can treat the bracketed part as a design-dependent constant and get\\[\\sigma_{t}^2 \\propto f_{ref}/P\\]So\\[S_{\\phi} \\propto f_{ref}^2/P\\]Phase Noise Due to the CP\\[\\begin{align}S_{i} &amp;= 8 kT \\gamma \\cdot (2 I_{CP}/ V_{ov})\\\\S_{i,CP} &amp;= S_{i} \\cdot (\\tau_{PD} / T_{ref})\\end{align}\\]Power\\[P_{CP} = I_{CP} V_{dd} \\tau_{PD} \\cdot f_{ref}\\]and using \\(K_{d} = I_{CP}/2\\pi\\)\\[\\frac{S_{i,CP}}{K_{d}^2} = \\frac{f_{ref}^2}{P_{CP}} \\cdot \\bigg( \\tau_{PD}^2 \\cdot \\frac{64 \\pi^2 \\gamma \\cdot kT \\cdot V_{dd}}{V_{ov}} \\bigg) \\propto \\frac{f_{ref}^2}{P_{CP}}\\]Loop Phase Noise BenchmarkingThus\\[L_{loop} \\propto N^2 \\cdot \\frac{f_{ref}^2}{P_{loop}} = \\frac{f_{out}^2}{P_{loop}}\\]define\\[\\begin{align}FOM_{loop} &amp;= 10 \\log \\bigg( L_{loop} \\cdot \\big( \\frac{1 Hz}{f_{out}} \\big)^2 \\cdot \\frac{P_{loop}}{1mW} \\bigg)\\\\L_{loop} &amp;= 10^{FOM_{loop}/10} \\cdot (\\frac{f_{out}}{1 Hz})^2 \\cdot \\frac{1mW}{P_{loop}}\\end{align}\\]PLL Jitter and BenchmarkingTo calculate the long-term PLL absolute jitter due to VCO\\[\\begin{align} \\sigma_{t,VCO}^2 = \\frac{1}{2\\pi^2 f_{out}^2} \\cdot \\int_{0}^{\\infty} L_{VCO}(f) \\cdot |H_{VCO}(s)|^2 d f\\end{align}\\]\\[\\sigma_{t,VCO}^2 = \\frac{2 L_{VCO} (f_r) \\cdot f_r^2}{f_{out}^2} \\cdot \\frac{f_{c,0}}{f_c} \\cdot \\int_{0}^{\\infty} \\bigg|\\frac{1}{s \\cdot [1 + G_0(s)]}\\bigg|^2 df\\]\\[\\sigma_{t,loop}^2 = \\frac{L_{loop}}{2\\pi^2 f_{out}^2} \\cdot \\frac{f_{c}}{f_{c,0}} \\cdot \\int_{0}^{\\infty} \\bigg| \\frac{G_{0}(s)}{1+G_0(s)} \\bigg|^2 df\\]\\[\\sigma_{t,PLL} = \\sigma_{t,VCO} + \\sigma_{t,loop}\\]\\[\\sigma_{t,PLL}^2 = \\frac{1}{P_{PLL}} \\cdot \\bigg( 10^{\\frac{FOM_{loop} + FOM_{VCO}}{20}} \\cdot \\frac{4}{\\pi} \\cdot \\frac{1 mW}{1 Hz} \\bigg) \\cdot \\square\\]\\[\\mathrm{FOM}_{\\mathrm{PLL}} = 10 \\log \\Big( \\big(\\frac{\\sigma_{\\mathrm{t,PLL}}}{1 \\mathrm{ s}}\\big)^2 \\cdot \\frac{P_{\\mathrm{PLL}}}{1 \\mathrm{ mW}} \\Big)\\]\\[\\mathrm{FOM}_{\\mathrm{PLL}} = 10 \\log \\Big( \\big(\\frac{\\sigma_{\\mathrm{t,PLL}}}{1 \\mathrm{ s}}\\big)^2 \\cdot \\frac{\\color{red}P_{\\mathrm{PLL}}}{1 \\mathrm{ mW}} \\Big)\\]\\[\\mathrm{FOM} = 10 \\log \\Big( \\big(\\frac{\\sigma_{\\mathrm{t,out}}}{1 \\mathrm{ s}}\\big)^2 \\cdot \\frac{P_{\\mathrm{total}}}{1 \\mathrm{ mW}} \\Big)\\]" }, { "title": "IT Tips", "url": "/posts/fresh-install/", "categories": "cs", "tags": "shell", "date": "2022-09-14 10:40:00 +0000", "snippet": "SpacemacsSPC w F # make-frameSPC w o # other-frameInstall Program Locally without sudo PermissionYou need to compile these from source. It should just be a matter ofapt-get source PACKAGE # Or download the sourcecode release from github./configure --prefix=$HOMEmakemake installCondaconda create -n envname python=3.9.7 ipythonconda acitvate envnamePDFGREPpdfgrep -in \"pattern\" *.pdf# -i, --ignore-case# -n, --page-numberThe binary would then be located in ~/bin.WSL Install Problem wsl –install (double slash), then restart Windows. Sometimes Ubuntu can’t be installed. It stucked at “Installing, this may take a few minutes”. Just hit ctrl+c to stop it. Try to open Ubuntu to see if it is successful. If not, use wsl -l to check if Ubuntu is installed or not. If it is not installed, uninstall Ubuntu in the Start Menu, then use wsl --install -d Ubuntu reinstall.Spacemacs Problem Spacemacs fail to download the packages Use emacs --insecureSource Code ProUse this-link to install Source Code Pro Font for Spacemacs.WSL GUI BlurryUse this-link to fix WSL GUI Blurry. It is okay to just change high DPI settings on the XLaunch shortcut. I don’t need to change the variables like GDK_SCALE etc.To launch the server at start, put config.xlaunch in Startup folder.test" }, { "title": "PLL 01", "url": "/posts/pll-01/", "categories": "analog-circuit", "tags": "analog, pll", "date": "2022-09-13 17:00:00 +0000", "snippet": "PLL Typical ArchitecturePLL Typical ArchitectureType-I PLLThe PLL using XOR as PD is an example of type-I PLL, since there is only one integrator (the VCO) in the loop.XOR is called PD since phase detectos produce little information if thet sense unequal frequencies at their inputs.Type-II PLLThe PLL using PFD/CP is an example of type-II PLL, since there are two integrators in the loop.\\[\\begin{align}K_d &amp;= \\dfrac{I_{cp}}{2\\pi}\\\\F_{LF}(s) &amp;= R + \\dfrac{1}{sC}\\end{align}\\]The PLL open-loop transfer function is\\[G(s) = \\dfrac{K_d}{N} \\cdot F_{LF}(s) \\cdot \\dfrac{K_{VCO}}{s}\\]The VCO phase noise can be expressed by\\[\\begin{align}\\mathcal{L}_{VCO}(f_m) &amp;= \\dfrac{10^{FOM_{VCO}/10}}{P_{VCO}/1mW} \\cdot \\dfrac{f_{VCO}^2}{f_m^2}\\end{align}\\]The noise transfer function from VCO to the PLL output is\\[H_{VCO}(s) = \\dfrac{1}{1+G(s)}\\]Thus the PLL output noise caused VCO can be calculated as\\[\\begin{align}\\sigma_{t,VCO}^2 &amp;= \\dfrac{1}{2\\pi^2 f_{out}^2} \\cdot \\int_{0}^{\\infty} \\mathcal{L}_{VCO} (f_m) \\cdot |H_{VCO}(j2\\pi f_m)|^2 df_m\\end{align}\\]The CP current noise is\\[\\begin{align}S_{i,CP} &amp;= 8 kT \\gamma g_m \\cdot (\\tau_{PD}/T_{ref})\\\\&amp;= 8kT \\gamma \\cdot (2 I_{CP}/V_{ov}) \\cdot (\\tau_{PD}/T_{ref})\\end{align}\\]The power of CP is\\[P_{CP} = I_{CP} V_{dd} \\cdot (\\tau_{PD}/T_{ref}) = I_{CP}V_{dd}\\tau_{PD} \\cdot f_{ref}\\]The in-band phase noise caused by CP is\\[\\begin{align}S_{\\phi,in-band,CP} &amp;\\approx \\dfrac{S_{i,CP}}{(K_d/N)^2}\\\\&amp;= \\dfrac{N^2 f_{ref}^2}{P_{CP}} \\cdot \\Bigg\\{ \\tau_{PD}^2 \\cdot \\dfrac{64\\pi^2 kT \\gamma V_{dd}}{V_{ov}} \\Bigg\\}\\end{align}\\]The output in-band phase noise caused by CP can be expressed by\\[\\mathcal{L}_{CP} = \\dfrac{1}{2} \\cdot S_{\\phi,in-band,CP} = \\dfrac{10^{FOM_{CP}/10}}{P_{CP}/1mW} \\cdot \\bigg(\\dfrac{f_{out}}{1 Hz}\\bigg)^2\\]The noise transfer function from CP to the PLL output is\\[H_{CP}(s) = \\dfrac{G(s)}{1+G(s)}\\]and the PLL output noise caused by CP can be calculated as\\[\\sigma_{t,CP}^2 = \\dfrac{1}{2\\pi^2 f_{out}^2} \\cdot \\int_{0}^{\\infty} \\mathcal{L}_{CP} \\cdot |H_{CP}(j2\\pi f_m)|^2 df_m\\]Optimal Bandwidthuse\\[\\begin{align}\\mathcal{L}_{\\mathrm{VCO}} &amp;= \\dfrac{\\mathcal{L}_{\\mathrm{VCO}}(f_r) \\cdot f_r^2}{f_m^2}\\\\\\sigma_{t,\\mathrm{VCO}}^2 &amp;= \\dfrac{1}{2\\pi^2 f_{out}^2} \\cdot \\int_{0}^{\\infty} \\mathcal{L}_{\\mathrm{VCO}} \\cdot \\Big| \\dfrac{1}{1+G(j2\\pi f_m)} \\Big|^2 df_m\\end{align}\\]Assuming a given open-loop transfer function \\(G_0(s)\\) that results in a closed-loop transfer function with a 3-dB bandwidth of \\(f_{c,0}\\), scaling the bandwidth to \\(f_c\\) while keeping the same shape (thus the phase margin) results in a new\\[G(s) = G_{0}(s \\cdot \\dfrac{f_{c,0}}{f_c})\\]then\\[\\sigma_{t,\\mathrm{VCO}}^2 = \\dfrac{2 \\mathcal{L}_{\\mathrm{VCO}}(f_r)\\cdot f_r^2}{f_{out}^2} \\cdot \\dfrac{f_{c,0}}{f_c} \\cdot \\int_{0}^{\\infty} \\Big| \\dfrac{1}{s\\cdot [1+G_0(s)]} \\Big|^2 df\\]Also, use\\[\\begin{align}\\sigma_{t,\\mathrm{Loop}}^2 &amp;= \\dfrac{1}{2\\pi^2 f_{out}^2} \\cdot \\int_{0}^{\\infty} \\mathcal{L}_{\\mathrm{Loop}} \\cdot \\Big| \\dfrac{G(j2\\pi f_m)}{1+G(j2\\pi f_m)} \\Big|^2 df_m\\\\&amp;= \\dfrac{\\mathcal{L}_{\\mathrm{Loop}}}{2\\pi^2 f_{out}^2} \\cdot \\dfrac{f_c}{f_{c,0}} \\cdot \\int_{0}^{\\infty} \\Big| \\dfrac{G_0(s)}{1+G_0(s)} \\Big|^2 df\\end{align}\\]Thus\\[f_{c,opt} = \\sqrt{\\dfrac{\\mathcal{L}_{\\mathrm{VCO}}(f_r)\\cdot f_r^2}{\\mathcal{L}_{Loop}}} \\cdot 2\\pi \\cdot \\sqrt{f_{c,0}^2 \\cdot \\dfrac{\\int_{0}^{\\infty}\\big|\\dfrac{1}{s\\cdot[1+G_0(s)]}\\big|^2 df}{\\int_{0}^{\\infty} \\big|\\dfrac{G_0(s)}{1+G_0(s)}\\big|^2 df}}\\]then\\[\\sigma_{t,\\mathrm{PLL,min}}^2 = 2 \\sqrt{\\dfrac{2 \\mathcal{L}_{\\mathrm{VCO}}(f_r)\\cdot f_r^2}{f_{out}^2} \\cdot \\dfrac{\\mathcal{L}_{\\mathrm{Loop}}}{2\\pi^2 f_{out}^2} \\cdot \\int_{0}^{\\infty} \\big|\\dfrac{G_0(s)}{1+G_0(s)}\\big|^2 df \\cdot \\int_{0}^{\\infty}\\big|\\dfrac{1}{s\\cdot[1+G_0(s)]}\\big|^2 df}\\]use\\[\\begin{align}\\dfrac{\\mathcal{L}_{\\mathrm{VCO}}(f_r) \\cdot f_r^2}{f_{out}^2} &amp;= \\dfrac{10^{\\mathrm{FOM_{VCO}}/10}}{P_{\\mathrm{VCO}}/1 \\mathrm{mW}}\\\\\\dfrac{\\mathcal{L}_{\\mathrm{Loop}}}{f_{out}^2} &amp;= \\dfrac{10^{\\mathrm{FOM_{Loop}}/10}}{P_{\\mathrm{Loop}}/1 \\mathrm{mW}}\\end{align}\\]then\\[\\sigma_{t,\\mathrm{PLL,min}}^2 = \\dfrac{1}{\\sqrt{P_{\\mathrm{Loop}}\\cdot P_{\\mathrm{VCO}}}} \\cdot 10^{(\\mathrm{FOM_{Loop}+FOM_{VCO}})/20} \\cdot \\sqrt{\\int_{0}^{\\infty} \\big|\\dfrac{G_0(s)}{1+G_0(s)}\\big|^2 df \\cdot \\int_{0}^{\\infty}\\big|\\dfrac{1}{s\\cdot[1+G_0(s)]}\\big|^2 df} \\cdot \\dfrac{2}{\\pi} \\cdot \\dfrac{1mW}{1Hz}\\]Then with a given power budget, the optimal is \\(P_{Loop}=P_{VCO}\\)SSPLL\\[\\begin{align}G(s) &amp;= K_d \\cdot F_{LF}(s) \\cdot \\dfrac{K_{VCO}}{s}\\\\H_{\\mathrm{VCO}}(s) &amp;= \\dfrac{1}{1+G(s)}\\end{align}\\]" }, { "title": "SICP Lecture 03A", "url": "/posts/sicp-lec-03a/", "categories": "cs, scheme", "tags": "sicp, scheme", "date": "2022-09-01 08:00:00 +0000", "snippet": "Write Good CodeThe idea of extracting functions needs practice.Here is another example.It multiply each elements in the list by n.(define (scale-list n l) (if (null? l) nil (cons (* (car l) n) (scale-list n (cdr l))) ) )We should write such function using map.(define (scale-list n l) (map (lambda(x) (* n x)) l)) When you write your code in this way, you stop to thinking about the control procedure of the program." }, { "title": "SICP Lecture 02A", "url": "/posts/sicp-lec-02a/", "categories": "cs, scheme", "tags": "sicp, scheme", "date": "2022-08-31 12:00:00 +0000", "snippet": "Write Good CodeLast time we have the codes for calculating square root.(define (abs x) (if (&lt; x 0) (- x) x))(define (square x) (* x x))(define (average x y) (/ (+ x y) 2))(define (sqrt x) (define (improve guess) (average guess (/ x guess))) (define (good-enough? guess) (&lt; (abs (- (square guess) x)) 0.001)) (define (try guess) (if (good-enough? guess) guess (try (improve guess)))) (try 1))The program works.However it doesn’t really make sense for the body of the sqrt function.It is a lot of mess inside, and no one can understand what is going on inside within a short time.The problem can be solved by understanding what sqrt function is really doing.For a function\\[f(y) = \\frac{y + \\frac{x}{y}}{2}\\]\\(\\sqrt{x}\\) is the fixed-point of \\(f(y)\\), e.g., \\(f(\\sqrt{x}) = \\sqrt{x}\\)we can already written down the code for sqrt function, without worring about the implementation of fixed-point(define (sqrt x) (fixed-point (lambda(y) (average (/ x y) y)) 1))(define (fixed-point f start) (define tolerance 0.0001) (define (close-enuf? u v) (&lt; (abs (- u v)) tolerance)) (define (iter old new) (if (close-enuf? old new) new (iter new (f new)))) (iter start (f start)))You may say it’s just moving the complexity from sqrt function to fixed-point function. I have some arguments about this sqrt is a very concrete function. In fact the meaning of sqrt has nothing to do with our recursive implementation. There are potentially many ways to perform square root. fixed-point however is much more abstract than sqrt. And we can easily see the relationship between the meaning of fixed-point and its recursive implementation. To sum up, whenever you have a very complicate implemtation for a rather concrete function, it is usually a potential signal of a deeper abstraction. Think hard what the concrete function is really doing. The key observation here is, you can already written down the code for sqrt function using fixed-point abstration, even before you have such fixed-point function implementation. Btw, people says MIT doesn’t teach SICP anymore. One reason is nowadays, people already have almost every possible abstraction avaliable as a Python package or library. Thus learning to find and use these library can be much more efficient than making wheels.Now that using the fixed-point of \\(f(y)= \\frac{y + \\frac{x}{y}}{2}\\) is much better than the original sqrt implementation. We can further improve it.The function we used for fixed-point is not very natural for square root.The natural function in the mind should be \\(f(y) = x/y\\), since \\(\\sqrt{x}\\) is also its fixed-point.However \\(f(y) = x/y\\) wouldn’t work (try \\(\\sqrt{2}\\) start from 1), it will oscillate.The idea of \\(f(y)= \\frac{y + \\frac{x}{y}}{2}\\) is just average damp, and it is not particular for square root.(define (sqrt x) (fixed-point (average-damp (lambda(y) (/ x y))) 1))(define average-damp (lambda (f) (lambda (x) (average (f x) x))))where average-damp is a higher-order function.It is equivalent to use(define (average-damp f) (lambda (x) (average (f x) x)))" }, { "title": "SICP Lecture 01", "url": "/posts/sicp-lec-01/", "categories": "cs, scheme", "tags": "sicp, scheme", "date": "2022-08-31 11:30:00 +0000", "snippet": "A simple example to show essential scheme syntax.(define (abs x) (if (&lt; x 0) (- x) x))(define (square x) (* x x))(define (average x y) (/ (+ x y) 2))(define (sqrt x) (define (improve guess) (average guess (/ x guess))) (define (good-enough? guess) (&lt; (abs (- (square guess) x)) 0.001)) (define (try guess) (if (good-enough? guess) guess (try (improve guess)))) (try 1))" }, { "title": "Bandwidth Estimation for Circuits", "url": "/posts/bandwidth-estimation/", "categories": "analog-circuit", "tags": "analog", "date": "2022-08-29 12:00:00 +0000", "snippet": "For a \\(n\\)-th order system\\[H(s) = \\frac{a_0 + a_1 s + a_2 s^2 + \\dots + a_m s^m}{1 + b_1 s + b_2 s^2 + \\dots + b_n s^n}\\]A typical circuit will have transfer function with \\(\\omega_{l}\\) and \\(\\omega_{h}\\).And between the two frequencies, the gain is flat \\(a_{mid}\\)Bandwidth Estimation Using \\(b_1\\) Coefficient A paper (which paper?) shows that \\(1/b_1\\) is always smaller than \\(\\omega_h\\).\\[\\begin{align}\\omega_h \\approx 1/b_1 = \\frac{1}{\\sum_{i=1}^{N} \\tau_{i}^{0}}\\end{align}\\]Example 01Example 01Ignore \\(r_o\\) of the transistor and let\\[\\begin{align}C_{\\pi} &amp;= 100 fF \\quad \\quad \\quad r_{\\pi} = 2.5 k\\Omega \\\\C_{\\mu} &amp;= 20 fF \\quad \\quad \\quad \\,\\, g_{m} = 40 mS\\\\C_{L} &amp;= 200 fF \\quad \\quad \\quad \\beta = 100\\end{align}\\]the time constants\\[\\begin{align}\\tau_{\\pi}^{0} &amp;= (R_1 \\Vert r_{\\pi}) C_{\\pi} = (1k\\Omega \\Vert 2.5k\\Omega) \\cdot 100 fF = 70 ps\\\\\\tau_{\\mu}^{0} &amp;= (R_{left} + R_{right} + G_m R_{left} R_{right}) C_{\\mu} = 1200 ps\\\\\\tau_{L}^{0} &amp;= R_2 C_L = 400 ps\\end{align}\\]thus\\[\\begin{align}b_1 &amp;= \\tau_{\\pi}^{0} + \\tau_{\\mu}^{0} + \\tau_{L}^{0} = 1670 ps\\\\\\omega_{h} &amp;= \\frac{1}{b_1} = 2 \\pi \\times 95 MHz\\end{align}\\]\\(\\omega_{l}\\) is not shown in the model. $\\square$ Bandpass ModelThe reactive elements can be divided into two (non-overlap) groups: one is responsible for \\(\\omega_l\\) another one is responsible for \\(\\omega_h\\) so we got two different circuits to calculate \\(\\omega_{l}\\) and \\(\\omega_{h}\\)For \\(\\omega_{h}\\) we use zero value time constant (ZVT), meaning all the other elements are at zero value.\\[\\begin{align}\\omega_h \\approx 1/b_1 = \\frac{1}{\\sum_{i=1}^{N} \\tau_{i}^{0}}\\end{align}\\]For \\(\\omega_{l}\\) we use infinite value time constant (IVT), meaning all the other elements are at infinite value.\\[\\begin{align}\\omega_{l} \\approx \\frac{b_{n-1}}{b_n} = \\sum_{i=1}^{M} \\frac{1}{\\tau_{i}^{\\infty}}\\end{align}\\]Example 02Example 02\\[\\omega_{h} = \\frac{1}{\\tau_{\\pi}^{0}} = \\frac{1}{(R_1 \\Vert r_{\\pi}) C_{\\pi}}\\]\\[\\omega_{l} = \\frac{1}{\\tau_{c}^{\\infty}} = \\frac{1}{(R_1 + r_{\\pi})C_c}\\] $\\square$ Taking Zeros Into AccountFrom Lecture and Paper by Prof. Ali Hajimiri.\\[\\omega_h \\approx \\frac{1}{\\sum_{i=1}^{N} \\tau_i^{0} (1 - |\\frac{H^i}{H^0}|)}\\]Not very sure about the proof of this approximation (modified time constant)!Example 03Example 03Try to make \\(C_1\\) smaller!\\[\\begin{align}C_1 &amp;= 4.3 pF \\quad \\quad \\, \\, r_{\\pi} = 2.5 k\\Omega\\\\C_L &amp;= 200 fF \\quad \\quad g_m = 40 mS\\\\C_{\\pi} &amp;= 100 fF \\quad \\quad R_1 = 1 k\\Omega\\\\C_{\\mu} &amp;= 20fF \\quad \\quad \\, \\, R_2 = 2 k\\Omega\\end{align}\\]If using bandpass model, and let \\(C_1\\) being coupling capacitor.\\[\\begin{align}b_1 &amp;= R_2 (C_{\\mu} + C_{L}) = 440 ps\\\\\\omega_{h} &amp;= \\frac{1}{b_1} = 2\\pi \\times 362 MHz\\\\\\tau_{1}^{\\infty} &amp;= (R_1 \\Vert r_{\\pi}) C_1 = 3ns\\\\\\omega_{l} &amp;= \\frac{1}{\\tau_{1}^{\\infty}} = 2\\pi \\times 53 MHz\\end{align}\\]If using modified time constant\\[\\begin{align}H^{0} &amp;= -\\frac{r_{\\pi}}{R_1 + r_{\\pi}} \\cdot g_m R_2 = -57 \\\\H^{\\pi} &amp;= 0\\\\H^{\\mu} &amp;= + \\frac{r_m \\Vert R_2}{R_1 + r_m \\Vert R_2} = 24 m\\\\H^{L} &amp;= 0\\\\H^{1} &amp;= -g_m R_2 = -80\\end{align}\\]with\\[\\begin{align}\\tau_{\\pi}^{0} &amp;= (R_1 \\Vert r_{\\pi}) C_{\\pi} = (1k\\Omega \\Vert 2.5k\\Omega) \\cdot 100 fF = 70 ps\\\\\\tau_{\\mu}^{0} &amp;= (R_{left} + R_{right} + G_m R_{left} R_{right}) C_{\\mu} = 1200 ps\\\\\\tau_{L}^{0} &amp;= R_2 C_L = 400 ps\\\\\\tau_{1}^{0} &amp;= (R_1 \\Vert r_{\\pi}) C_1 = 3070 ps\\end{align}\\]modified time conetant\\[\\begin{align}\\tau_{\\pi}' &amp;= 70ps\\\\\\tau_{\\mu}' &amp;= 1200ps\\\\\\tau_{L}' &amp;= 400ps\\\\\\tau_{1}' &amp;= -1200ps\\end{align}\\]then\\[\\begin{align}\\omega_h = 2\\pi \\times 339 MHz\\end{align}\\] $\\square$ Second Order System Model" }, { "title": "Transistor Small Signal Model", "url": "/posts/transistor-small-signal-model/", "categories": "analog-circuit", "tags": "analog", "date": "2022-08-29 06:00:00 +0000", "snippet": "MOSFET\\[\\begin{align} C_{ox} &amp;= \\frac{3.9 \\epsilon_0}{t_{ox}} = \\frac{3.9 \\times 8.85e-12}{t_{ox}}\\\\ I_D &amp;= \\frac{1}{2}\\mu_n C_{ox} \\frac{W}{L}(V_{GS}-V_{TH})^2(1+\\lambda V_{DS}) \\quad \\quad \\text{pinch-off region}\\\\ I_D &amp;= \\mu_n C_{ox} \\frac{W}{L}\\big( (V_{GS} - V_{TH})V_{DS} - \\frac{1}{2}V_{DS}^2 \\big) \\quad \\quad \\text{triode region}\\end{align}\\]bode effect (simply remember as source degeneration will have higher threshold voltage).\\[\\begin{align} V_{th} = V_{th0} + \\gamma (\\sqrt{|V_{SB}+2\\Phi_F|} - \\sqrt{|2\\Phi_F|})\\end{align}\\]\\[\\begin{align*} g_m &amp;= \\frac{d I_{DS}}{d V_{GS}} = \\mu_n C_{ox}\\frac{W}{L}(V_{GS} - V_{TH}) = \\frac{2 I_{DS}}{V_{GS} - V_{TH}} = \\sqrt{2\\mu_n C_{ox}\\frac{W}{L}I_{DS}}\\\\ g_{ds} &amp;= \\lambda I_{DS} \\quad \\quad r_o = \\frac{1}{\\lambda I_{DS}} = \\dfrac{L}{L_{typ}} \\cdot \\dfrac{1}{\\lambda_{typ} I_{DS}}\\\\ g_{mb} &amp;= g_m \\dfrac{\\gamma}{2\\sqrt{2\\phi_F + V_{SB}}}\\end{align*}\\]\\(\\pi\\) ModelMOSFET \\(\\pi\\) model The arrow indicates source of nmos or pmos. pmos has the same small signal model as nmos. However pmos has source at the top. The small signal model is flipped. \\(T\\) ModelMOSFET \\(T\\) modelBJT\\[\\begin{align}r_{\\pi} &amp;= \\beta r_m\\\\\\alpha &amp;= \\frac{\\beta}{\\beta+1}\\\\\\beta &amp;= \\frac{\\alpha}{1-\\alpha}\\end{align}\\]Transfer Function For a pole at frequency \\(\\omega_0\\), at \\(\\omega_0 / \\sqrt{3}\\), the phase drops \\(30^\\circ\\). At \\(\\omega_0\\), the phase drops \\(45^\\circ\\). For a first-order transfer function \\(H(s) = \\dfrac{a + b s}{1 + s \\tau}\\), the DC gain is \\(a\\), the prop path is \\(\\dfrac{b}{\\tau}\\), with time constant \\(\\tau\\)." }, { "title": "Resistance Calculation", "url": "/posts/impedance-calculation/", "categories": "analog-circuit", "tags": "analog", "date": "2022-08-27 16:00:00 +0000", "snippet": "From Lectures by Prof. Ali Hajimiri.Example 01Example 01Solution\\[\\begin{align}R_{\\pi} &amp;= \\frac{R_1 + R_2}{1+g_m R_2}\\\\R_{\\mu} &amp;= R_1 + R_3 + G_m R_1 R_3\\\\G_m &amp;= \\frac{g_m}{1+g_m R_2}\\\\R_{\\theta} &amp;= \\frac{R_2 + R_3}{1+g_m R_2}\\end{align}\\]Step-by-Step SolutionThe resistance looking into the port \\(\\pi\\)The resistance looking into the port \\(\\pi\\)KCL\\[\\begin{align}i_1 &amp;= i_x\\\\i_2 + i_s = i_x \\quad \\implies i_2 &amp;= i_x - i_s = i_x - \\frac{v_x}{r_m}\\end{align}\\]KVL\\[\\begin{align}i_1 R_1 + i_2 R_2 = v_x\\end{align}\\]thus\\[\\begin{align}R_{\\pi} &amp;= \\frac{v_x}{i_x} = \\frac{R_1 + R_2}{1+g_m R_2}\\end{align}\\]The resistance looking into the port \\(\\mu\\)The resistance looking into the port \\(\\mu\\)using\\[\\begin{align}i_1 &amp;= i_x\\\\i_s (r_m + R_2) &amp;= - i_1 R_1\\\\i_3 + i_s &amp;= i_x\\\\v_x &amp;= R_1 i_1 + R_3 i_3\\end{align}\\]we got\\[\\begin{align}R_{\\mu} &amp;= R_1 + R_3 + G_m R_1 R_3\\\\G_m &amp;= \\frac{g_m}{1+g_m R_2}\\end{align}\\]The resistance looking into the port \\(\\theta\\)The resistance looking into the port \\(\\theta\\)using\\[\\begin{align}r_m i_s &amp;= R_2 i_2\\\\i_2 + i_s &amp;= i_x\\\\i_3 + i_s &amp;= i_x\\\\i_3 R_3 + i_2 R_2 &amp;= v_x\\end{align}\\]we got\\[R_{\\theta} = \\frac{R_2 + R_3}{1+g_m R_2}\\]Example 02Example 02Solution\\[\\begin{align}R_{\\pi} &amp;= r_{\\pi} \\Vert \\frac{R_1 + R_2}{1+g_m R_2}\\\\R_{\\mu} &amp;= R_{left} + R_{right} + G_m R_{left} R_{right}\\\\R_{left} &amp;= R_1 \\Vert [r_{\\pi} + (1+\\beta) R_2]\\\\R_{right} &amp;= R_3\\\\G_{m} &amp;= \\frac{g_m}{1 + g_m R_2}\\\\R_{\\theta} &amp;= \\frac{R_2 + R_3}{1+\\frac{1+\\beta}{\\beta+g_m R_1} g_m R_2} \\approx \\frac{R_2 + R_3}{1 + g_m R_2}\\end{align}\\]" }, { "title": "Time and Transfer Constants in Circuits", "url": "/posts/time-transfer-constants/", "categories": "analog-circuit", "tags": "analog", "date": "2022-08-27 06:00:00 +0000", "snippet": "From Lectures by Prof. Ali Hajimiri.First Order System\\[\\begin{align}H(s) = \\frac{a_0 + a_1 s}{1 + b_1 s} = \\frac{H^{0} + \\tau H^{1}s}{1 + \\tau s}\\end{align}\\] \\(\\tau\\) is the time constant. For capacitor \\(\\tau = R C\\) For inductor \\(\\tau = L/R\\) \\(R\\) is the impedance looking into the port of \\(C\\) or \\(L\\) (when nulling the source). \\(H^{0}\\) is the gain at DC. \\(H^{1}\\) is transfer constant: the value of the transfer function when making \\(C\\) or \\(L\\) infinite value. Infinite \\(C\\) is equivalent to short circuit. Infinite \\(L\\) is equivalent to open circuit. Example 01Example 01\\[\\begin{align}\\tau &amp;= (R_1 + R_2 + g_m R_1 R_2) C_{\\mu}\\\\H^{0} &amp;= -g_m R_2\\\\H^{1} &amp;= \\frac{r_m \\Vert R_2}{R_1 + r_m \\Vert R_2}\\end{align}\\]the full transfer function is represented by\\[\\begin{align}H(s) = \\frac{H^{0} + \\tau H^{1}s}{1 + \\tau s}\\end{align}\\] $\\square$ General SystemsFor a \\(n\\)-th order system\\[\\begin{equation*} H(s) = \\frac{a_0 + a_1 s + a_2 s^2 + \\dots}{1 + b_1 s + b_2 s^2 + \\dots}\\end{equation*}\\]\\[\\begin{align*} b_1 &amp;= -\\sum_{i=1}^{N'}\\frac{1}{p_i} = \\sum_{i=1}^{N}\\tau_{i}^{0}\\\\ b_2 &amp;= \\sum_{i}\\sum_{j}^{1 \\le i&lt;j \\le N} \\tau_i^{0}\\tau_{j}^{i}\\\\ b_3 &amp;= \\sum_{i}\\sum_{j}\\sum_{k} \\tau_{i}^{0}\\tau_{j}^{i}\\tau_{k}^{ij} \\\\ &amp;\\dots \\\\ a_0 &amp;= H^{0}\\\\ a_1 &amp;= \\sum_{i=1}^{N}H^{i}\\tau_{i}^{0}\\\\ a_2 &amp;= \\sum_{i}\\sum_{j}\\tau_{i}^{0}\\tau_{j}^{i}H^{ij}\\\\ a_3 &amp;= \\sum_{i}\\sum_{j}\\sum_{k}\\tau_{i}^{0}\\tau_{j}^{i}\\tau_{k}^{ij}H^{ijk}\\\\ &amp;\\dots\\end{align*}\\]\\(b_1\\) CoefficientIf there are \\(N\\) energy storage elements (\\(L\\) or \\(C\\))\\[\\begin{align}b_1 &amp;= \\sum_{i=1}^{N} \\tau_{i}^{0}\\end{align}\\] \\(\\tau_{i}^{0}\\) is zero-valued time constant (ZVT) for energy storage element \\(i\\), when all the other elements are zero valued. zero \\(C\\) is equivalent to open circuit. zero \\(L\\) is equivalent to short circuit. the value of \\(b_1\\) may or may not reflect the bandwidth of \\(H(s)\\). I will use another post to explain bandwidth estimation techniques.Example 02Example 02Ignore \\(r_o\\) of the transistor and let\\[\\begin{align}C_{\\pi} &amp;= 100 fF \\quad \\quad \\quad r_{\\pi} = 2.5 k\\Omega \\\\C_{\\mu} &amp;= 20 fF \\quad \\quad \\quad \\,\\, g_{m} = 40 mS\\\\C_{L} &amp;= 200 fF \\quad \\quad \\quad \\beta = 100\\end{align}\\]the time constants\\[\\begin{align}\\tau_{\\pi}^{0} &amp;= (R_1 \\Vert r_{\\pi}) C_{\\pi} = (1k\\Omega \\Vert 2.5k\\Omega) \\cdot 100 fF = 70 ps\\\\\\tau_{\\mu}^{0} &amp;= (R_{left} + R_{right} + G_m R_{left} R_{right}) C_{\\mu} = 1200 ps\\\\\\tau_{L}^{0} &amp;= R_2 C_L = 400 ps\\end{align}\\]thus\\[b_1 = \\tau_{\\pi}^{0} + \\tau_{\\mu}^{0} + \\tau_{L}^{0} = 1670 ps\\] $\\square$ " }, { "title": "Noise in Circuits", "url": "/posts/circuit-noise/", "categories": "analog-circuit", "tags": "analog", "date": "2022-08-26 16:40:00 +0000", "snippet": "Noise Type Thermal noise:\\[\\overline{v_n^2}/ \\Delta f = 4 k T R, \\quad \\overline{i_n^2} / \\Delta f = \\frac{4kT}{R}\\] MOSFET channel is (non-uniform) resistor\\[\\overline{i_n^2}/\\Delta f = 4 kT \\gamma g_{d0}\\]\\[g_{do} = \\frac{\\partial I_{D}}{\\partial V_{DS}} \\Bigg|_{V_{DS}=0}\\] Shot noise:\\[\\overline{i_n^2} /\\Delta f = 2 q I_{DC}\\] Flicker noise:\\[\\overline{v_n^2} / \\Delta f = \\frac{K}{C_{ox}WL} \\cdot \\frac{1}{f}\\] Flicker noise doesn’t depend on bias current or temperature (really?). PMOS typically has smaller flicker noise than NMOS. Larger device (larger \\(WL\\)) will have smaller flicker noise.Output NoiseExample 01Example 01\\[\\begin{align}\\overline{v_{out}^2}/\\Delta f &amp;= 4kTR \\Big|\\frac{1}{1+j2\\pi f RC}\\Big|^2\\end{align}\\]\\[\\begin{align} \\overline{v_{out}^2}=&amp;\\int_{0}^{\\infty} \\Big| \\frac{1}{1 + j2\\pi f RC} \\Big|^2 4 kT R d f = 4 k T R \\int_{0}^{\\infty} \\frac{1}{1 + (2 \\pi R C f)^2} df \\\\ = &amp; \\frac{4 k T R}{2 \\pi RC}\\int_{0}^{\\infty} \\frac{2 \\pi R C}{1 + (2\\pi R C f)^2}df\\\\ =&amp; \\frac{2 kT}{\\pi C} \\int_{0}^{\\infty}\\frac{1}{1+x^2}dx\\\\ =&amp; \\frac{2 kT}{\\pi C} \\frac{\\pi}{2} = \\frac{kT}{C}\\end{align}\\]the calculation can be viewed as, using the cutoff frequency \\(f_c = \\frac{1}{2\\pi RC}\\) of \\(H(s) = \\frac{1}{1 + sRC}\\)\\[\\begin{align*} 4 k T R \\cdot \\frac{1}{2 \\pi RC} \\cdot \\frac{\\pi}{2} = \\frac{k T}{C}\\end{align*}\\]the additional factor \\(\\frac{\\pi}{2}\\) for equivalent noise bandwidth. $\\square$ Example 02Example 02Use T model for the nmos, we can find the transfer function\\[\\frac{v_{out}}{i_{n}} = \\frac{R}{2}\\]" }, { "title": "Physical Constants", "url": "/posts/physical-constants/", "categories": "physics", "tags": "", "date": "2022-08-26 10:50:00 +0000", "snippet": " 1e3 1e6 1e9 1e12 1e15 1e18 1e21 1e24 k M G T P E Z Y 1e-3 1e-6 1e-9 1e-12 1e-15 1e-18 1e-21 1e-24 m u n p f a z y \\[\\begin{align*} e &amp;= 1.602 \\times 10^{-19} C\\\\ h &amp;= 6.626 \\times 10^{-34} J \\cdot s\\\\ \\hbar &amp;= 1.055 \\times 10^{-34} J \\cdot s\\\\ c &amp;= 3 \\times 10^8 m \\cdot s^{-1}\\\\ \\epsilon_0 &amp;= 8.85 \\times 10^{-12} F \\cdot m^{-1}\\\\ \\mu_0 &amp;= 1.26 \\times 10^{-6} N \\cdot A^{-2}\\\\ m_e &amp;= 9.11 \\times 10^{-31} kg\\\\ k &amp;= 1.38 \\times 10^{-23} J \\cdot K^{-1}\\end{align*}\\]" }, { "title": "Xargs", "url": "/posts/xargs/", "categories": "cs, shell", "tags": "shell", "date": "2022-08-26 10:40:00 +0000", "snippet": " # put the output to the end of echols *str* | xargs echo # put the output between echo and hellols *str* | xargs -I {} echo {} hello" }, { "title": "Residual Theorem", "url": "/posts/residues/", "categories": "math", "tags": "calculus", "date": "2022-08-25 17:40:00 +0000", "snippet": "\\[\\begin{aligned}\\int_{C} f(s)ds = 2\\pi j\\sum_{k=1}^{m}d_k\\end{aligned}\\]where \\(C\\) is a positive contour(anti-clockwise), and there are \\(m\\) poles inside the contour.Each \\(d_k\\) is the residual of each pole.ResidualFor a \\(n\\)-th order pole at \\(s_0\\), its residual\\[d = \\frac{1}{(n-1)!} \\frac{d^{n-1}}{ds^{n-1}}(s-s_0)^n f(s)\\Bigg|_{s=s_0}\\]for example, for a first order pole\\[d = (s-s_0)f(s)\\Big|_{s=s_0}\\]Example 01\\[\\begin{aligned}\\int_{C} \\frac{ds}{a^2 - s^2}\\end{aligned}\\]where \\(C\\) is a positive contour around \\(s_0 = a\\). Since it is a first order pole\\[\\begin{aligned}d = \\frac{s - a}{a^2 - s^2} \\Bigg|_{s=a} = -\\frac{1}{a+s}\\Bigg|_{s=a} = -\\frac{1}{2a}\\end{aligned}\\]thus\\[\\begin{aligned}\\int_{C} \\frac{ds}{a^2 - s^2} = 2\\pi j \\Big(-\\frac{1}{2a}\\Big) = -j\\frac{\\pi}{a}\\end{aligned}\\]side noteusing Jordan’s Lemma, we can calculate the below real integral, using the above result\\[\\int_{-\\infty}^{\\infty}\\frac{dy}{a^2 + y^2} = -j (2 \\pi j) (-\\frac{1}{2a}) (-1) = \\frac{\\pi}{a}\\]" }, { "title": "Integration by Subsitution", "url": "/posts/integration-by-substitution/", "categories": "math", "tags": "calculus", "date": "2022-08-25 17:30:00 +0000", "snippet": "Example 01\\[\\begin{aligned}\\int_{1}^{2}xdx &amp;= 4\\int_{1}^{2}\\frac{1}{2}x \\cdot \\frac{1}{2} dx \\\\&amp;= 4 \\int_{1/2}^{1}t dt\\end{aligned}\\]Example 02\\[\\begin{aligned}\\int_{-\\infty}^{\\infty}\\frac{dy}{a^2+y^2} &amp;= -j\\int_{-\\infty}^{\\infty}\\frac{jdy}{a^2-(jy)^2} \\\\&amp;= -j\\int_{C}\\frac{dz}{a^2-z^2}\\end{aligned}\\]Example 03\\[X(\\omega) = \\int_{-\\infty}^{\\infty} x(t) e^{-j\\omega t} dt\\]\\[X(t) = \\int_{-\\infty}^{\\infty} x(u)e^{-jut} du\\]\\[\\omega = -u\\]san jian shi: bei ji han shu ji fen bian liang shang xia xian\\[X(t) = \\int_{\\dots}^{\\dots} x(-\\omega) e^{j\\omega t} d(-\\omega)\\]\\[X(t) = \\int_{\\dots}^{\\dots} x(-\\omega) e^{j\\omega t} (-1) d\\omega\\]\\[X(t) = \\int_{\\infty}^{-\\infty} x(-\\omega) e^{j\\omega t} (-1) d\\omega\\]" }, { "title": "Higher Order Functions", "url": "/posts/higher-order-functions/", "categories": "cs, python", "tags": "cs61a, python", "date": "2022-08-25 11:30:00 +0000", "snippet": "CS61Adef make_withdraw(balance, password): \"\"\"Return a password-protected withdraw function. &gt;&gt;&gt; w = make_withdraw(100, 'hax0r') &gt;&gt;&gt; w(25, 'hax0r') 75 &gt;&gt;&gt; error = w(90, 'hax0r') &gt;&gt;&gt; error 'Insufficient funds' &gt;&gt;&gt; error = w(25, 'hwat') &gt;&gt;&gt; error 'Incorrect password' &gt;&gt;&gt; new_bal = w(25, 'hax0r') &gt;&gt;&gt; new_bal 50 &gt;&gt;&gt; w(75, 'a') 'Incorrect password' &gt;&gt;&gt; w(10, 'hax0r') 40 &gt;&gt;&gt; w(20, 'n00b') 'Incorrect password' &gt;&gt;&gt; w(10, 'hax0r') \"Frozen account. Attempts: ['hwat', 'a', 'n00b']\" &gt;&gt;&gt; w(10, 'l33t') \"Frozen account. Attempts: ['hwat', 'a', 'n00b']\" &gt;&gt;&gt; type(w(10, 'l33t')) == str True \"\"\" \"*** YOUR CODE HERE ***\" times = 0 attempts = [] def bank(amount, pwd): nonlocal balance nonlocal times nonlocal attempts if times == 3: return \"Frozen account. Attempts: \" + str(attempts) elif pwd != password: times += 1 attempts.append(pwd) return 'Incorrect password' elif balance &lt; amount: return 'Insufficient funds' else: balance -= amount return balance return bankdef make_joint(withdraw, old_pass, new_pass): \"\"\"Return a password-protected withdraw function that has joint access to the balance of withdraw. &gt;&gt;&gt; w = make_withdraw(100, 'hax0r') &gt;&gt;&gt; w(25, 'hax0r') 75 &gt;&gt;&gt; make_joint(w, 'my', 'secret') 'Incorrect password' &gt;&gt;&gt; j = make_joint(w, 'hax0r', 'secret') &gt;&gt;&gt; w(25, 'secret') 'Incorrect password' &gt;&gt;&gt; j(25, 'secret') 50 &gt;&gt;&gt; j(25, 'hax0r') 25 &gt;&gt;&gt; j(100, 'secret') 'Insufficient funds' &gt;&gt;&gt; j2 = make_joint(j, 'secret', 'code') &gt;&gt;&gt; j2(5, 'code') 20 &gt;&gt;&gt; j2(5, 'secret') 15 &gt;&gt;&gt; j2(5, 'hax0r') 10 &gt;&gt;&gt; j2(25, 'password') 'Incorrect password' &gt;&gt;&gt; j2(5, 'secret') \"Frozen account. Attempts: ['my', 'secret', 'password']\" &gt;&gt;&gt; j(5, 'secret') \"Frozen account. Attempts: ['my', 'secret', 'password']\" &gt;&gt;&gt; w(5, 'hax0r') \"Frozen account. Attempts: ['my', 'secret', 'password']\" &gt;&gt;&gt; make_joint(w, 'hax0r', 'hello') \"Frozen account. Attempts: ['my', 'secret', 'password']\" \"\"\" \"*** YOUR CODE HERE ***\" check = withdraw(0, old_pass) if type(check) == str: return check def joint(amount, passwd): if passwd == new_pass: return withdraw(amount, old_pass) return withdraw(amount, passwd) return joint" }, { "title": "CS61A", "url": "/posts/cs61a-mutable-values/", "categories": "cs, python", "tags": "cs61a, python", "date": "2022-08-24 11:30:00 +0000", "snippet": "currently working on lab07CS61A Week 6 objects have attributes today.year and method today.strftime(...) objects with good design will do what it supposed to do str(tmr - today) objects contain built in informations today.strftime(%A %B %d). We didn’t pass in any vocabulary in the constructor.&gt;&gt;&gt; from datetime import date&gt;&gt;&gt; date&lt;class 'datetime.date'&gt;&gt;&gt;&gt; today = date(2022, 8, 24)&gt;&gt;&gt; tmr = date(2022, 8, 25)&gt;&gt;&gt; str(tmr - today)'1 day, 0:00:00'&gt;&gt;&gt; today.year2022&gt;&gt;&gt; today.month8&gt;&gt;&gt; today.strftime('%A %B %d')'Wednesday August 24'string is also object&gt;&gt;&gt; s = 'Hello'&gt;&gt;&gt; s.upper()'HELLO'&gt;&gt;&gt; s.lower()'hello'&gt;&gt;&gt; s.swapcase()'hELLO'&gt;&gt;&gt; a = 'A'&gt;&gt;&gt; ord(a)65&gt;&gt;&gt; hex(ord(a))'0x41'&gt;&gt;&gt; from unicodedata import name, lookup&gt;&gt;&gt; name('A')'LATIN CAPITAL LETTER A'&gt;&gt;&gt; name('a')'LATIN SMALL LETTER A'&gt;&gt;&gt; lookup('SNOWMAN')'☃'&gt;&gt;&gt; lookup('BABY')'👶'&gt;&gt;&gt; lookup('BABY').encode()b'\\xf0\\x9f\\x91\\xb6'Lists Are Mutable Objectes when assign an object to a name, name is just a alias.. all the different names point to the same object the differences between == and is&gt;&gt;&gt; a = [1, 2, 3]&gt;&gt;&gt; b = a&gt;&gt;&gt; b[1, 2, 3]&gt;&gt;&gt; b.append(4)&gt;&gt;&gt; b[1, 2, 3, 4]&gt;&gt;&gt; a[1, 2, 3, 4] if a list is passed to a function through argument, the function can modify the list. even if it is not passed to the argument, function can still change the lista = [1, 2, 3]func(a) # now a = [1, 2, 3, 4]g() # now a = [1, 2, 3, 4, 5] a very dangerous and confusing desgin is the default value for argument&gt;&gt;&gt; def f(s=[]):... s.append(1)... return s...&gt;&gt;&gt; f()[1]&gt;&gt;&gt; f()[1, 1]&gt;&gt;&gt; f()[1, 1, 1] When append, extend, addition, slicing lists, it can be very confusing which generate new lists. Be careful!Tuples Are Immutable Objects objects such as list and dict are mutable objects. tuple is similar to list, but it not mutable objects, it is not allowed to make new assignments to the tuple but it is not saying it’s impossible to change tuplea = ([1, 2], 3) # a[0] = 4 # NOT ALLOWED # it is not allowed to assign to a[0] # but you can do it on a[0][0]a[0][0] = 4 # now a = ([4, 2], 3)Iteratorlst = [1, 2, 3]t = iter(lst)next(t)or iterator for dictt = iter(d.keys()) # or iter(d)t = iter(d.values())t = iter(d.items())Built-in functions for iteration: many built-in Python sequence operations return iterators that compute results lazilymap(func, iterable)filter(func, iterable)zip(first_iter, second_iter)reversed(sequence)list(iterable)tuple(iterable)sorted(iterable)" } ]
