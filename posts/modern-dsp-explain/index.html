<!DOCTYPE html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.2.2" /><meta property="og:title" content="Modern Digital Signal Processing Explanation" /><meta property="og:locale" content="en" /><meta name="description" content="P1" /><meta property="og:description" content="P1" /><link rel="canonical" href="/posts/modern-dsp-explain/" /><meta property="og:url" content="/posts/modern-dsp-explain/" /><meta property="og:site_name" content="Looooooong" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2024-06-12T04:00:00+00:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Modern Digital Signal Processing Explanation" /><meta name="twitter:site" content="@twitter_username" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2024-06-23T19:58:25+00:00","datePublished":"2024-06-12T04:00:00+00:00","description":"P1","headline":"Modern Digital Signal Processing Explanation","mainEntityOfPage":{"@type":"WebPage","@id":"/posts/modern-dsp-explain/"},"url":"/posts/modern-dsp-explain/"}</script><title>Modern Digital Signal Processing Explanation | Looooooong</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Looooooong"><meta name="application-name" content="Looooooong"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get MODE_ATTR() { return "data-mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } static get ID() { return "mode-toggle"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } let self = this; /* always follow the system prefers */ this.sysDarkPrefers.addEventListener("change", () => { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.notify(); }); } /* constructor() */ get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get isLightMode() { return this.mode === ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } setDark() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_ATTR); sessionStorage.removeItem(ModeToggle.MODE_KEY); } /* Notify another plugins that the theme mode has changed */ notify() { window.postMessage({ direction: ModeToggle.ID, message: this.modeStatus }, "*"); } } /* ModeToggle */ const toggle = new ModeToggle(); function flipMode() { if (toggle.hasMode) { if (toggle.isSysDarkPrefer) { if (toggle.isLightMode) { toggle.clearMode(); } else { toggle.setLight(); } } else { if (toggle.isDarkMode) { toggle.clearMode(); } else { toggle.setDark(); } } } else { if (toggle.isSysDarkPrefer) { toggle.setLight(); } else { toggle.setDark(); } } toggle.notify(); } /* flipMode() */ </script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" class="mx-auto"> </a></div><div class="site-title mt-3"> <a href="/">Looooooong</a></div><div class="site-subtitle font-italic">Don't confuse them, if you cannot convince them.</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <button class="mode-toggle btn" aria-label="Switch Mode"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://github.com/github_username" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/twitter_username" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['helong_ms','outlook.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper"><div id="topbar" class="container d-flex align-items-center justify-content-between h-100 pl-3 pr-3 pl-md-4 pr-md-4"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>Modern Digital Signal Processing Explanation</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper" class="d-flex justify-content-center"><div id="main" class="container pl-xl-4 pr-xl-4"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-9 pr-xl-4"><div class="post pl-1 pr-1 pl-md-2 pr-md-2"><h1 data-toc-skip>Modern Digital Signal Processing Explanation</h1><div class="post-meta text-muted"> <span> Posted <em class="" data-ts="1718164800" data-df="ll" data-toggle="tooltip" data-placement="bottom"> Jun 12, 2024 </em> </span> <span> Updated <em class="" data-ts="1719172705" data-df="ll" data-toggle="tooltip" data-placement="bottom"> Jun 23, 2024 </em> </span><div class="d-flex justify-content-between"> <span> By <em> <a href="https://twitter.com/username">Long</a> </em> </span><div> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="2325 words"> <em>12 min</em> read</span></div></div></div><div class="post-content"><p><strong>P1</strong></p><p><a href="https://www.bilibili.com/video/BV1ga4y157L5/">video lectures</a></p><p>Textbook:</p><ul><li>S.M.kay, fundamental of statistical signal processing vol i ii, this book is easy to understand.<li>S. Haykin: adaptive filter theory, fifth edition.<li>Petre Stoica and Randolph Moses: spectral analysis of signal<li>Murphy, machine learning</ul><p>probability is defined on the subsets of sample spaces</p>\[P(A \cup B) = P(A) + P(B), A \cap B = \emptyset\] \[\begin{align} &amp; \forall A_1, A_2, \dots, A_k \dots \subseteq \Omega, A_i \cap A_j = \emptyset, \forall i,j\\ &amp; P(\bigcup\limits_{k=1}^{\infty} A_k) = \sum_{k=1}^{\infty} P(A_k), \text{(countable additivity)} \end{align}\] \[P(\bigcup\limits_{k=1}^{\infty} A_k) = \sum_{k=1}^{\infty} P(A_k) - \sum_{i &lt; j}P(A_i \cap A_j) + \sum_{i&lt;j&lt;k} P(A_i \cap A_j \cap A_k) - \dots \text{(inclusive exclusive)}\]<p>example for using inclusive exclusive: assuming n persons has n hats, now they randomly throw the hats and get one, what is the probability such that no one get the correct hat?</p><p>Let \(P(A_k)\) be the probability that person \(k\) get the correct hat, obviously</p>\[P(A_k) = \dfrac{(n-1)!}{n!} = \dfrac{1}{n}\]<p>the probability that no one get the correct hat</p>\[P(\overline{A_1} \cap \overline{A_2} \cap \dots \cap \overline{A_n})\]<p>using De Morgen’s law</p>\[P(\overline{A_1 \cup A_2 \cup \dots \cup A_n}) = 1 - P(A_1 \cup \dots \cup A_n)\] \[P(A_1 \cup \dots \cup A_n) = \sum_{k=1}^{n} P(A_k) - \sum_{i&lt;j}P(A_i \cap A_j) + \sum_{i&lt;j&lt;k}P(A_i \cap A_j \cap A_k) - \dots\]<p>where</p>\[\begin{align} P(A_k) &amp;= \dfrac{(n-1)!}{n!} = \dfrac{1}{n}\\ P(A_i \cap A_j) &amp;= \dfrac{(n-2)!}{n!} = \dfrac{1}{n(n-1)}\\ P(A_i \cap A_j \cap A_k) &amp;= \dfrac{(n-3)!}{n!} = \dfrac{1}{n(n-1)(n-2)}\\ &amp; \vdots \end{align}\] \[\begin{align} P(A_1 \cup \dots \cup A_n) &amp;= n \cdot \dfrac{1}{n} - \binom{n}{2} \dfrac{1}{n(n-1)} + \binom{n}{3} \dfrac{1}{n(n-1)(n-2)} - \dots\\ &amp;= 1 - \dfrac{1}{2!} + \dfrac{1}{3!} - \dots \end{align}\]<p>the probability is \(1 - P(A_1 \cup \dots \cup A_n )= \dfrac{1}{2!} - \dfrac{1}{3!} + \dfrac{1}{4!} - \dots\)</p><p><strong>independent</strong></p>\[P(AB) = P(A) \cdot P(B)\]<p><strong>conditional probability</strong></p>\[P(B|A) = P(AB)/P(A)\]<p>conditional probability is also a probability, defined on a new sample space, the original sample space is \(\Omega\), the new sample space is \(A\). related name for conditional inclusing prior.</p><p>if \(A\) and \(B\) are independent, \(P(B\vert A) = P(B)\), meaning knowing \(A\) doesn’t give information regarding \(B\).</p><p><strong>total probability formula</strong></p><p>if \(A_i \cap A_j = \emptyset\) and \(\bigcup\limits_{k=1}^n A_k = \Omega\)</p>\[P(B) = P(B|A_1)P(A_1) + \dots + P(B|A_n)P(A_n)\]<p>example: n person n hats, the first one will randomly pick one hat, then start from the second person, if his hat is still there, he will pick his own hat; otherwise he randomly pick one hat. For the last person, what is the probability that he pick his own hat?</p><p>Let the event \(A_n\) be person \(n\) pick his own hat, when there is in total \(n\) person,let the events \(B_1, B_2, \dots, B_n\) be the first person pick hat \(1, 2,\dots, n\). Using total probability formula</p>\[P(A_n) = P(A_n \vert B_1)P(B_1) + \dots + P(A_n \vert B_n) P(B_n)\] \[P(A_n \vert B_1) P(B_1) = 1 \cdot \dfrac{1}{n}\] \[P(A_n \vert B_n) P(B_n) = 0 \cdot \dfrac{1}{n}\]<p>for \(P(A_n \vert B_k)\), we know that person \(2, \dots, k-1\) pick their own hat, then person \(k\) need to pick randomly from the remainly \(n-k+1\) hats. We can imagin hat \(1\) is person \(k\)’s original hat, then</p>\[P(A_n \vert B_k) = P(A_{n-k+1})\] \[P(A_n) = 1 \cdot \dfrac{1}{n} + \left(\sum_{k=2}^{n-1} \dfrac{1}{n} P(A_{n-k+1})\right) + 0 \cdot \dfrac{1}{n}\] \[P(A_1) = 1\] \[P(A_2) = \dfrac{1}{2}\] \[P(A_3) = \dfrac{1}{3} + \dfrac{1}{3} P(A_2) = \dfrac{1}{2}\]<p>assumes \(P(A_2) = P(A_3) = \dots = P(A_{n-1}) = 1/2\)</p>\[P(A_n) = \dfrac{1}{n} + \dfrac{n-2}{n} \cdot \dfrac{1}{2} = \dfrac{1}{2}\]<p><strong>Bayesian</strong></p>\[P(B \vert A) = \dfrac{P(A \vert B) P(B)}{P(A)} = \dfrac{P(A \vert B)P(B)}{\sum_{k} P(A \vert C_k) P(C_k)}\]<p>example</p><p>The first bag has 4 black and 3 white balls, the second bag has 5 black and 2 white ball. Now randomly choose 2 balls from first bag to second bag, then pick one ball from second bag, we found it is white ball. Based on this condition what is the probability that the choosed 2 balls have same color?</p><p>Let event \(B\) be the 2 balls have same color, let event \(A\) be we found it is white. Let event \(C_1, C_2, C_3\) be the 2 balls are all white, all black and one white one black.</p>\[P(B\vert A) = \dfrac{P(AB)}{P(A)} = \dfrac{P(AB\vert C_1) P(C_1) + P(AB \vert C_2) P(C_2) + P(AB \vert C_3) P(C_3)}{P(A\vert C_1)P(C_1) + P(A\vert C_2)P(C_2) + P(A \vert C_3) P(C_3)}\]<p><strong>random variables</strong> is a determinstic function \(X: \Omega \to \mathbb{R}\).</p><p><strong>probability density function</strong> and <strong>culmilitive density function</strong></p><p><strong>Bernoulli distribution</strong></p><p><strong>Binomial distribution</strong> \(P(x=k) = \binom{n}{k}p^k (1-p)^{n-k}\)</p><p><strong>Possion distribution</strong> \(P(x=k) = \dfrac{\lambda^k}{k!}\exp(-\lambda)\)</p><p>possion distribution can be derived from binomial distribution, let \(n \to \infty, p \to 0\) while \(np=\lambda\)</p>\[\begin{align} P(x=k) &amp;= \dfrac{n!}{k!(n-k)!} \dfrac{\lambda^k}{n^k} \left(1 - \dfrac{\lambda}{n}\right)^{n-k}\\ &amp;=\dfrac{\lambda^k}{k!} \cdot \dfrac{n!}{(n-k)! \cdot n^k} \cdot \left(1 - \dfrac{\lambda}{n}\right)^n \cdot \left(1- \dfrac{\lambda}{n}\right)^{-k}\\ &amp;= \dfrac{\lambda^k}{k!} \cdot \dfrac{n \cdot (n-1) \dots (n-k+1)}{n^k} \cdot \left(1 - \dfrac{\lambda}{n}\right)^n \cdot \left(1- \dfrac{\lambda}{n}\right)^{-k}\\ &amp;= \dfrac{\lambda^k}{k!}\exp(-\lambda) \end{align}\]<p>here I use \(\lim_{n\to\infty} \left(1 + \dfrac{a}{n}\right)^n = \exp(a)\)</p><p><strong>uniform distribution</strong>: \(f(x) = \dfrac{1}{b-a}I_{[a,b]}(x)\)</p><p><strong>exponetional distribution</strong>: \(f(x) = \lambda \exp(-\lambda x)I_{[0,\infty)}(x)\)</p><p><strong>Gaussian distribution</strong>: \(f(x) = \dfrac{1}{\sqrt{2\pi\sigma^2}} \exp(-\dfrac{(x-\mu)^2}{2\sigma^2})\)</p><p><strong>P2</strong></p><p><strong>expectation (mean)</strong></p>\[E(X) = \int_{-\infty}^{\infty} x f(x) dx\]<p><strong>expectation is linear</strong></p>\[E\left(\sum_{k=1}^{n} X_k\right) = \sum_{k=1}^{n} E(X_k)\]<p>example</p><p>still \(N\) person randomly choose \(N\) hats, what is the expection number of the person pick their own hat?</p>\[E(X_1 + \dots X_N) = N \cdot \dfrac{1}{N} = 1\]<p><strong>variance</strong></p>\[Var(X) = E(X-EX)^2 = E(X^2) - (E(X))^2\]<p><strong>convex function</strong>: convex function is like a bowl, for example \(g(x) = x^2\)</p>\[g(\alpha x + (1-\alpha)y) \le \alpha g(x) + (1-\alpha)g(y)\] \[g\left(\sum_{k=1}^{n} \alpha_k x_k\right) \le \sum_{k=1}^{n} \alpha_k g(x_k)\quad \text{ where } \quad \left(\alpha_k \ge 0, \sum_{k=1}^{n} \alpha_k = 1\right)\]<p>then</p>\[E\left(g(X)\right) \ge g\left(E(X)\right)\]<p>to feel the correctness for the equation above, note that convex function has another property, such that it has a supporting plane at any point of the function (P2, 0:43:00)</p>\[g(x) \ge g(a) + L_a (x-a)\]<p>then</p>\[g(X) \ge g(a) + L_a (X-a)\] \[E(g(X)) \ge g(a) + L_a \left(E\left(X\right)-a\right)\]<p>since \(a\) can be arbitary, let \(a = E(X)\)</p>\[E(g(X)) \ge g(E(X))\]<p>for example, \(E(X^2) \ge (E(X))^2\), then it is easy to show, if we let \(g(X) = X^2\)</p>\[Var(X) = E(X - EX)^2 = E(X^2) - (E(X))^2 \ge 0\]<p><strong>approximation</strong></p><p>we define mean square distance between two random variable</p>\[\left(E(X-Y)^2\right)^{1/2} = d(X,Y)\]<p>if we want to find the best approximation in terms of a constant</p>\[\min\limits_{a} E(X-a)^2\] \[\dfrac{d}{da} E(X-a)^2 = 0\] \[-2E(X-a) = 0\] \[a=EX\]<p>now can understand mean is the best approximation using an constant, and variance is the square distance.</p>\[\min\limits_{g} E(X-g(Y))^2\]<p>to find the result above, we need <strong>conditional expection</strong></p>\[E(X \vert Y)\]<p>conditional expection has several important properties.</p><ol><li>  \(E(X \vert Y)\) is a random variable.<li>  \(E\left(\sum\limits_{k=1}^{n} X_k \vert Y\right) = \sum\limits_{k=1}^{n} E(X_k \vert Y)\)<li>  \(E(E(X\vert Y)) = E(X)\)<li>  \(E(X h(Y) \vert Y) = h(Y) E(X \vert Y)\)</ol><p>now we can find \(\min\limits_{g} E(X-g(Y))^2\) intuitively</p>\[E(X-g(Y))^2 = E(E((X-g(Y))^2 \vert Y))\]<p>we know</p>\[E((X-g(Y))^2 \vert Y) \ge E((X - E(X \vert Y))^2 \vert Y)\]<p>thus</p>\[E(X-g(Y))^2 = E(E((X-g(Y))^2 \vert Y)) \ge E( E((X - E(X \vert Y))^2 \vert Y) ) = E( (X - E(X \vert Y))^2)\]<p>thus the optimal \(g(Y) = E(X \vert Y)\).</p><p>The above is an intuitive understanding, now we give the formal prove</p>\[\begin{align} &amp; E(X-g(Y))^2 = E(X - E(X \vert Y) + E(X \vert Y) - g(Y))^2\\ = &amp; E(X-E(X \vert Y))^2 + E(E(X \vert Y) - g(Y))^2 + 2 E((X - E(X \vert Y))(E(X \vert Y) - g(Y))) \end{align}\]<p>we only need to show the last term equals zero.</p>\[\begin{align} &amp; E((X - E(X \vert Y))(E(X \vert Y) - g(Y)))\\ = &amp; E( E( (X - E(X \vert Y))(E(X \vert Y) - g(Y))\vert Y) ) = 0 \end{align}\]<p>we typicaly want to do <strong>parametric</strong> or <strong>non-parametric</strong> model, an example for <strong>parametric</strong> can be we know the data follows guassian distribution, and we want to find its mean and variance; an example for non-parametric model is clustering.</p><p>We first look at parametric model. We want to find a function \(\hat{\theta}(X_1, \dots, X_n)\), such a function we may call it estimator, statistic or feature.</p><p>For such estimator, there are two school of thoughts: <strong>frequencist</strong> and <strong>bayesian</strong>. We first look at frequencist. It assume the actual \(\theta\) is an unknown constant (not random).</p>\[\min\limits_{\hat{\theta}} E(\hat{\theta}(X_1, \dots, X_n) - \theta)^2\] \[\hat{\theta}_{opt} = E(\theta \vert X_1, \dots, X_n) = \theta\] \[\begin{align} &amp; E(\hat{\theta} - \theta)^2 = E(\hat{theta} - E(\hat{\theta}) + E(\hat{\theta}) - \theta)^2\\ = &amp; E(\hat{\theta} - E(\hat{\theta}))^2 + E(E(\hat{\theta}) - \theta)^2 + 2E((\hat{\theta} - E(\hat{\theta}))(E(\hat{\theta})-\theta))\\ = &amp; E(\hat{\theta} - E(\hat{\theta}))^2 + E(E(\hat{\theta}) - \theta)^2 \end{align}\]<p>The first term is variance, the second term is bias.</p><p>for example, a typical estimator for mean is</p>\[\hat{\theta}(X_1, \dots, X_n) = \dfrac{1}{n} \sum_{k=1}^{n} X_k\]<p>assume all \(E(X_k) = \theta\)</p>\[E(\hat{\theta}) = \theta\] \[\begin{align} &amp; Var(\hat{\theta}) = E(\dfrac{1}{n} \sum_{k=1}^{n} X_k - \theta)^2\\ = &amp; \dfrac{1}{n^2} E(\sum_{k=1}^{n} (X_k - \theta))^2\\ = &amp; \dfrac{1}{n^2} \left( \sum_{k=1}^{n} E(X_k - \theta)^2 \right) + \dfrac{1}{n^2} \sum_{i \ne j} E (X_i - \theta)(X_j - \theta) \end{align}\]<p>assume \(E (X_i - \theta)(X_j - \theta) = 0\) when \(i \ne j\)</p>\[Var(\hat{\theta}) = \dfrac{\sigma^2}{n}\]<p>a typical estimator for variance is</p>\[\hat{\theta} = \dfrac{1}{n-1} \sum_{k=1}^n (X_k - \overline{X})^2\]<p>where</p>\[\overline{X} = \dfrac{1}{n} \sum_{k=1}^{n} X_k\]<p>we can check \(E(\hat{\theta})\)</p>\[\begin{align} &amp; (n-1) E(\hat{\theta}) = E(\sum_{k=1}^{n} (X_k - \overline{X})^2)\\ = &amp; \sum_{k=1}^n E(X_k^2 - 2 X_k \overline{X} + \overline{X}^2)\\ = &amp; \sum_{k=1}^n E(X_k^2) - 2 E\left((\sum_{k=1}^n X_k) \overline{X}\right) + n E(\overline{X}^2)\\ = &amp; \sum_{k=1}^n E(X_k^2) - 2n E(\overline{X}^2) + n E(\overline{X}^2)\\ = &amp; \sum_{k=1}^n E(X_k^2) - n E(\overline{X}^2) \end{align}\] \[E(X_k^2) = \sigma^2 + \mu^2\] \[E(\overline{X}^2) = \dfrac{\sigma^2}{n} + \mu^2\] \[(n-1)E(\hat{\theta}) = n (\sigma^2 + \mu^2) - n (\dfrac{\sigma^2}{n} + \mu^2) = (n-1) \sigma^2\]<p><strong>conditional variance</strong></p>\[Var(X \vert Y) = E\left( \left( X - E(X \vert Y) \right)^2 \vert Y \right)\]<p>we can show</p>\[Var(X) = Var( E (X \vert Y)) + E( Var(X \vert Y))\] \[\begin{align} &amp; E(X - EX)^2 = E(X - E(X \vert Y) + E(X \vert Y) - EX)^2\\ = &amp; E(X - E(X \vert Y))^2 + E(E(X \vert Y) - EX)^2 + 2 E \left( (X-E(X \vert Y))(E(X \vert Y) - EX) \right) \end{align}\]<p>we can show the last term is zero</p>\[E \left( (X-E(X \vert Y))(E(X \vert Y) - EX) \right) = E\left( E \left( (X-E(X \vert Y))(E(X \vert Y) - EX) \vert Y \right) \right) = 0\]<p>the first term</p>\[E(X - E(X \vert Y))^2 = E\left( E\left( (X - E(X \vert Y))^2 \vert Y \right) \right) = E(Var(X \vert Y))\]<p>the second term</p>\[E(E(X \vert Y) - EX)^2 = E( (E(X \vert Y) - E(E(X \vert Y)) )^2 ) = Var(E(X \vert Y))\]<p>P3</p><p><strong>Mean Square Error (MSE)</strong></p>\[MSE(\hat{\theta}) = E(\hat{\theta} - \theta)^2\]<p><strong>unbias estimator</strong></p>\[E(\hat{\theta}) = \theta\]<p>then</p>\[MSE(\hat{\theta}) = Var(\hat{\theta})\]<p><strong>sufficiency</strong></p>\[f(x,\theta \vert s = t) \text{ is independent of } \theta\]<p>for example, \(X_1, \dots X_n\) iid ber(p), each \(X\) is either 0 or 1.</p>\[f(x_1, \dots, x_n) = p^{\sum_{k=1}^n x_k} (1-p)^{n - \sum_{k=1}^n x_k}\]<p>let \(s = \sum_{k=1}^n x_k\)</p>\[\begin{align} &amp; P(x_1, \dots, x_n \vert s = t)\\ = &amp; P(x_1, \dots, x_n, s = t) / P(s=t)\\ = &amp; p^t (1-p)^{n-t} / \binom{n}{t} p^t (1-p)^{n-t}\\ = &amp; 1 / \binom{n}{t} \end{align}\]<p><strong>Naymen Facterization (without proof)</strong></p>\[s \text{ is sufficient} \quad \iff \quad f(x,\theta) = g(s(x),\theta)h(x)\]<p><strong>poisson</strong></p>\[f(x_1, \dots, x_n, \lambda) = \lambda^{\sum_{k=1}^{n} x_k} \exp(-\lambda) \prod_{k=1}^{n} \dfrac{1}{(x_k)!}\] \[s = \sum_{k=1}^{n} x_k\]<p><strong>gaussian</strong></p><p>assume \(\sigma^2\) is known, but \(\mu\) is unknown</p>\[f(x_1, \dots, x_n, \mu) =\]<p><strong>Rao Blackwell Procedure</strong></p><p>assume \(\hat{\theta}\) is unbias estimator, then if \(s\) is sufficient</p>\[E(\hat{\theta} \vert s)\]<p>is also an estimator, it is also unbias, and its MSE is smaller than \(MSE(\hat{\theta})\), to prove it, first it is an estimator, we didn’t prove it here, but \(s\) is sufficient will make sure \(E(\hat{\theta} \vert s)\) is not a function of \(\theta\) (without proof).</p><p>Then it is unbias</p>\[E(E(\hat{\theta} \vert s )) = E(\hat{\theta}) = \theta\]<p>then MSE is smaller</p>\[Var(\hat{\theta}) = Var(E(\hat{\theta} \vert s)) + E(Var(\hat{\theta} \vert s))\]<p>for example, \(X_1, \dots, X_n\) iid \(N(\mu, \sigma^2)\), \(\sigma^2\) is known, \(s = \sum_{k=1}^{n} X_k\), assume \(\hat{\theta} = X_1\)</p>\[E(X_1 \vert \sum_{k=1}^{n} X_k) \text{ is the new estimator}\]<p>from symmetry</p>\[E(X_j \vert \sum_{k=1}^{n} X_k) = E(X_i \vert \sum_{k=1}^{n} X_k)\] \[n \cdot E(X_1 \vert \sum_{k=1}^{n} X_k) = E( \sum_{k=1}^{n} X_k \vert \sum_{k=1}^{n} X_k ) = \sum_{k=1}^{n} X_k\] \[E(X_j \vert \sum_{k=1}^{n} X_k) = \dfrac{\sum_{k=1}^{n} X_k}{n}\]<p>it is clear the MSE get smaller.</p><p><strong>completeness</strong></p><p>Say \(T\) is a statistic, it is said to be complete, meaning for every function \(g\), if \(E(g(T)) = 0\) then \(g(T) = 0\).</p><p><strong>Lehmam Scheffe theorem</strong></p><p>if \(T\) is sufficient and complete, and \(E(h(T)) = \theta\), then \(h(T)\) is minimum variance unbias estimator</p><p>for any unbias estimator \(\hat{\theta}\), we know \(E(\hat{\theta} \vert T)\) has smaller or equal MSE.</p>\[E(h(T) - E(\hat{\theta} \vert T)) = \theta - \theta = 0\]<p>so \(h(T) = E(\hat{\theta} \vert T)\), so \(h(T)\) is better or equal to any unbias estimator.</p><p><strong>Craner Rao Lower Bound</strong></p><p>for any unbias estimaor \(\hat{\theta}\)</p>\[\theta = \int_{-\infty}^{\infty} \hat{\theta(x)} f(x, \theta) dx\] \[\begin{align} 1 &amp;= \dfrac{\partial}{\partial \theta} \int_{-\infty}^{\infty} \hat{\theta}(x) f(x,\theta) dx\\ &amp;= \int_{-\infty}^{\infty} \hat{\theta}(x) \dfrac{\partial}{\partial \theta} f(x,\theta) dx \end{align}\] \[1 = \int_{-\infty}^{\infty} f(x,\theta) dx\] \[0 = \int_{-\infty}^{\infty} \dfrac{\partial}{\partial \theta} f(x,\theta) dx\] \[0 = \int_{-\infty}^{\infty} \theta \dfrac{\partial}{\partial \theta} f(x,\theta) dx\] \[\begin{align} 1 &amp;= \int_{-\infty}^{\infty} (\hat{\theta}(x) - \theta) \dfrac{\partial}{\partial \theta} f(x,\theta) dx\\ &amp;= \int_{-\infty}^{\infty} (\hat{\theta}(x) - \theta) \left(\dfrac{\partial}{\partial \theta} \ln f(x,\theta)\right) f(x,\theta) dx \end{align}\]<p>we want to use Cauchy-Schwarz inequality</p>\[\left( \int f(x) g(x) dx \right)^2 \le \int f(x)^2 dx \int g(x)^2 dx\] \[\begin{align} 1^2 &amp;= \left(\int_{-\infty}^{\infty} (\hat{\theta}(x) - \theta) \sqrt{f(x,\theta)} \left(\dfrac{\partial}{\partial \theta} \ln f(x,\theta)\right) \sqrt{f(x,\theta)} dx\right)^2\\ &amp; \le \int_{-\infty}^{\infty} (\hat{\theta}(x) - \theta)^2 f(x,\theta) dx \int_{-\infty}^{\infty} \left(\dfrac{\partial}{\partial \theta} \ln f(x,\theta)\right)^2 f(x,\theta) dx\\ &amp;= E(\hat{\theta}(x) - \theta)^2 \cdot E \left(\dfrac{\partial}{\partial \theta} \ln f(x,\theta)\right)^2 \end{align}\] \[E(\hat{\theta}(x) - \theta)^2 \ge 1 / E \left(\dfrac{\partial}{\partial \theta} \ln f(x,\theta)\right)^2\]<p>where \(E \left(\dfrac{\partial}{\partial \theta} \ln f(x,\theta)\right)^2\) is called Fisher information.</p><p>It can be proved that (P3, 1:57:00)</p>\[E \left(\dfrac{\partial}{\partial \theta} \ln f(x,\theta)\right)^2 = -E\left( \dfrac{\partial^2}{\partial \theta^2} \ln f(x,\theta) \right)\]<p>the cauchy is equal if and only if</p>\[\dfrac{\partial}{\partial \theta} \ln f(x,\theta) = k(\theta)(\hat{\theta}(x)-\theta)\]<p><strong>notese until P3 somewhere</strong></p><p>video until P6 1:00:00</p><p>jump to video P10</p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/math/'>math</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/math/" class="post-tag no-text-decoration" >math</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Modern+Digital+Signal+Processing+Explanation+-+Looooooong&url=%2Fposts%2Fmodern-dsp-explain%2F" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Modern+Digital+Signal+Processing+Explanation+-+Looooooong&u=%2Fposts%2Fmodern-dsp-explain%2F" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=%2Fposts%2Fmodern-dsp-explain%2F&text=Modern+Digital+Signal+Processing+Explanation+-+Looooooong" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recently Updated</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/analog-circuits/">Analog Circuits</a><li><a href="/posts/signal-and-system-01/">Signal And System 01</a><li><a href="/posts/integration-by-substitution/">Integration by Subsitution</a><li><a href="/posts/signal-and-system-explain/">Signal And System Explain</a><li><a href="/posts/engineering-mathematics/">Engineering Mathematics</a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/math/">math</a> <a class="post-tag" href="/tags/analog/">analog</a> <a class="post-tag" href="/tags/pll/">pll</a> <a class="post-tag" href="/tags/fourier-analysis/">fourier-analysis</a> <a class="post-tag" href="/tags/scheme/">scheme</a> <a class="post-tag" href="/tags/sicp/">sicp</a> <a class="post-tag" href="/tags/calculus/">calculus</a> <a class="post-tag" href="/tags/cs61a/">cs61a</a> <a class="post-tag" href="/tags/em/">EM</a> <a class="post-tag" href="/tags/probability/">probability</a></div></div></div></div></div><div class="row"><div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 pl-3 pr-3 pr-xl-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/engineering-mathematics-explanation/"><div class="card-body"> <em class="small" data-ts="1690099200" data-df="ll" > Jul 23, 2023 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Engineering Mathematics Explanation</h3><div class="text-muted small"><p> First-Order First-Degree ODE Useful Formulas Integrations [\begin{align} &amp;amp; \int x^n dx = \dfrac{x^{n+1}}{n+1} + C &amp;amp; \int \dfrac{1}{x} dx = \ln \vert x \vert + C &amp;amp; \int \frac{1}{x+a} ...</p></div></div></a></div><div class="card"> <a href="/posts/topics-in-the-theory-of-random-noise-01/"><div class="card-body"> <em class="small" data-ts="1671530400" data-df="ll" > Dec 20, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Topics in The Theory of Random Noise 01</h3><div class="text-muted small"><p> From book_topics_in_the_theory_of_random_noise Chapter 1: Random Functions and Their Statistical Characteristics 1. Random Variables A random variable $\xi$ should have definition mean of any fu...</p></div></div></a></div><div class="card"> <a href="/posts/probability-theory-and-examples-01/"><div class="card-body"> <em class="small" data-ts="1672221600" data-df="ll" > Dec 28, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Probability: Theory and Examples 01</h3><div class="text-muted small"><p> From book_probability_theory_and_examples Measure Theory Probability Spaces Here and throughout the book, terms being defined are set in boldface. Here and in what follows, countable means fini...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/modern-dsp/" class="btn btn-outline-primary" prompt="Older"><p>Modern Digital Signal Processing</p></a><div class="btn btn-outline-primary disabled" prompt="Newer"><p>-</p></div></div></div></div><footer class="row pl-3 pr-3"><div class="col-12 d-flex justify-content-between align-items-center text-muted pl-0 pr-0"><div class="footer-left"><p class="mb-0"> © 2024 <a href="https://twitter.com/username">Long</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/math/">math</a> <a class="post-tag" href="/tags/analog/">analog</a> <a class="post-tag" href="/tags/pll/">pll</a> <a class="post-tag" href="/tags/fourier-analysis/">fourier-analysis</a> <a class="post-tag" href="/tags/scheme/">scheme</a> <a class="post-tag" href="/tags/sicp/">sicp</a> <a class="post-tag" href="/tags/calculus/">calculus</a> <a class="post-tag" href="/tags/cs61a/">cs61a</a> <a class="post-tag" href="/tags/em/">EM</a> <a class="post-tag" href="/tags/probability/">probability</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a><div id="notification" class="toast" role="alert" aria-live="assertive" aria-atomic="true" data-animation="true" data-autohide="false"><div class="toast-header"> <button type="button" class="ml-2 ml-auto close" data-dismiss="toast" aria-label="Close"> <span aria-hidden="true">&times;</span> </button></div><div class="toast-body text-center pt-0"><p class="pl-2 pr-2 mb-3">A new version of content is available.</p><button type="button" class="btn btn-primary" aria-label="Update"> Update </button></div></div><script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No results found.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lozad/dist/lozad.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/dayjs@1/dayjs.min.js,npm/dayjs@1/locale/en.min.js,npm/dayjs@1/plugin/relativeTime.min.js,npm/dayjs@1/plugin/localizedFormat.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script> <script defer src="/app.js"></script>
